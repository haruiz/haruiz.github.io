"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[907],{1934:e=>{e.exports=JSON.parse('{"permalink":"/blog/improve-rag-systems-reliability-with-citations","source":"@site/blog/2025-03-25-improve-rag-systems-reliability-with-citations/index.md","title":"Building Trustworthy RAG Systems with In Text Citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"rag","permalink":"/blog/tags/rag"},{"inline":true,"label":"retrieval-augmented-generation","permalink":"/blog/tags/retrieval-augmented-generation"},{"inline":true,"label":"rag-pipelines","permalink":"/blog/tags/rag-pipelines"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"},{"inline":true,"label":"generative-ai","permalink":"/blog/tags/generative-ai"},{"inline":true,"label":"explainable-ai","permalink":"/blog/tags/explainable-ai"},{"inline":true,"label":"ai-for-research","permalink":"/blog/tags/ai-for-research"},{"inline":true,"label":"citation-generation","permalink":"/blog/tags/citation-generation"},{"inline":true,"label":"langchain","permalink":"/blog/tags/langchain"},{"inline":true,"label":"llamaindex","permalink":"/blog/tags/llamaindex"},{"inline":true,"label":"gemini-api","permalink":"/blog/tags/gemini-api"},{"inline":true,"label":"google-genai","permalink":"/blog/tags/google-genai"},{"inline":true,"label":"trustworthy-ai","permalink":"/blog/tags/trustworthy-ai"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"semantic-search","permalink":"/blog/tags/semantic-search"},{"inline":true,"label":"python","permalink":"/blog/tags/python"}],"readingTime":16.675,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Building Trustworthy RAG Systems with In Text Citations","slug":"improve-rag-systems-reliability-with-citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","authors":["haruiz"],"tags":["rag","retrieval-augmented-generation","rag-pipelines","llms","generative-ai","explainable-ai","ai-for-research","citation-generation","langchain","llamaindex","gemini-api","google-genai","trustworthy-ai","machine-learning","semantic-search","python"]},"unlisted":false,"prevItem":{"title":"Getting Started with Ray on Google Cloud Platform","permalink":"/blog/getting-started-with-ray-on-google-cloud-platform"},"nextItem":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","permalink":"/blog/intro-to-system-design"}}')},4950:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/figure-5c44a2c95718ef15fee82b054ebaa879.png"},5209:(e,n,t)=>{t.d(n,{A:()=>x});var i=t(6540);const r="audioPlayer_bD77",s="controlButton_18Mu",a="playPauseButton_BOck",o="timeDisplay__nfh",l="progressBar_GLd5",d="volumeControl_jpxX",c="volumeSlider_ecko",h="speedControl_V7oK",u="speedSelect_tSpO";var g=t(5604),m=t(4848);const p=e=>{if(isNaN(e)||e===1/0)return"00:00";const n=Math.floor(e/60),t=Math.floor(e%60);return`${String(n).padStart(2,"0")}:${String(t).padStart(2,"0")}`},x=e=>{let{audioSrc:n,title:t="Audio Version"}=e;const[x,f]=(0,i.useState)(!1),[v,y]=(0,i.useState)(0),[b,_]=(0,i.useState)(0),[w,j]=(0,i.useState)(1),[k,A]=(0,i.useState)(!1),[I,C]=(0,i.useState)(1),[S,T]=(0,i.useState)(!1),G=(0,i.useRef)(null),L=(0,i.useRef)(null),E=(0,i.useRef)(null);(0,i.useEffect)((()=>{const e=G.current;if(!e)return;const n=()=>{y(e.duration),_(e.currentTime),T(!0)},t=()=>_(e.currentTime),i=()=>{f(!1),_(e.duration),cancelAnimationFrame(E.current)};return f(!1),_(0),y(0),T(!1),E.current&&cancelAnimationFrame(E.current),e.addEventListener("loadedmetadata",n),e.addEventListener("timeupdate",t),e.addEventListener("ended",i),()=>{e.removeEventListener("loadedmetadata",n),e.removeEventListener("timeupdate",t),e.removeEventListener("ended",i),E.current&&cancelAnimationFrame(E.current)}}),[n]);const R=(0,i.useCallback)((()=>{if(G.current&&L.current){const e=G.current,n=L.current,t=e.currentTime;_(t),n.value=t,n.style.setProperty("--seek-before-width",t/v*100+"%"),E.current=requestAnimationFrame(R)}}),[v]);(0,i.useEffect)((()=>(x?(G.current.play(),E.current=requestAnimationFrame(R)):(G.current.pause(),cancelAnimationFrame(E.current)),()=>cancelAnimationFrame(E.current))),[x,R]);return(0,m.jsxs)("div",{className:r,children:[(0,m.jsx)("audio",{ref:G,src:n,preload:"metadata"}),(0,m.jsx)("button",{onClick:()=>{S&&f((e=>!e))},className:`${s} ${a}`,"aria-label":x?"Pause":"Play",disabled:!S,children:x?(0,m.jsx)(g.kwt,{}):(0,m.jsx)(g.gSK,{})}),(0,m.jsx)("span",{className:o,children:p(b)}),(0,m.jsx)("input",{ref:L,type:"range",className:l,value:b,max:v||0,onChange:e=>{if(!S)return;const n=Number(e.target.value);G.current.currentTime=n,_(n),L.current.style.setProperty("--seek-before-width",n/v*100+"%")},"aria-label":"Seek progress",disabled:!S,style:{"--seek-before-width":b/v*100+"%"}}),(0,m.jsx)("span",{className:o,children:p(v)}),(0,m.jsxs)("div",{className:d,children:[(0,m.jsx)("button",{onClick:()=>{const e=G.current;if(k){const n=w>0?w:.5;j(n),e.volume=n,A(!1)}else e.volume=0,A(!0),j(0)},className:s,"aria-label":k?"Unmute":"Mute",children:k?(0,m.jsx)(g.FZ2,{}):(0,m.jsx)(g.pPd,{})}),(0,m.jsx)("input",{type:"range",min:"0",max:"1",step:"0.01",value:k?0:w,onChange:e=>{const n=Number(e.target.value);j(n),G.current.volume=n,A(0===n)},className:c,"aria-label":"Volume",style:{"--volume-before-width":(k?0:100*w)+"%"}})]}),(0,m.jsx)("div",{className:h,children:(0,m.jsxs)("select",{value:I,onChange:e=>{const n=Number(e.target.value);C(n),G.current.playbackRate=n},className:u,"aria-label":"Playback speed",children:[(0,m.jsx)("option",{value:"0.5",children:"0.5x"}),(0,m.jsx)("option",{value:"0.75",children:"0.75x"}),(0,m.jsx)("option",{value:"1",children:"1x"}),(0,m.jsx)("option",{value:"1.25",children:"1.25x"}),(0,m.jsx)("option",{value:"1.5",children:"1.5x"}),(0,m.jsx)("option",{value:"2",children:"2x"})]})})]})}},7372:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/medias/audio-75281001c5d50c11b08161854301953d.wav"},7536:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(1934),r=t(4848),s=t(8453),a=t(5209);const o={title:"Building Trustworthy RAG Systems with In Text Citations",slug:"improve-rag-systems-reliability-with-citations",description:'Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a "black box,".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.',authors:["haruiz"],tags:["rag","retrieval-augmented-generation","rag-pipelines","llms","generative-ai","explainable-ai","ai-for-research","citation-generation","langchain","llamaindex","gemini-api","google-genai","trustworthy-ai","machine-learning","semantic-search","python"]},l=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"In This Post You Will Learn",id:"in-this-post-you-will-learn",level:2},{value:"Why are Citations Essential?",id:"why-are-citations-essential",level:2},{value:"Implementing Citations in RAG Systems",id:"implementing-citations-in-rag-systems",level:2},{value:"Hands-on Implementations (In-text) Citations",id:"hands-on-implementations-in-text-citations",level:2},{value:"LangChain example",id:"langchain-example",level:3},{value:"\u2705 Here is all the code:",id:"-here-is-all-the-code",level:3},{value:"Key Steps:",id:"key-steps",level:3},{value:"1. <strong>Environment Setup and Imports</strong>",id:"1-environment-setup-and-imports",level:4},{value:"2. <strong>Query arXiv for Relevant Papers</strong>",id:"2-query-arxiv-for-relevant-papers",level:4},{value:"3. <strong>Download and Process Papers</strong>",id:"3-download-and-process-papers",level:4},{value:"4. <strong>Create or Load FAISS Index</strong>",id:"4-create-or-load-faiss-index",level:4},{value:"5. <strong>Semantic Search over the Indexed Chunks</strong>",id:"5-semantic-search-over-the-indexed-chunks",level:4},{value:"6. <strong>Generate an LLM-Based Answer with Citations</strong>",id:"6-generate-an-llm-based-answer-with-citations",level:4},{value:"7. <strong>Main Program Logic</strong>",id:"7-main-program-logic",level:4},{value:"LlamaIndex example <a>[3]</a>",id:"llamaindex-example-3",level:3},{value:"\u2705 Here is all the code:",id:"-here-is-all-the-code-1",level:3},{value:"Key Steps:",id:"key-steps-1",level:3},{value:"1. <strong>Environment Setup and Logging Configuration</strong>",id:"1-environment-setup-and-logging-configuration",level:4},{value:"2. <strong>Define Prompt Templates</strong>",id:"2-define-prompt-templates",level:4},{value:"3. <strong>Declare Custom Events</strong>",id:"3-declare-custom-events",level:4},{value:"4. <strong>Build the Citation Query Workflow</strong>",id:"4-build-the-citation-query-workflow",level:4},{value:"5. <strong>Execute the Workflow</strong>",id:"5-execute-the-workflow",level:4},{value:"Using Grounding with Google Search in the Gemini API example",id:"using-grounding-with-google-search-in-the-gemini-api-example",level:3},{value:"\u2705 Here is all the code:",id:"-here-is-all-the-code-2",level:3},{value:"Key Steps:",id:"key-steps-2",level:3},{value:"1. <strong>Set Up and Define the Citation Schema</strong>",id:"1-set-up-and-define-the-citation-schema",level:4},{value:"2. <strong>Call Gemini\u2019s Multimodal API with Google Search Tooling</strong>",id:"2-call-geminis-multimodal-api-with-google-search-tooling",level:4},{value:"3. <strong>Extract Grounded Citations</strong>",id:"3-extract-grounded-citations",level:4},{value:"4. <strong>Inject Inline Citations</strong>",id:"4-inject-inline-citations",level:4},{value:"5. <strong>Format the Citation Section</strong>",id:"5-format-the-citation-section",level:4},{value:"6. <strong>Main Execution</strong>",id:"6-main-execution",level:4},{value:"Key Differences and Summary",id:"key-differences-and-summary",level:2},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.A,{audioSrc:t(7372).A}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["Retrieval-Augmented Generation (RAG) has emerged as a promising method to address the \u201challucination\u201d problem in large language models (LLMs) by grounding responses in external, traceable knowledge sources ",(0,r.jsx)("a",{href:"#arvix:1",children:"[1]"}),'\n. By integrating retrieval mechanisms with generative capabilities, RAG systems produce more accurate, informative, and up-to-date outputs, making them especially powerful for question-answering and content creation tasks. However, a critical but often overlooked aspect is the ability to attribute claims to their original sources. Without proper citations, RAG becomes a "black box," undermining the trustworthiness and verifiability of its responses. While much of the existing work has focused on enhancing response quality, less attention has been paid to source attribution',(0,r.jsx)("a",{href:"#arvix:2",children:"[2]"}),"."]}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.h2,{id:"in-this-post-you-will-learn",children:"In This Post You Will Learn"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Why ",(0,r.jsx)(n.strong,{children:"citations are critical"})," in RAG systems for improving trust, traceability, and reducing hallucinations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Some approaches for implementing citations in content generation and RAG pipelines:"})," source-aware generation, inline citations, post-hoc attribution, and more."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"How to build and implement in-line-text citation-aware RAG pipelines using:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LangChain"})," + FAISS for LLM-backed scholarly QA."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LlamaIndex"})," for structured workflows and granular citations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Google\u2019s Gemini API"})," with Google Search grounding for real-time web references."]}),"\n"]}),"\n"]}),"\n"]})]}),"\n",(0,r.jsx)(n.h2,{id:"why-are-citations-essential",children:"Why are Citations Essential?"}),"\n",(0,r.jsx)(n.p,{children:"Retrieving external context is a proven strategy for reducing hallucinations and enhancing the reliability of generative AI outputs. However, surfacing relevant documents is only part of the equation\u2014explicitly citing sources is critical for building user trust and enabling verification. Much of the existing research has focused on citation correctness\u2014whether the cited document supports the claim\u2014but correctness alone isn\u2019t sufficient. To establish true credibility, we also need citation faithfulness, which ensures the cited content accurately reflects the intended meaning of the generated response."}),"\n",(0,r.jsx)(n.p,{children:"Together, these two dimensions\u2014correctness and faithfulness\u2014are foundational to the trustworthiness, transparency, and usability of RAG systems. They support a range of key benefits:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Verifiability:"})," Allow users to trace claims back to original documents."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trust:"})," Build user confidence by grounding outputs in identifiable sources."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transparency and Explainability:"})," Help users understand where information comes from."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Debugging and Improvement:"})," Make it easier to audit and correct flawed generations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduced Hallucinations:"})," Anchor responses in concrete evidence."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Respecting Intellectual Property:"})," Ensure proper attribution to original authors and sources."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["While citations don\u2019t guarantee the elimination of hallucinations, they play a crucial role in mitigating them by offering a transparent path to verify information and understand its original context. In the paper \u201cCorrectness is not Faithfulness in RAG Attributions\u201d ",(0,r.jsx)("a",{href:"#arvix:2",children:"[2]"}),", the authors emphasize the importance of distinguishing between citation correctness (does the source support the claim?) and citation faithfulness (does the citation reflect the actual meaning?). Their work calls for more nuanced evaluation metrics that go beyond simple correctness to better assess the quality and reliability of AI-generated citations."]}),"\n",(0,r.jsx)(n.p,{children:"This distinction is illustrated in the following figure:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"Correctness vs. Faithfulness in RAG Attributions",src:t(4950).A+"",width:"1263",height:"767"}),"\n",(0,r.jsx)(n.em,{children:'(a) Faithful citation: The model generates the correct answer ("Berlin is Germany\'s capital") based on a relevant retrieved document ("Berlin : German capital") and correctly cites that document. (b) Citing related context: The model generates the correct answer, likely based on the relevant document or its memory, but incorrectly cites a related but non-supporting retrieved document ("Bonn: no longer capital").(c) Correct but unfaithful: The model generates the correct answer using its internal memory, not the retrieved documents, but still cites a retrieved document ("Berlin : German capital") that happens to support the answer. The citation is unfaithful because the cited source wasn\'t the basis for the generation. (d) Incorrect citation: The model generates the correct answer, likely from memory, but incorrectly cites a retrieved document containing false information ("Bonn : German capital").'})]}),"\n",(0,r.jsx)(n.h2,{id:"implementing-citations-in-rag-systems",children:"Implementing Citations in RAG Systems"}),"\n",(0,r.jsxs)(n.p,{children:["Common approaches to implementing citations in Retrieval-Augmented Generation (RAG) systems include source-aware generation, highlight-based attribution, post-hoc attribution, inline citations, and aggregated source lists. ",(0,r.jsx)("a",{href:"#arvix:1",children:"[1]"}),(0,r.jsx)("a",{href:"#arvix:1",children:"[2]"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"In source-aware generation"}),", the model is specifically designed or trained to associate facts with their respective source documents during answer generation. This may involve fine-tuning the model on examples that include citations or labeling retrieved text in the prompt to allow the model to reference those labels."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Highlight-based"})," attribution visually connects parts of the output to supporting sources using cues such as color-coding or tooltips. While this method enhances clarity, it requires precise alignment. Instead of merely inserting reference numbers, the system highlights sections of the answer to indicate their sources. For instance, certain sentences may be color-coded or underlined, with hover actions revealing excerpts from the original documents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Post-hoc attribution"})," works by generating the answer first and adding citations afterward. In this strategy, the RAG system produces a response without citations and subsequently searches the retrieved documents for evidence to support each statement, integrating citations into the final output."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Inline citation"})," involves embedding references, such as \u201c[1],\u201d directly within the text, thereby improving traceability while necessitating well-structured prompts or additional training. This method is typically implemented by directing the model to insert citations during response generation. For example, a prompt might instruct the model to \u201cinclude source numbers in brackets for the information you use,\u201d leading to sentences that end with references like \u201c[1]\u201d or \u201c[2]\u201d corresponding to the source documents. This live citing method (sometimes termed \u201cpre-hoc\u201d) treats citations as an integral part of the answer."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Aggregated source"}),' lists provide a summary of all sources utilized without integrating citations directly into the text. In this approach, the RAG system presents the answer followed by a bullet list or section titled "Sources," detailing all relevant documents that informed the answer, though it does not specify which fact corresponds to which source within the text.']}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In the following sections, we will explore three distinct approaches for implementing RAG applications with In-line citations using 1) Google\u2019s Generative AI SDK, Gemini and Google Search grounding, 2) LangChain, and 3) LlamaIndex. Each method offers unique features and trade-offs tailored to different use cases and preferences."}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-implementations-in-text-citations",children:"Hands-on Implementations (In-text) Citations"}),"\n",(0,r.jsx)(n.h3,{id:"langchain-example",children:"LangChain example"}),"\n",(0,r.jsx)(n.p,{children:"In the following example, we implement a pipeline that searches, downloads, semantically processes, and queries scientific papers from arXiv, leveraging vector search and a Large Language Model (LLM) to generate contextual responses with citations:"}),"\n",(0,r.jsx)(n.h3,{id:"-here-is-all-the-code",children:"\u2705 Here is all the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import itertools\nimport typing\nfrom pathlib import Path\nimport arxiv\nimport os\nfrom dotenv import load_dotenv\n\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom rich import print\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Constants\nFAISS_INDEX_PATH = Path("faiss_index")\nDOWNLOAD_FOLDER = Path("downloads")\n\n\ndef fetch_arxiv_papers(query: str, max_results: int = 5) -> list[arxiv.Result]:\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n    return list(arxiv.Client().results(search))\n\n\ndef process_papers(results: list[arxiv.Result], download_folder: Path) -> list:\n    openai_embeddings = OpenAIEmbeddings()\n    text_splitter = SemanticChunker(embeddings=openai_embeddings)\n    download_folder.mkdir(exist_ok=True, parents=True)\n    documents = []\n\n    for index, result in enumerate(results):\n        pdf_path = result.download_pdf(dirpath=str(download_folder))\n        pdf_docs = PyPDFLoader(pdf_path).load_and_split(text_splitter)\n\n        for doc in pdf_docs:\n            doc.metadata.update({\n                "title": result.title,\n                "authors": [author.name for author in result.authors],\n                "entry_id": result.entry_id.split(\'/\')[-1],\n                "year": result.published.year,\n                "url": result.entry_id,\n                "ref": f"[{index + 1}]"\n            })\n        documents.extend(pdf_docs)\n\n    return documents\n\n\ndef load_or_create_faiss_index(query: str, index_path: Path, download_folder: Path) -> FAISS:\n    openai_embeddings = OpenAIEmbeddings()\n\n    if index_path.exists():\n        print("Loading existing FAISS index...")\n        return FAISS.load_local(str(index_path), openai_embeddings, allow_dangerous_deserialization=True)\n\n    print("Creating a new FAISS index...")\n    documents = process_papers(fetch_arxiv_papers(query), download_folder)\n    faiss_index = FAISS.from_documents(documents, openai_embeddings)\n    faiss_index.save_local(str(index_path))\n    return faiss_index\n\n\ndef search_similar_documents(faiss_index: FAISS, query: str, top_k: int = 20) -> list:\n    return faiss_index.similarity_search(query, k=top_k)\n\n\ndef generate_response(context_docs: list, question: str) -> str:\n    sorted_docs = sorted(context_docs, key=lambda doc: doc.metadata.get("ref", "Unknown"))\n    formatted_context = [\n        f"{doc.metadata[\'ref\']} {doc.metadata[\'title\']}: {doc.page_content}" for doc in sorted_docs\n    ]\n\n    prompt_template = PromptTemplate(\n        template="""\n        Write a blog post based on the user query.\n        When referencing information from the context, cite the appropriate source(s) using their corresponding numbers.\n        Each source has been provided with a number and a title.\n        Every answer should include at least one source citation.\n        If none of the sources are helpful, indicate that.\n\n        ------\n        {context}\n        ------\n        Query: {query}\n        Answer:\n        """,\n        input_variables=["query", "context"]\n    )\n\n    model = ChatOpenAI(model="gpt-4o-mini")\n    parser = StrOutputParser()\n    chain = prompt_template | model | parser\n\n    return chain.invoke({"context": formatted_context, "query": question})\n\n\ndef main():\n    query = "hallucination in LLMs"\n    question = "How to mitigate hallucination ?"\n\n    faiss_index = load_or_create_faiss_index(query, FAISS_INDEX_PATH, DOWNLOAD_FOLDER)\n    relevant_docs = search_similar_documents(faiss_index, question)\n    response = generate_response(relevant_docs, question)\n\n    print("\\nGenerated Response:\\n", response)\n\n    bibliography = "\\n\\n### References\\n"\n    sorted_docs = sorted(relevant_docs, key=lambda doc: doc.metadata.get("ref", "Unknown"))\n\n    for doc_key, documents in itertools.groupby(sorted_docs, key=lambda doc: doc.metadata.get("ref", "Unknown")):\n        doc = next(documents)\n        bibliography += (\n            f"{doc.metadata.get(\'ref\', \'Unknown\')} {doc.metadata.get(\'title\', \'Unknown\')}, "\n            f"{\', \'.join(doc.metadata.get(\'authors\', \'Unknown\'))}, arXiv {doc.metadata.get(\'entry_id\', \'Unknown\')}, "\n            f"{doc.metadata.get(\'year\', \'Unknown\')}. {doc.metadata.get(\'url\', \'Unknown\')}\\n"\n        )\n    print(bibliography)\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.p,{children:"This workflow uses LangChain, OpenAI's models, FAISS for vector storage, and arXiv's Python client. Let\u2019s break it down:"}),"\n",(0,r.jsx)(n.h3,{id:"key-steps",children:"Key Steps:"}),"\n",(0,r.jsxs)(n.h4,{id:"1-environment-setup-and-imports",children:["1. ",(0,r.jsx)(n.strong,{children:"Environment Setup and Imports"})]}),"\n",(0,r.jsxs)(n.p,{children:["The code imports standard libraries (",(0,r.jsx)(n.code,{children:"os"}),", ",(0,r.jsx)(n.code,{children:"itertools"}),", ",(0,r.jsx)(n.code,{children:"pathlib"}),", etc.), third-party tools (",(0,r.jsx)(n.code,{children:"arxiv"}),", ",(0,r.jsx)(n.code,{children:"dotenv"}),"), and key LangChain modules for PDF loading, text splitting, embeddings, LLM interaction, and vector storage. It also loads API keys securely via environment variables."]}),"\n",(0,r.jsxs)(n.h4,{id:"2-query-arxiv-for-relevant-papers",children:["2. ",(0,r.jsx)(n.strong,{children:"Query arXiv for Relevant Papers"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"fetch_arxiv_papers()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Uses the ",(0,r.jsx)(n.code,{children:"arxiv"})," Python package to search for academic papers matching a query string. Returns a list of arXiv ",(0,r.jsx)(n.code,{children:"Result"})," objects."]}),"\n",(0,r.jsxs)(n.h4,{id:"3-download-and-process-papers",children:["3. ",(0,r.jsx)(n.strong,{children:"Download and Process Papers"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"process_papers()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Downloads the PDF files, splits them into semantically meaningful text chunks using OpenAI embeddings, and attaches rich metadata such as title, authors, publication year, and a reference number."}),"\n",(0,r.jsxs)(n.h4,{id:"4-create-or-load-faiss-index",children:["4. ",(0,r.jsx)(n.strong,{children:"Create or Load FAISS Index"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"load_or_create_faiss_index()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Checks whether a local FAISS index already exists. If not, it creates one by embedding the paper chunks and saving the index locally."}),"\n",(0,r.jsxs)(n.h4,{id:"5-semantic-search-over-the-indexed-chunks",children:["5. ",(0,r.jsx)(n.strong,{children:"Semantic Search over the Indexed Chunks"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"search_similar_documents()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Takes a user query and searches for the most semantically similar document chunks using vector similarity (k-nearest neighbors search)."}),"\n",(0,r.jsxs)(n.h4,{id:"6-generate-an-llm-based-answer-with-citations",children:["6. ",(0,r.jsx)(n.strong,{children:"Generate an LLM-Based Answer with Citations"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"generate_response()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Uses a prompt template and an OpenAI Chat model (",(0,r.jsx)(n.code,{children:"gpt-4o-mini"}),") to generate a response grounded in the top retrieved documents, including source references inline using numbered citations."]}),"\n",(0,r.jsxs)(n.h4,{id:"7-main-program-logic",children:["7. ",(0,r.jsx)(n.strong,{children:"Main Program Logic"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"main()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Puts all the above steps together to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Search and download papers about ",(0,r.jsx)(n.em,{children:"hallucinations in LLMs"})]}),"\n",(0,r.jsxs)(n.li,{children:["Answer the question: ",(0,r.jsx)(n.em,{children:"How to mitigate hallucination?"})]}),"\n",(0,r.jsx)(n.li,{children:"Print the generated response"}),"\n",(0,r.jsx)(n.li,{children:"Print a formatted bibliography of the cited papers"}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"llamaindex-example-3",children:["LlamaIndex example ",(0,r.jsx)("a",{href:"#llamaindex:1",children:"[3]"})]}),"\n",(0,r.jsx)(n.p,{children:"Following with hands-on implementations, let's explore how to build a citation-aware query engine using LlamaIndex. This implementation starts by retrieving relevant text chunks from a set of documents, splits them into citable segments, and uses a Large Language Model (LLM) to synthesize a well-cited answer to a given query."}),"\n",(0,r.jsx)(n.h3,{id:"-here-is-all-the-code-1",children:"\u2705 Here is all the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport logging\nfrom typing import List, Union\n\nfrom dotenv import load_dotenv\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.workflow import Context, Workflow, StartEvent, StopEvent, step, Event\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, TextNode\nfrom llama_index.core.response_synthesizers import ResponseMode, get_response_synthesizer\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.prompts import PromptTemplate\n\n# Load environment variables\nload_dotenv(verbose=True)\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Prompt templates for citation-based QA\nCITATION_QA_TEMPLATE = PromptTemplate(\n    "Please provide an answer based solely on the provided sources. "\n    "When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. "\n    "Every answer should include at least one source citation. "\n    "Only cite a source when you are explicitly referencing it. "\n    "If none of the sources are helpful, you should indicate that. \\n"\n    "Example:\\n"\n    "Source 1:\\nThe sky is red in the evening and blue in the morning.\\n"\n    "Source 2:\\nWater is wet when the sky is red.\\n"\n    "Query: When is water wet?\\n"\n    "Answer: Water will be wet when the sky is red [2], which occurs in the evening [1].\\n"\n    "Now it\'s your turn. Below are several numbered sources:\\n"\n    "{context_str}\\nQuery: {query_str}\\nAnswer: "\n)\n\nCITATION_REFINE_TEMPLATE = PromptTemplate(\n    "Please refine the existing answer based solely on the provided sources. "\n    "Cite sources where necessary, following this format:\\n"\n    "Example:\\n"\n    "Existing answer: {existing_answer}\\n"\n    "{context_msg}\\n"\n    "Query: {query_str}\\nRefined Answer: "\n)\n\nDEFAULT_CITATION_CHUNK_SIZE = 512\nDEFAULT_CITATION_CHUNK_OVERLAP = 20\n\n\nclass RetrieverEvent(Event):\n    """Event triggered after document retrieval."""\n\n    nodes: List[NodeWithScore]\n\n\nclass CreateCitationsEvent(Event):\n    """Event triggered after creating citations."""\n\n    nodes: List[NodeWithScore]\n\n\nclass CitationQueryEngineWorkflow(Workflow):\n    """Workflow for processing queries with retrieval-augmented generation (RAG)."""\n\n    @step\n    async def retrieve(self, ctx: Context, ev: StartEvent) -> Union[RetrieverEvent, None]:\n        """Retrieve relevant nodes based on the query."""\n        query = ev.get("query")\n        if not query:\n            logger.warning("No query provided.")\n            return None\n\n        logger.info(f"Querying database: {query}")\n\n        await ctx.set("query", query)\n\n        if ev.index is None:\n            logger.error("Index is empty. Load documents before querying!")\n            return None\n\n        retriever = ev.index.as_retriever(similarity_top_k=2)\n        nodes = retriever.retrieve(query)\n\n        logger.info(f"Retrieved {len(nodes)} nodes.")\n        return RetrieverEvent(nodes=nodes)\n\n    @step\n    async def create_citation_nodes(self, ev: RetrieverEvent) -> CreateCitationsEvent:\n        """Create granular citation nodes from retrieved text chunks."""\n        nodes = ev.nodes\n        new_nodes: List[NodeWithScore] = []\n\n        text_splitter = SentenceSplitter(\n            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\n            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\n        )\n\n        for node in nodes:\n            text_chunks = text_splitter.split_text(\n                node.node.get_content(metadata_mode=MetadataMode.NONE)\n            )\n\n            for idx, text_chunk in enumerate(text_chunks, start=len(new_nodes) + 1):\n                text = f"Source {idx}:\\n{text_chunk}\\n"\n\n                new_node = NodeWithScore(\n                    node=TextNode.model_validate(node.node), score=node.score\n                )\n                new_node.node.text = text\n                new_nodes.append(new_node)\n\n        logger.info(f"Created {len(new_nodes)} citation nodes.")\n        return CreateCitationsEvent(nodes=new_nodes)\n\n    @step\n    async def synthesize(self, ctx: Context, ev: CreateCitationsEvent) -> StopEvent:\n        """Generate an AI response based on retrieved citations."""\n        llm = OpenAI(model="gpt-4o-mini")\n        query = await ctx.get("query", default=None)\n\n        synthesizer = get_response_synthesizer(\n            llm=llm,\n            text_qa_template=CITATION_QA_TEMPLATE,\n            refine_template=CITATION_REFINE_TEMPLATE,\n            response_mode=ResponseMode.COMPACT,\n            use_async=True,\n        )\n\n        response = await synthesizer.asynthesize(query, nodes=ev.nodes)\n        return StopEvent(result=response)\n\n\nasync def run_workflow():\n    """Initialize the index and run the query workflow."""\n    logger.info("Loading documents...")\n    documents = SimpleDirectoryReader("downloads").load_data()\n\n    index = VectorStoreIndex.from_documents(\n        documents=documents,\n        embed_model=OpenAIEmbedding(model_name="text-embedding-3-small"),\n    )\n\n    logger.info("Running citation query workflow...")\n    workflow = CitationQueryEngineWorkflow()\n    result = await workflow.run(query="Write a blog post about agents?", index=index)\n\n    bibliography = "\\n\\n### References\\n"\n    for node in result.source_nodes:\n        bibliography += f"{node.get_text()}\\n"\n    print(bibliography)\n\n    return result\n\n\nif __name__ == "__main__":\n    result = asyncio.run(run_workflow())\n    print(result)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"key-steps-1",children:"Key Steps:"}),"\n",(0,r.jsxs)(n.h4,{id:"1-environment-setup-and-logging-configuration",children:["1. ",(0,r.jsx)(n.strong,{children:"Environment Setup and Logging Configuration"})]}),"\n",(0,r.jsxs)(n.p,{children:["The code loads environment variables from ",(0,r.jsx)(n.code,{children:".env"})," using ",(0,r.jsx)(n.code,{children:"dotenv"}),", and configures logging to help track events during the workflow execution."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"load_dotenv(verbose=True)\nlogging.basicConfig(level=logging.INFO)\n"})}),"\n",(0,r.jsxs)(n.h4,{id:"2-define-prompt-templates",children:["2. ",(0,r.jsx)(n.strong,{children:"Define Prompt Templates"})]}),"\n",(0,r.jsx)(n.p,{children:"Two prompt templates are defined for instructing the LLM:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"CITATION_QA_TEMPLATE"}),": Generates answers with numbered citations based on the provided context."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"CITATION_REFINE_TEMPLATE"}),": Refines an existing answer with additional citation context."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These templates ensure the model cites only relevant sources and produces trustworthy output."}),"\n",(0,r.jsxs)(n.h4,{id:"3-declare-custom-events",children:["3. ",(0,r.jsx)(n.strong,{children:"Declare Custom Events"})]}),"\n",(0,r.jsxs)(n.p,{children:["Custom ",(0,r.jsx)(n.code,{children:"Event"})," classes (",(0,r.jsx)(n.code,{children:"RetrieverEvent"}),", ",(0,r.jsx)(n.code,{children:"CreateCitationsEvent"}),") are defined to structure the flow of data across the steps of the workflow."]}),"\n",(0,r.jsxs)(n.h4,{id:"4-build-the-citation-query-workflow",children:["4. ",(0,r.jsx)(n.strong,{children:"Build the Citation Query Workflow"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"CitationQueryEngineWorkflow"})," is a subclass of ",(0,r.jsx)(n.code,{children:"Workflow"})," with three main ",(0,r.jsx)(n.code,{children:"@step"}),"s:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"retrieve()"})}),":",(0,r.jsx)(n.br,{}),"\n","Retrieves top-k relevant document nodes from a vector index based on the user's query using similarity search."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"create_citation_nodes()"})}),":",(0,r.jsx)(n.br,{}),"\n","Splits the retrieved chunks into smaller, clearly numbered sources (e.g., ",(0,r.jsx)(n.code,{children:"Source 1"}),", ",(0,r.jsx)(n.code,{children:"Source 2"}),"), ensuring each text chunk can be referenced independently."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"synthesize()"})}),":",(0,r.jsx)(n.br,{}),"\n","Generates a final, citation-rich response using the GPT-4o-mini model via the LlamaIndex synthesizer tools."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.h4,{id:"5-execute-the-workflow",children:["5. ",(0,r.jsx)(n.strong,{children:"Execute the Workflow"})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"run_workflow()"})," function loads all documents from the ",(0,r.jsx)(n.code,{children:"downloads/"})," directory and builds a ",(0,r.jsx)(n.code,{children:"VectorStoreIndex"})," using OpenAI embeddings. It then runs the query engine on the question:\n",(0,r.jsx)(n.strong,{children:'"Write a blog post about agents?"'})]}),"\n",(0,r.jsx)(n.p,{children:"Finally, it prints both the result and a nicely formatted bibliography of all cited text chunks."}),"\n",(0,r.jsx)(n.h3,{id:"using-grounding-with-google-search-in-the-gemini-api-example",children:"Using Grounding with Google Search in the Gemini API example"}),"\n",(0,r.jsx)(n.p,{children:"Finally, in this last example, you will learn how to leverage Google Search capabilities within the Gemini API to generate content with inline citations. This approach combines the power of Gemini\u2019s 2.0 with real-time web search to produce informative, grounded responses with proper attributions."}),"\n",(0,r.jsx)(n.h3,{id:"-here-is-all-the-code-2",children:"\u2705 Here is all the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import itertools\nfrom typing import Optional\n\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom rich import print\n\nfrom google import genai\nfrom google.genai.types import (\n    GenerateContentConfig,\n    Tool,\n    GoogleSearch,\n    GroundingChunk\n)\n\nload_dotenv()\n\nclass Citation(BaseModel):\n    """Represents a citation extracted from the Gemini grounding metadata."""\n    title: str\n    score: float\n    link: str\n    chunk_index: int\n    chunk_text: Optional[str] = None\n    start_index: Optional[int] = None\n    end_index: Optional[int] = None\n\n\ndef generate_content(prompt: str, model: str) -> genai.types.GenerateContentResponse:\n    client = genai.Client()\n    return client.models.generate_content(\n        model=model,\n        contents=prompt,\n        config=GenerateContentConfig(\n            response_modalities=["TEXT"],\n            tools=[Tool(google_search=GoogleSearch())],\n        ),\n    )\n\n\ndef extract_citations(response: genai.types.GenerateContentResponse) -> list[Citation]:\n    citations = []\n    grounding_metadata = response.candidates[0].grounding_metadata\n    for support in grounding_metadata.grounding_supports:\n        for idx, score in zip(support.grounding_chunk_indices, support.confidence_scores):\n            chunk: GroundingChunk = grounding_metadata.grounding_chunks[idx]\n            citations.append(\n                Citation(\n                    title=chunk.web.title,\n                    link=chunk.web.uri,\n                    score=score,\n                    chunk_index=idx,\n                    chunk_text=support.segment.text,\n                    start_index=support.segment.start_index,\n                    end_index=support.segment.end_index,\n                )\n            )\n    return citations\n\n\ndef inject_citations_into_text(text: str, citations: list[Citation]) -> str:\n    citations.sort(key=lambda x: (x.start_index, x.end_index))\n    offset = 0\n    for (start, end), group in itertools.groupby(citations, key=lambda x: (x.start_index, x.end_index)):\n        group_list = list(group)\n        indices = ",".join(str(c.chunk_index + 1) for c in group_list)\n        citation_str = f"[{indices}]"\n        text = text[:end + offset] + citation_str + text[end + offset:]\n        offset += len(citation_str)\n    return text\n\n\ndef format_citation_section(citations: list[Citation]) -> str:\n    result = "\\n\\n**Citations**\\n\\n"\n    sorted_citations = sorted(citations, key=lambda x: x.chunk_index)\n    for chunk_index, group in itertools.groupby(sorted_citations, key=lambda x: x.chunk_index):\n        citation = list(group)[0]\n        result += f"[{chunk_index + 1}] {citation.title} - {citation.link}\\n"\n    return result\n\n\ndef main():\n    MODEL_NAME = "gemini-2.0-flash"\n    response = generate_content("Write a blog post about Agents", MODEL_NAME)\n    citations = extract_citations(response)\n\n    generated_text = response.text\n    final_text = inject_citations_into_text(generated_text, citations)\n    final_text += format_citation_section(citations)\n    print(final_text)\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"key-steps-2",children:"Key Steps:"}),"\n",(0,r.jsxs)(n.h4,{id:"1-set-up-and-define-the-citation-schema",children:["1. ",(0,r.jsx)(n.strong,{children:"Set Up and Define the Citation Schema"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class Citation(BaseModel)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Defines a schema for handling citation information, including title, score, link, and text span indices for precise inline placement."}),"\n",(0,r.jsxs)(n.h4,{id:"2-call-geminis-multimodal-api-with-google-search-tooling",children:["2. ",(0,r.jsx)(n.strong,{children:"Call Gemini\u2019s Multimodal API with Google Search Tooling"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"generate_content()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This function generates content using Gemini (e.g., ",(0,r.jsx)(n.code,{children:"gemini-2.0-flash"}),") and includes a ",(0,r.jsx)(n.code,{children:"GoogleSearch"})," tool, enabling grounded web references."]}),"\n",(0,r.jsxs)(n.h4,{id:"3-extract-grounded-citations",children:["3. ",(0,r.jsx)(n.strong,{children:"Extract Grounded Citations"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"extract_citations()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Pulls out metadata like source titles, URLs, and confidence scores from the model's response using Gemini\u2019s ",(0,r.jsx)(n.code,{children:"grounding_supports"}),"."]}),"\n",(0,r.jsxs)(n.h4,{id:"4-inject-inline-citations",children:["4. ",(0,r.jsx)(n.strong,{children:"Inject Inline Citations"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"inject_citations_into_text()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Adds numbered citation markers like ",(0,r.jsx)(n.code,{children:"[1]"}),", ",(0,r.jsx)(n.code,{children:"[2]"}),", etc., directly into the generated text using the start and end positions returned by Gemini."]}),"\n",(0,r.jsxs)(n.h4,{id:"5-format-the-citation-section",children:["5. ",(0,r.jsx)(n.strong,{children:"Format the Citation Section"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"format_citation_section()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Generates a clean, markdown-style bibliography list at the end of the post, matching each inline marker with its source."}),"\n",(0,r.jsxs)(n.h4,{id:"6-main-execution",children:["6. ",(0,r.jsx)(n.strong,{children:"Main Execution"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"main()\n"})}),"\n",(0,r.jsx)(n.p,{children:'Combines everything: generates content about "Agents", extracts and injects citations, and prints the final, publication-ready blog post with proper attributions.'}),"\n",(0,r.jsx)(n.h2,{id:"key-differences-and-summary",children:"Key Differences and Summary"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Grounding with Google Search in the Gemini API"})," enhances the accuracy and freshness of model responses by leveraging Google\u2019s real-time search and grounding capabilities. It\u2019s straightforward to implement but is tightly integrated with Google\u2019s Gemini models family."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LangChain:"})," Provides a modular framework for building RAG pipelines. Offers flexibility in choosing components (document loaders, text splitters, vector stores, LLMs). Requires more manual setup, but allows for greater customization. Focuses on creating a reference string in the metadata and using that in the prompt."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LlamaIndex:"})," Offers higher-level abstractions for RAG, including workflows and specialized components for citation handling. Emphasizes creating granular citation nodes for precise referencing. Uses very explicit prompt templates to guide the LLM."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"All three approaches achieve the same goal \u2013 generating responses with citations \u2013 but they use different mechanisms and levels of abstraction. The choice of which to use depends on your specific needs and preferences. The most important common thread is the careful management of metadata to track the source of information."}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"In the era of AI-generated content, trust is everything. Retrieval-Augmented Generation has unlocked new levels of intelligence and context-awareness, but without clear, faithful citations, even the most accurate answers remain suspect. Citations are not just a safeguard against hallucinations\u2014they are the bridge between AI and human understanding, offering transparency, accountability, and traceability. This post has walked through practical, hands-on implementations using LangChain, LlamaIndex, and Google\u2019s Gemini API to demonstrate that citation-aware RAG isn't just a research ideal\u2014it\u2019s an achievable standard. As builders and researchers, the responsibility is ours to push beyond plausible-sounding responses and deliver outputs that are grounded, explainable, and verifiably true. The future of reliable AI starts with showing your sources."}),"\n",(0,r.jsxs)(n.p,{children:["All the code snippets and examples in this post are available on GitHub following this link:\n",(0,r.jsx)(n.a,{href:"https://github.com/haruiz/llm-app-patterns/tree/main",children:"llm-app-patterns"})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"References"})}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"arvix:1",href:"https://arxiv.org/abs/2410.11217",target:"_blank",children:"[1] Qian, H., Fan, Y., Zhang, R., & Guo, J. (2024, October 15). On the Capacity of Citation Generation by Large Language Models"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"arvix:2",href:"https://arxiv.org/abs/2412.18004",target:"_blank",children:"[2] Wallat, J., Heuss, M., Maarten, D. R., & Anand, A. (2024, December 23). Correctness is not Faithfulness in RAG Attributions"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"llamaindex:1",href:"https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/",target:"_blank",children:"[3] LlamaIndex Documentation: Build RAG with in-line citations"})})]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}}}]);