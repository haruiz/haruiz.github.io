"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3099],{3890:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"getting-started-with-ray-on-google-cloud-platform","metadata":{"permalink":"/blog/getting-started-with-ray-on-google-cloud-platform","source":"@site/blog/2025-04-01-getting-started-with-ray-on-google-cloud-platform/index.md","title":"Getting Started with Ray on Google Cloud Platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"ray","permalink":"/blog/tags/ray"},{"inline":true,"label":"google-cloud","permalink":"/blog/tags/google-cloud"},{"inline":true,"label":"vertex-ai","permalink":"/blog/tags/vertex-ai"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"scalable-ai","permalink":"/blog/tags/scalable-ai"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"cloud-computing","permalink":"/blog/tags/cloud-computing"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"},{"inline":true,"label":"mlops","permalink":"/blog/tags/mlops"},{"inline":true,"label":"ray-datasets","permalink":"/blog/tags/ray-datasets"},{"inline":true,"label":"parallel-computing","permalink":"/blog/tags/parallel-computing"}],"readingTime":19.285,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Getting Started with Ray on Google Cloud Platform","slug":"getting-started-with-ray-on-google-cloud-platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","authors":["haruiz"],"tags":["ray","google-cloud","vertex-ai","distributed-computing","machine-learning","scalable-ai","python","cloud-computing","kubernetes","data-science","mlops","ray-datasets","parallel-computing"]},"unlisted":false,"nextItem":{"title":"Building Trustworthy RAG Systems with In Text Citations","permalink":"/blog/improve-rag-systems-reliability-with-citations"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\n### Introduction \\n\\nAs AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax.\\n\\nWhen combined with the power and scalability of Google Cloud Platform (GCP), particularly Vertex AI\u2014Ray enables seamless orchestration of large-scale data science and machine learning workflows, from local prototyping to full production deployments.\\n\\nThis post introduces the fundamentals of Ray and walks you through how to deploy and manage Ray clusters on GCP using Vertex AI, empowering you to run scalable and efficient distributed workloads with minimal operational overhead.\\n\\n:::tip \\n## In This Post You Will Learn\\n- What Ray is and how it supports scalable Python and AI applications\\n- Core components of a Ray cluster: head nodes, worker nodes, autoscaler, and scheduler\\n- How to configure and manage Ray clusters on Google Cloud Platform with Vertex AI\\n- How Ray handles distributed task execution and resource allocation across nodes and GPUs\\n- How to submit jobs and interact with Ray clusters using the Ray Job API and Python SDK\\n- Best practices for using Ray Datasets to efficiently ingest, transform, and serve distributed data\\n- Practical examples for configuring autoscaling, deploying workloads, and optimizing cluster usage\\n:::\\n\\n### What is Ray?\\n\\n**Ray** is an open-source, unified framework engineered to enable scalable and distributed computing for AI and Python-based applications. It provides an efficient and extensible compute layer for executing parallel and distributed workloads, abstracting the complexities typically associated with distributed systems such as resource management, orchestration, and fault tolerance.\\n\\nBy decoupling infrastructure concerns from application logic, Ray allows developers and researchers to seamlessly scale individual components or end-to-end machine learning (ML) pipelines\u2014from prototyping on a local machine to executing large-scale workloads across multi-node, multi-GPU clusters.\\nRay\u2019s modular and composable architecture supports diverse roles across the machine learning and data science lifecycle, delivering specific advantages for each:\\n\\n- **For Data Scientists:** Ray enables transparent parallelization of data preprocessing, feature extraction, model training, and evaluation workflows across heterogeneous compute resources, without requiring in-depth knowledge of distributed systems. Its native integration with popular Python libraries (e.g., NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch) ensures smooth adoption into existing ecosystems.\\n\\n- **For Machine Learning Engineers:** Ray facilitates the development of production-grade ML platforms through unified APIs that allow the same codebase to scale seamlessly from a developer\'s laptop to large-scale clusters. This consistency accelerates the deployment lifecycle, reduces operational overhead, and ensures reproducibility in model training and experimentation workflows.\\n\\n- **For Systems Engineers and DevOps Teams:** Ray handles lower-level system functions including task scheduling, resource allocation, fault tolerance, and elastic scaling. It also supports Kubernetes-native deployments, making it easier to integrate with modern cloud-native infrastructure and CI/CD pipelines.\\n\\n### Ray\'s Core Components\\nRay enables seamless scaling of high-performance analytical workloads\u2014from a single laptop to a large, distributed cluster. You can get started locally with a simple `ray.init()` call during development. However, to scale applications across multiple nodes in production, you\'ll need to *deploy a Ray cluster*.\\n\\nA **Ray cluster** is a collection of nodes working together to execute tasks and manage data in a distributed environment. Each cluster consists of a **head node** and one or more **worker nodes**, each with distinct responsibilities that enable efficient and scalable execution.\\n\\n#### Head Node\\n\\nThe **head node** acts as the central coordinator of the cluster. It runs the core components responsible for orchestration and cluster management, including:\\n\\n- **Global Control Store (GCS):** A distributed metadata store that tracks task specifications, function definitions, object locations, and scheduling events. GCS ensures scalability and fault tolerance across the cluster.\\n- **Autoscaler:** A daemon process running on the head node\u2014or as a sidecar container within the head pod in Kubernetes environments\u2014that continuously monitors resource utilization and dynamically adjusts the number of worker nodes in response to real-time workload demands, enabling elastic cluster scaling both upward and downward as needed.\\n- **Driver Processes:** Entry points for Ray applications. Each driver launches and manages a job, which may include thousands of distributed tasks and actors.\\n\\nWhile the head node is capable of executing tasks and actors, in larger clusters it\'s typically configured to focus solely on coordination duties to optimize performance and avoid resource contention.\\n\\n#### Worker Nodes\\n\\n**Worker nodes** are dedicated to executing user-defined tasks and actors. They do not run any cluster management components, allowing them to fully commit their resources to computation. Worker nodes participate in distributed scheduling and contribute to Ray\'s object store, which enables efficient sharing of intermediate results across tasks.\\n\\nThe follwing figure illustrates the core components of a Ray cluster:\\n\\n![Ray Components](figure2.png)\\n***Figure 1: Ray Cluster Components***\\n\\nBy integrating these core components, Ray delivers a flexible and powerful framework for distributed computing\u2014capable of supporting diverse workloads such as large-scale data processing, machine learning pipelines, and real-time inference.\\n\\n:::info\\nRay nodes are implemented as pods when running on Kubernetes.\\n:::\\n\\n### From Cores to Clusters: Understanding Worker Distribution and Computation Scaling\\n\\nAs mentioned, in a Ray cluster, worker nodes are responsible for executing tasks and actors, utilizing the available computational resources such as CPU cores and GPUs. The distribution of workloads across these resources is managed through Ray\'s resource allocation mechanisms. The resource allocation is managed by the **Ray Scheduler**, which assigns tasks and actors to nodes based on specified resource requirements.\\n\\n- **Resource Specification:**\\n\\nDevelopers can specify worker resource requirements using the @ray.remote decorator and ray_actor_options to allocate CPUs, GPUs, and custom resources. The scheduler then ensures tasks are executed on nodes with sufficient resources to optimize cluster performance.\\n\\n```python\\nimport ray\\n\\n# defining task resources using the @ray.remote decorator\\n@ray.remote(num_cpus=2, num_gpus=1)\\ndef my_function():\\n    # Function implementation\\n    pass\\n\\n@ray.remote(num_cpus=1)\\nclass MyActor:\\n    # Actor implementation\\n    pass\\n```\\n\\nIn this example, `my_function` requires 2 CPU cores and 1 GPU, while `MyActor` requires 1 CPU core. By default, if resource requirements are not specified, Ray assigns 1 CPU core to each task or actor.\\n\\n- **Fractional Resource Allocation:**\\n\\nAdditionally, ray supports fractional resource specifications, allowing tasks or actors to utilize portions of a resource. This is particularly useful for lightweight tasks or when sharing resources among multiple processes. For example, to allocate half a CPU core to a task\\n\\n```Python\\n# defining a task with fractional CPU allocation\\n@ray.remote(num_cpus=0.5)\\ndef lightweight_task():\\n    # Task implementation\\n    pass\\n```\\n\\nSimilarly, if two actors can share a single GPU:\\n\\n```Python\\n# defining actors with shared GPU allocation\\n@ray.remote(num_gpus=0.5)\\nclass SharedGPUActor:\\n    # Actor implementation\\n    pass\\n```\\n\\nThis configuration allows for efficient utilization of resources by enabling multiple processes to share the same hardware components.\\n\\n- **Node Resource Configuration:**\\n\\nWhen initializing a Ray cluster, you can define the resources available on each node. By default, Ray detects the number of CPU cores and GPUs on a machine and assigns them as available resources. However, you can override these defaults during initialization:[Max Pumperla](https://maxpumperla.com/learning_ray/ch_02_ray_core/)\\n\\n```python\\nimport ray\\n\\nray.init(num_cpus=4, num_gpus=2, resources={\\"custom_resource\\": 1})\\n````\\nThis command starts a Ray node with 4 CPU cores, 2 GPUs, and an additional custom resource labeled \\"custom_resource\\" with a quantity of 1. \\n\\n- **Resource Management in Distributed Environments:**\\n\\nIn distributed setups, such as those using Kubernetes with KubeRay, resource requests and limits can be specified in the pod templates to ensure appropriate allocation:[Medium+1Ray+1](https://medium.com/google-cloud/simplifying-ray-and-distributed-computing-2c4b5ca72ad8)\\n\\n```yaml\\nresources:\\n  limits:\\n    nvidia.com/gpu: 1\\n  requests:\\n    memory: \\"4Gi\\"\\n    cpu: \\"2\\"\\n```\\n\\nThis configuration requests 2 CPU cores, 4 GiB of memory, and 1 GPU for the pod, ensuring that the Ray worker has the necessary resources allocated by the Kubernetes scheduler.\\n\\nBy explicitly defining resource requirements and configurations, Ray effectively manages the distribution of tasks and actors across CPU cores, processes, machines, and GPUs, optimizing the utilization of computational resources within the cluster.\\n\\n### Ray Scaling mode\\n\\nRay is designed to scale Python applications in two ways: across multiple machines (**horizontal scaling**) and within a single machine (**vertical scaling**).\\n\\n1. **Horizontal Scaling:** Ray seamlessly expands applications from a single node to a cluster of machines. By dynamically distributing tasks across nodes, it enables efficient parallel processing\u2014particularly valuable for large-scale machine learning tasks like distributed training and hyperparameter tuning.\\n2. **Vertical Scaling:** On a single machine, Ray maximizes resource utilization by parallelizing tasks across multiple CPU cores and GPUs. This optimization enhances performance for operations like data preprocessing and model inference.\\n\\nThrough these complementary scaling strategies, Ray offers the flexibility to handle varying computational demands, making it ideal for diverse AI and machine learning applications.\\n\\n### How Data Is Shared Across Worker Nodes\\n\\nIn **Ray**, efficient **data sharing and distributed computation** are foundational to its performance. At the heart of this is the **Ray Object Store**, a distributed shared-memory system designed to facilitate fast and scalable data sharing across the cluster.  Each node in a Ray cluster maintains its **own local object store**, which stores immutable objects\u2014such as datasets or model parameters\u2014used by Ray tasks and actors. This design allows for **zero-copy access** within a node, significantly reducing serialization overhead and memory duplication. When multiple workers on the same node need to access the same object, they can do so directly from shared memory, leading to highly efficient **intra-node data sharing**. (https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring)\\n\\nHowever, when data needs to be accessed across nodes (**inter-node sharing**), Ray\u2019s **distributed scheduler** comes into play. It orchestrates the transfer by serializing the object on the source node, transmitting it over the network, and deserializing it into the object store on the destination node. To avoid unnecessary data movement and associated costs, Ray incorporates **locality-aware scheduling**\u2014a strategy where tasks are preferentially scheduled on nodes where the needed data is already present. This greatly improves system performance and reduces latency.\\n\\nRay also provides a high-level API for data handling through its [**Datasets**](https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring) module, which serves as the primary user-facing Python interface for working with distributed data.\\n\\nAt its core, **Ray Datasets** represent a **distributed dataset abstraction**, where the underlying data is partitioned into blocks that are distributed across the Ray cluster and stored in distributed memory. This architecture enables parallelism by design.\\n\\nEach block of data can be loaded in parallel by worker tasks, with each task pulling a block (e.g., from cloud storage like S3) and storing it in the local object store of its node. The client-side `Dataset` object maintains lightweight references to these distributed blocks, enabling efficient tracking and manipulation without needing to move data unnecessarily. When operations are applied to the `Dataset`, they are executed in parallel across the distributed blocks\u2014allowing scalable data transformations and preprocessing workflows.\\n\\nA typical usage pattern for Ray Datasets involves:\\n\\n1. **Creating** a `Dataset` from external storage or in-memory data.\\n2. **Applying** transformations using parallel operations (e.g., `map`, `filter`, `split`, `groupby`).\\n3. **Consuming** the processed dataset by either writing it to external storage or feeding it into training and scoring pipelines.\\n\\n:::note\\nAs it is shown in the figure 3, the **Ray Datasets API** is not designed to replace general-purpose data processing frameworks like Spark. Instead, it serves as the **last-mile bridge** between upstream ETL pipelines and **distributed applications running on Ray**.\\n\\nThis bridging role becomes especially powerful when combined with **Ray-native DataFrame libraries** during the data processing stage. By keeping data in memory across stages, you can seamlessly run an **end-to-end data-to-ML pipeline entirely within Ray**, without the overhead of writing intermediate results to external storage.\\n\\nIn this architecture, **Ray acts as the universal compute substrate**, and Datasets function as the **distributed data backbone**, connecting each stage of the pipeline\u2014from data ingestion and transformation to training and inference\u2014with high efficiency and flexibility.\\n:::\\n\\n![Ray Datasets](figure3.png)\\n***Figure 2: Ray Data Ingestion Pipeline***\\n\\n### Blocks\\n\\nA **block** is the fundamental unit of data storage and transfer in **Ray Data**. Each block holds a disjoint subset of rows and is stored in Ray\u2019s **shared-memory object store**, enabling efficient parallel loading, transformation, and distribution across the cluster.\\n\\nBlocks can contain data of any modality\u2014such as text, binary data (e.g., images), or numerical arrays. However, the full capabilities of Ray Datasets are best realized with **tabular data**. In this case, each block represents a **partition of a distributed table**, internally stored as an **Apache Arrow Table**, forming a highly efficient, columnar, distributed Arrow dataset.\\n\\nThe figure below illustrates a dataset composed of three blocks, each containing 1,000 rows. The `Dataset` object itself resides in the process that initiates execution (typically the **driver process**), while the blocks are stored as immutable objects in the Ray object store, distributed across the cluster.\\n\\n![image.png](figure4.png)\\n***Figure 3: Ray Dataset Blocks***\\n\\n### **Data format compatibility**\\n\\nRay Datasets supports a wide range of **popular tabular file formats**\u2014including **CSV**, **JSON**, and **Parquet**\u2014as well as various **storage backends** like local disk, **Amazon S3**, **Google Cloud Storage (GCS)**, **Azure Blob Storage**, and **HDFS**. This broad compatibility is made possible by **Arrow\u2019s I/O layer**, which provides a unified and efficient interface for reading and writing data.\\n\\nIn addition to tabular data, Ray Datasets also supports **parallel reads and writes** of **NumPy arrays**, **text**, and **binary files**, enabling seamless ingestion of multi-modal datasets.\\n\\nTogether, this flexible I/O support and Ray\u2019s scalable execution engine make Datasets a powerful tool for **efficiently loading large-scale data** into your cluster\u2014ready for transformation, training, or serving.\\n\\n```python\\n# Read structured data from disk, cloud storage, etc.\\nray.data.read_parquet(\\"s3://path/to/parquet\\")\\nray.data.read_json(\\"...\\")\\nray.data.read_csv(\\"...\\")\\nray.data.read_text(\\"...\\")\\n\\n# Read tensor / image / file data.\\nray.data.read_numpy(\\"...\\")\\nray.data.read_binary_files(\\"...\\")\\n\\n# Create from in-memory objects.\\nray.data.from_objects([list, of, python, objects])\\nray.data.from_pandas([list, of, pandas, dfs])\\nray.data.from_numpy([list, of, numpy, arrays])\\nray.data.from_arrow([list, of, arrow, tables])\\n```\\n\\n### Seamless Data Framework Compatibility\\n\\nBeyond just storage I/O, **Ray Datasets** supports **bidirectional in-memory data exchange** with a wide range of popular data frameworks\u2014making it easy to integrate into your existing workflows.\\n\\nWhen frameworks like **Spark**, **Dask**, **Modin**, or **Mars** are run on Ray, Datasets can interact directly with them in memory, enabling efficient data sharing without unnecessary serialization or disk I/O. For smaller-scale, local data operations, Datasets also works smoothly with **Pandas** and **NumPy**.\\n\\nTo simplify data loading into machine learning models, Ray Datasets includes built-in adapters for both **PyTorch** and **TensorFlow**. These adapters allow you to convert a Ray Dataset into a framework-native structure\u2014such as `torch.utils.data.IterableDataset` or `tf.data.Dataset`\u2014so you can plug them directly into your training loops with minimal effort.\\n\\n```python\\n# Convert from existing DataFrames.\\nray.data.from_spark(spark_df)\\nray.data.from_dask(dask_df)\\nray.data.from_modin(modin_df)\\n\\n# Convert to DataFrames and ML datasets.\\ndataset.to_spark()\\ndataset.to_dask()\\ndataset.to_modin()\\ndataset.to_torch()\\ndataset.to_tf()\\n\\n# Convert to objects in the shared memory object store.\\ndataset.to_numpy_refs()\\ndataset.to_arrow_refs()\\ndataset.to_pandas_refs()\\n```\\n\\n# Deploying Ray Clusters at Scale\\n\\nRay offers native cluster deployment support on these technology stacks:\\n\\n- On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported integrations exist for Azure, Aliyun, and vSphere, and natively on GCP using vertexai.\\n- On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), through the officially supported KubeRay project.\\n- On [Anyscale](https://www.anyscale.com/ray-on-anyscale?utm_source=ray_docs&utm_medium=docs&utm_campaign=ray-doc-upsell&utm_content=ray-cluster-deployment&__hstc=152921254.1a5e5e4ba1437e11b22cfd8a9df17049.1742234597786.1743123888642.1743186807718.5&__hssc=152921254.2.1743186807718&__hsfp=4074942027), a fully managed Ray platform by Ray\'s creators. You can use existing AWS, GCP, Azure and Kubernetes clusters, or utilize Anyscale\'s hosted compute layer.\\n\\n# Deploying Ray Clusters on Vertex AI\\n\\nVertex AI offers a flexible and scalable environment for running Ray, allowing you to harness the power of Ray\u2019s distributed computing within Google Cloud\u2019s managed ML platform. Whether you\'re running training, tuning, or serving workloads, deploying Ray on Vertex AI gives you full control over cluster lifecycle and resource usage.\\n\\nRay clusters on Vertex AI are designed to **stay active** to ensure consistent capacity for critical machine learning workloads or seasonal demand spikes. **Unlike custom jobs**, which automatically release resources after completion, **Ray clusters persist** until explicitly deleted.\\n\\n**When to use long-running Ray clusters:**\\n\\n- You repeatedly submit the **same Ray job** and want to benefit from **data or image caching**.\\n- You run **many short-lived jobs** where startup time outweighs job runtime\u2014making persistent clusters more efficient.\\n\\n### Setting up Ray on VertexAI\\n\\nTo run Ray clusters on Vertex AI, follow these quick setup steps:\\n\\n1. First, **enable the Vertex AI API in your Google Cloud project**:\\n\\n```bash\\ngcloud services enable aiplatform.googleapis.com\\n```\\n\\n2. **Install the Vertex AI SDK** for Python\\n\\nThe SDK includes features such as the [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html), BigQuery integration, cluster management, and prediction APIs.\\n\\n- **Via Google Cloud Console:**\\n\\n  After [creating a Ray cluster](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster), the Vertex AI console provides access to a **Colab Enterprise notebook** that guides you through the SDK installation.\\n\\n- **Via Workbench or Custom Environment:**\\n\\n  If you\'re using **Vertex AI Workbench** or any other Python environment, install the SDK manually:\\n\\n    ```bash\\n    pip install google-cloud-aiplatform[ray]\\n    ```\\n\\n\\n:::tip \ud83d\udd10 Networking & Security Tips for Ray on Vertex AI\\n\\n- \ud83d\udd10 **(Optional)** Enable **VPC Service Controls** to reduce the risk of data exfiltration. Be aware that this restricts access to resources outside the perimeter (e.g., public Cloud Storage buckets).\\n- \ud83c\udf10 **Use an auto mode VPC network** (one per project is recommended). Avoid custom mode or multiple VPC networks in the same project, as these may cause cluster creation to fail.\\n:::\\n\\n3. **Create a Ray Cluster**. To perform this task, you can use either the Google Cloud Console or the Vertex AI SDK for Python. Ray clusters on Vertex AI support up to **2,000 total nodes**, with a maximum of **1,000 nodes per worker pool**. Although there is no limit to the number of worker pools, creating too many (e.g., 1,000 pools with a single node each) can lead to reduced performance. It\'s recommended to balance the number of worker pools and nodes per pool for optimal scalability and efficiency.\\n\\n- **Via GCP Console**\\n\\nTo create a Ray cluster on Vertex AI from the GCP console,\xa0**navigate to the Ray on Vertex AI page in the Google Cloud console, click \\"Create Cluster,\\" configure the cluster settings (name, region, machine types, etc.), and then click \\"Create\\"**\\n\\n![image.png](figure5.png)\\n***Figure 4: Ray Cluster on Vertex AI Menu***\\n\\n![image.png](figure6.png)\\n***Figure 5: Ray Cluster on Vertex AI Creation Form***\\n\\nFor detailed instructions and guidance on choosing the best configuration for your needs, follow the link: https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#console.\\n\\n- **Via Python**\\n\\nAlternatively, you can create, and manage your Ray clusters using Python with the following code snippets:\\n\\n```python\\nimport time\\nimport logging\\nimport ray\\nfrom ray.job_submission import JobSubmissionClient, JobStatus\\nfrom google.cloud import aiplatform as vertexai\\nfrom google.oauth2 import service_account\\n\\nfrom vertex_ray import create_ray_cluster, update_ray_cluster, delete_ray_cluster, list_ray_clusters, get_ray_cluster\\nfrom vertex_ray import Resources, AutoscalingSpec\\n\\n# -----------------------------\\n# Configuration\\n# -----------------------------\\nPROJECT_NAME = \\"<your-project-name>\\"\\nPROJECT_NUMBER = \\"<your-project-number>\\"\\nREGION = \\"us-central1\\"\\nCLUSTER_NAME = \\"ray-cluster\\"\\nSERVICE_ACCOUNT_FILE = \\"<path-to-your-service-account-file>.json\\"\\nRAY_VERSION = \\"2.33\\"\\nPYTHON_VERSION = \\"3.10\\"\\n\\n# -----------------------------\\n# Setup Logging\\n# -----------------------------\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# -----------------------------\\n# Authenticate Vertex AI\\n# -----------------------------\\ncredentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\\nvertexai.init(credentials=credentials, location=REGION)\\n\\n# -----------------------------\\n# Cluster Management\\n# -----------------------------\\ndef get_or_create_basic_ray_cluster():\\n    \\"\\"\\"Create a default Ray cluster on Vertex AI.\\"\\"\\"\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    if cluster:\\n        logger.info(f\\"Cluster {CLUSTER_NAME} already exists.\\")\\n        return cluster.cluster_resource_name\\n\\n    logger.info(f\\"Creating cluster {CLUSTER_NAME}...\\")\\n    head = Resources(machine_type=\\"n1-standard-16\\", node_count=1)\\n    workers = [Resources(\\n        machine_type=\\"n1-standard-8\\",\\n        node_count=2,\\n        accelerator_type=\\"NVIDIA_TESLA_T4\\",\\n        accelerator_count=1\\n    )]\\n    cluster = create_ray_cluster(\\n        head_node_type=head,\\n        worker_node_types=workers,\\n        cluster_name=CLUSTER_NAME,\\n        ray_version=RAY_VERSION,\\n        python_version=PYTHON_VERSION\\n    )\\n    return cluster\\n\\ndef create_autoscaling_ray_cluster():\\n    \\"\\"\\"Create a Ray cluster with autoscaling on Vertex AI.\\"\\"\\"\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    if cluster:\\n        logger.info(f\\"Cluster {CLUSTER_NAME} already exists.\\")\\n        return cluster.cluster_resource_name\\n\\n    logger.info(f\\"Creating cluster {CLUSTER_NAME}...\\")\\n    autoscaling = AutoscalingSpec(min_replica_count=1, max_replica_count=3)\\n    head = Resources(machine_type=\\"n1-standard-16\\", node_count=1)\\n    workers = [Resources(\\n        machine_type=\\"n1-standard-16\\",\\n        accelerator_type=\\"NVIDIA_TESLA_T4\\",\\n        accelerator_count=1,\\n        autoscaling_spec=autoscaling\\n    )]\\n    cluster = create_ray_cluster(\\n        head_node_type=head,\\n        worker_node_types=workers,\\n        cluster_name=CLUSTER_NAME,\\n        ray_version=RAY_VERSION,\\n        python_version=PYTHON_VERSION\\n    )\\n    return cluster\\n\\ndef scale_ray_cluster(cluster_resource_name: str, new_replica_count: int):\\n    \\"\\"\\"Update worker replica count of an existing Ray cluster.\\"\\"\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    for worker in cluster.worker_node_types:\\n        worker.node_count = new_replica_count\\n    updated = update_ray_cluster(\\n        cluster_resource_name=cluster.cluster_resource_name,\\n        worker_node_types=cluster.worker_node_types\\n    )\\n    return updated\\n\\ndef delete_cluster(cluster_resource_name: str):\\n    \\"\\"\\"Delete the specified Ray cluster.\\"\\"\\"\\n    logger.info(f\\"Deleting Ray cluster: {cluster_resource_name}\\")\\n    delete_ray_cluster(cluster_resource_name)\\n\\ndef list_clusters():\\n    \\"\\"\\"List all Ray clusters on Vertex AI.\\"\\"\\"\\n    clusters = list_ray_clusters()\\n    for cluster in clusters:\\n        logger.info(cluster)\\n```\\n\\nAfter deploying a Ray cluster, you can start running your Ray applications! As shown in the next figure, you can either use the Ray Jobs API or run the job interactively.\\n\\n![image.png](figure7.png)\\n***Figure 6: Way to run Ray jobs***\\n\\n### 1. **Ray Jobs API (Recommended)**\\n\\nUse the CLI, Python SDK, or REST API to:\\n\\n- Submit jobs with an entrypoint like `python script.py`\\n- Define a runtime environment (e.g., dependencies)\\n- Run jobs remotely and independently of the client connection\\n- View job status, logs, and manage runs\\n\\nJobs are tied to the cluster\u2019s lifetime \u2014 if the cluster stops, so do all jobs.\\n\\n### 2. **Interactive Mode**\\n\\n- SSH into the cluster node and run scripts directly (via `ray attach`)\\n- Use Ray Client (for advanced users) to connect live from your machine\\n\\nNote: Jobs started this way aren\'t tracked by the Ray Jobs API.\\n\\n\ud83d\udc49 [Full Guide](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html)\\n\\n```python\\n# -----------------------------\\n# Ray Job Submission\\n# -----------------------------\\ndef submit_ray_job(script_path: str):\\n    \\"\\"\\"Submit a Ray job to a given cluster.\\"\\"\\"\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    ray_cluster = get_ray_cluster(cluster_resource_name)\\n    client = JobSubmissionClient(f\\"vertex_ray://{ray_cluster.dashboard_address}\\")\\n\\n    job_id = client.submit_job(\\n        entrypoint=f\\"python3 {script_path}\\",\\n        runtime_env={\\n            \\"working_dir\\": \\".\\",\\n            \\"pip\\": [\\n            f\\"ray=={RAY_VERSION}\\"\\n        ],\\n        },\\n    )\\n    while True:\\n        status = client.get_job_status(job_id)\\n        if status == JobStatus.SUCCEEDED:\\n            logger.info(\\"Job succeeded.\\")\\n            break\\n        elif status == JobStatus.FAILED:\\n            logger.error(\\"Job failed.\\")\\n            break\\n        else:\\n            logger.info(\\"Job is running...\\")\\n            time.sleep(30)\\n\\n# -----------------------------\\n# Direct Cluster Usage\\n# -----------------------------\\ndef run_remote_ray_job():\\n    \\"\\"\\"Example Ray job executed on the cluster.\\"\\"\\"\\n    @ray.remote(num_cpus=1)\\n    def heavy_task(x):\\n        return sum(i * i for i in range(x))\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    ray.init(address=f\\"vertex_ray://{cluster_resource_name}\\", ignore_reinit_error=True)\\n\\t\\t\\n\\t\\t# fetch the computation results\\n    results = [heavy_task.remote(1000000) for _ in range(1000)]\\n    outputs = ray.get(results)\\n    logger.info(f\\"Total result: {sum(outputs)}\\")\\n\\n    ray.shutdown()\\n\\n```\\n\\n```python\\n# -----------------------------\\n# Main Entry Point\\n# -----------------------------\\ndef main():\\n    cluster_resource_name = get_or_create_basic_ray_cluster()\\n\\n    if not cluster_resource_name:\\n        raise RuntimeError(\\"Ray cluster creation failed\\")\\n\\n    logger.info(\\"Listing clusters...\\")\\n    clusters = list_ray_clusters()\\n    if not clusters:\\n        raise RuntimeError(\\"No Ray clusters found.\\")\\n\\n    latest_cluster = clusters[-1].cluster_resource_name\\n    logger.info(f\\"Submitting job to cluster: {latest_cluster}\\")\\n    submit_ray_job(\\"ray_job.py\\")\\n    run_remote_ray_job()\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n\\n```\\n\\n***ray_job.py***\\n```python\\nimport time\\nimport ray\\n\\n#Initialize Ray\\nray.init()\\n\\n# Define a computationally intensive task\\n@ray.remote(num_cpus=1)\\ndef heavy_task(x):\\n    \\"\\"\\"\\n    Simulates a heavy workload by performing a CPU-bound operation.\\n    This example calculates the sum of squares for a range of numbers.\\n    \\"\\"\\"\\n    total = 0\\n    for i in range(x):\\n        total += i * i\\n    time.sleep(1)  # Simulate some work duration\\n    return total\\n\\n# Generate a large number of tasks\\nnum_tasks = 1000\\nresults = []\\n# Initialize connection to the Ray cluster on Vertex AI.\\nray.init(ignore_reinit_error=True) # local testing\\nfor i in range(num_tasks):\\n    results.append(heavy_task.remote(1000000))\\n\\n# Retrieve results (this will trigger autoscaling if needed)\\noutputs = ray.get(results)\\n# Print the sum of the results (optional)\\nprint(f\\"Sum of results: {sum(outputs)}\\")\\n# Terminate the process\\nray.shutdown()\\n```\\n\\n### Conclusion\\n\\nIn this post, we explored how **Ray** provides a powerful framework for scaling AI, data science, and Python applications across distributed infrastructure. We walked through the process of creating and managing Ray clusters using both the **Google Cloud Console** and the **Vertex AI Python SDK**, including how to configure **autoscaling** for dynamic resource management. The accompanying code examples showcased essential capabilities such as **cluster provisioning**, **job submission**, and efficiently executing **distributed workloads** across multiple nodes.\\n\\nIn upcoming posts, we\u2019ll dive deeper into:\\n\\n- **Optimizing machine learning pipelines** with Ray\\n- Implementing **distributed training** for deep learning models\\n- Leveraging Ray\u2019s **advanced libraries** (such as Ray Train and Ray Tune) for scalable, production-ready AI\\n\\nStay tuned\u2014and I hope this post has been a helpful introduction to getting started with Ray!\\n\\n### References\\n\\n- [Ray Documentation](https://docs.ray.io/en/latest/)\\n- [Ray Datasets API](https://docs.ray.io/en/latest/data/dataset.html)\\n- [Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview)\\n- [Ray on Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html)\\n- [Ray Datasets for large-scale machine learning ingest and scoring](https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring)\\n- [Getting Started with Ray Core](https://maxpumperla.com/learning_ray)\\n- [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html)"},{"id":"improve-rag-systems-reliability-with-citations","metadata":{"permalink":"/blog/improve-rag-systems-reliability-with-citations","source":"@site/blog/2025-03-25-improve-rag-systems-reliability-with-citations/index.md","title":"Building Trustworthy RAG Systems with In Text Citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"rag","permalink":"/blog/tags/rag"},{"inline":true,"label":"retrieval-augmented-generation","permalink":"/blog/tags/retrieval-augmented-generation"},{"inline":true,"label":"rag-pipelines","permalink":"/blog/tags/rag-pipelines"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"},{"inline":true,"label":"generative-ai","permalink":"/blog/tags/generative-ai"},{"inline":true,"label":"explainable-ai","permalink":"/blog/tags/explainable-ai"},{"inline":true,"label":"ai-for-research","permalink":"/blog/tags/ai-for-research"},{"inline":true,"label":"citation-generation","permalink":"/blog/tags/citation-generation"},{"inline":true,"label":"langchain","permalink":"/blog/tags/langchain"},{"inline":true,"label":"llamaindex","permalink":"/blog/tags/llamaindex"},{"inline":true,"label":"gemini-api","permalink":"/blog/tags/gemini-api"},{"inline":true,"label":"google-genai","permalink":"/blog/tags/google-genai"},{"inline":true,"label":"trustworthy-ai","permalink":"/blog/tags/trustworthy-ai"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"semantic-search","permalink":"/blog/tags/semantic-search"},{"inline":true,"label":"python","permalink":"/blog/tags/python"}],"readingTime":16.675,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Building Trustworthy RAG Systems with In Text Citations","slug":"improve-rag-systems-reliability-with-citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","authors":["haruiz"],"tags":["rag","retrieval-augmented-generation","rag-pipelines","llms","generative-ai","explainable-ai","ai-for-research","citation-generation","langchain","llamaindex","gemini-api","google-genai","trustworthy-ai","machine-learning","semantic-search","python"]},"unlisted":false,"prevItem":{"title":"Getting Started with Ray on Google Cloud Platform","permalink":"/blog/getting-started-with-ray-on-google-cloud-platform"},"nextItem":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","permalink":"/blog/intro-to-system-design"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nRetrieval-Augmented Generation (RAG) has emerged as a promising method to address the \u201challucination\u201d problem in large language models (LLMs) by grounding responses in external, traceable knowledge sources <a href=\\"#arvix:1\\">[1]</a>\\n. By integrating retrieval mechanisms with generative capabilities, RAG systems produce more accurate, informative, and up-to-date outputs, making them especially powerful for question-answering and content creation tasks. However, a critical but often overlooked aspect is the ability to attribute claims to their original sources. Without proper citations, RAG becomes a \\"black box,\\" undermining the trustworthiness and verifiability of its responses. While much of the existing work has focused on enhancing response quality, less attention has been paid to source attribution<a href=\\"#arvix:2\\">[2]</a>.\\n\\n:::tip\\n## In This Post You Will Learn\\n\\n- Why **citations are critical** in RAG systems for improving trust, traceability, and reducing hallucinations.\\n- **Some approaches for implementing citations in content generation and RAG pipelines:** source-aware generation, inline citations, post-hoc attribution, and more.\\n- **How to build and implement in-line-text citation-aware RAG pipelines using:**\\n    - **LangChain** + FAISS for LLM-backed scholarly QA.\\n    - **LlamaIndex** for structured workflows and granular citations.\\n    - **Google\u2019s Gemini API** with Google Search grounding for real-time web references.\\n:::\\n\\n## Why are Citations Essential?\\n\\nRetrieving external context is a proven strategy for reducing hallucinations and enhancing the reliability of generative AI outputs. However, surfacing relevant documents is only part of the equation\u2014explicitly citing sources is critical for building user trust and enabling verification. Much of the existing research has focused on citation correctness\u2014whether the cited document supports the claim\u2014but correctness alone isn\u2019t sufficient. To establish true credibility, we also need citation faithfulness, which ensures the cited content accurately reflects the intended meaning of the generated response.\\n\\nTogether, these two dimensions\u2014correctness and faithfulness\u2014are foundational to the trustworthiness, transparency, and usability of RAG systems. They support a range of key benefits:\\n\\n- **Verifiability:** Allow users to trace claims back to original documents.\\n- **Trust:** Build user confidence by grounding outputs in identifiable sources.\\n- **Transparency and Explainability:** Help users understand where information comes from.\\n- **Debugging and Improvement:** Make it easier to audit and correct flawed generations.\\n- **Reduced Hallucinations:** Anchor responses in concrete evidence.\\n- **Respecting Intellectual Property:** Ensure proper attribution to original authors and sources.\\n\\nWhile citations don\u2019t guarantee the elimination of hallucinations, they play a crucial role in mitigating them by offering a transparent path to verify information and understand its original context. In the paper \u201cCorrectness is not Faithfulness in RAG Attributions\u201d <a href=\\"#arvix:2\\">[2]</a>, the authors emphasize the importance of distinguishing between citation correctness (does the source support the claim?) and citation faithfulness (does the citation reflect the actual meaning?). Their work calls for more nuanced evaluation metrics that go beyond simple correctness to better assess the quality and reliability of AI-generated citations.\\n\\nThis distinction is illustrated in the following figure:\\n\\n![Correctness vs. Faithfulness in RAG Attributions](figure.png)\\n*(a) Faithful citation: The model generates the correct answer (\\"Berlin is Germany\'s capital\\") based on a relevant retrieved document (\\"Berlin : German capital\\") and correctly cites that document. (b) Citing related context: The model generates the correct answer, likely based on the relevant document or its memory, but incorrectly cites a related but non-supporting retrieved document (\\"Bonn: no longer capital\\").(c) Correct but unfaithful: The model generates the correct answer using its internal memory, not the retrieved documents, but still cites a retrieved document (\\"Berlin : German capital\\") that happens to support the answer. The citation is unfaithful because the cited source wasn\'t the basis for the generation. (d) Incorrect citation: The model generates the correct answer, likely from memory, but incorrectly cites a retrieved document containing false information (\\"Bonn : German capital\\").*\\n\\n\\n## Implementing Citations in RAG Systems \\n\\nCommon approaches to implementing citations in Retrieval-Augmented Generation (RAG) systems include source-aware generation, highlight-based attribution, post-hoc attribution, inline citations, and aggregated source lists. <a href=\\"#arvix:1\\">[1]</a><a href=\\"#arvix:1\\">[2]</a>\\n\\n- **In source-aware generation**, the model is specifically designed or trained to associate facts with their respective source documents during answer generation. This may involve fine-tuning the model on examples that include citations or labeling retrieved text in the prompt to allow the model to reference those labels.\\n\\n- **Highlight-based** attribution visually connects parts of the output to supporting sources using cues such as color-coding or tooltips. While this method enhances clarity, it requires precise alignment. Instead of merely inserting reference numbers, the system highlights sections of the answer to indicate their sources. For instance, certain sentences may be color-coded or underlined, with hover actions revealing excerpts from the original documents.\\n\\n- **Post-hoc attribution** works by generating the answer first and adding citations afterward. In this strategy, the RAG system produces a response without citations and subsequently searches the retrieved documents for evidence to support each statement, integrating citations into the final output.\\n\\n- **Inline citation** involves embedding references, such as \u201c[1],\u201d directly within the text, thereby improving traceability while necessitating well-structured prompts or additional training. This method is typically implemented by directing the model to insert citations during response generation. For example, a prompt might instruct the model to \u201cinclude source numbers in brackets for the information you use,\u201d leading to sentences that end with references like \u201c[1]\u201d or \u201c[2]\u201d corresponding to the source documents. This live citing method (sometimes termed \u201cpre-hoc\u201d) treats citations as an integral part of the answer.\\n\\n- **Aggregated source** lists provide a summary of all sources utilized without integrating citations directly into the text. In this approach, the RAG system presents the answer followed by a bullet list or section titled \\"Sources,\\" detailing all relevant documents that informed the answer, though it does not specify which fact corresponds to which source within the text.\\n\\nIn the following sections, we will explore three distinct approaches for implementing RAG applications with In-line citations using 1) Google\u2019s Generative AI SDK, Gemini and Google Search grounding, 2) LangChain, and 3) LlamaIndex. Each method offers unique features and trade-offs tailored to different use cases and preferences.\\n\\n## Hands-on Implementations (In-text) Citations\\n\\n### LangChain example \\n\\nIn the following example, we implement a pipeline that searches, downloads, semantically processes, and queries scientific papers from arXiv, leveraging vector search and a Large Language Model (LLM) to generate contextual responses with citations:\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport itertools\\nimport typing\\nfrom pathlib import Path\\nimport arxiv\\nimport os\\nfrom dotenv import load_dotenv\\n\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_experimental.text_splitter import SemanticChunker\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom rich import print\\n\\n# Load environment variables from .env file\\nload_dotenv()\\n\\n# Constants\\nFAISS_INDEX_PATH = Path(\\"faiss_index\\")\\nDOWNLOAD_FOLDER = Path(\\"downloads\\")\\n\\n\\ndef fetch_arxiv_papers(query: str, max_results: int = 5) -> list[arxiv.Result]:\\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\\n    return list(arxiv.Client().results(search))\\n\\n\\ndef process_papers(results: list[arxiv.Result], download_folder: Path) -> list:\\n    openai_embeddings = OpenAIEmbeddings()\\n    text_splitter = SemanticChunker(embeddings=openai_embeddings)\\n    download_folder.mkdir(exist_ok=True, parents=True)\\n    documents = []\\n\\n    for index, result in enumerate(results):\\n        pdf_path = result.download_pdf(dirpath=str(download_folder))\\n        pdf_docs = PyPDFLoader(pdf_path).load_and_split(text_splitter)\\n\\n        for doc in pdf_docs:\\n            doc.metadata.update({\\n                \\"title\\": result.title,\\n                \\"authors\\": [author.name for author in result.authors],\\n                \\"entry_id\\": result.entry_id.split(\'/\')[-1],\\n                \\"year\\": result.published.year,\\n                \\"url\\": result.entry_id,\\n                \\"ref\\": f\\"[{index + 1}]\\"\\n            })\\n        documents.extend(pdf_docs)\\n\\n    return documents\\n\\n\\ndef load_or_create_faiss_index(query: str, index_path: Path, download_folder: Path) -> FAISS:\\n    openai_embeddings = OpenAIEmbeddings()\\n\\n    if index_path.exists():\\n        print(\\"Loading existing FAISS index...\\")\\n        return FAISS.load_local(str(index_path), openai_embeddings, allow_dangerous_deserialization=True)\\n\\n    print(\\"Creating a new FAISS index...\\")\\n    documents = process_papers(fetch_arxiv_papers(query), download_folder)\\n    faiss_index = FAISS.from_documents(documents, openai_embeddings)\\n    faiss_index.save_local(str(index_path))\\n    return faiss_index\\n\\n\\ndef search_similar_documents(faiss_index: FAISS, query: str, top_k: int = 20) -> list:\\n    return faiss_index.similarity_search(query, k=top_k)\\n\\n\\ndef generate_response(context_docs: list, question: str) -> str:\\n    sorted_docs = sorted(context_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\"))\\n    formatted_context = [\\n        f\\"{doc.metadata[\'ref\']} {doc.metadata[\'title\']}: {doc.page_content}\\" for doc in sorted_docs\\n    ]\\n\\n    prompt_template = PromptTemplate(\\n        template=\\"\\"\\"\\n        Write a blog post based on the user query.\\n        When referencing information from the context, cite the appropriate source(s) using their corresponding numbers.\\n        Each source has been provided with a number and a title.\\n        Every answer should include at least one source citation.\\n        If none of the sources are helpful, indicate that.\\n\\n        ------\\n        {context}\\n        ------\\n        Query: {query}\\n        Answer:\\n        \\"\\"\\",\\n        input_variables=[\\"query\\", \\"context\\"]\\n    )\\n\\n    model = ChatOpenAI(model=\\"gpt-4o-mini\\")\\n    parser = StrOutputParser()\\n    chain = prompt_template | model | parser\\n\\n    return chain.invoke({\\"context\\": formatted_context, \\"query\\": question})\\n\\n\\ndef main():\\n    query = \\"hallucination in LLMs\\"\\n    question = \\"How to mitigate hallucination ?\\"\\n\\n    faiss_index = load_or_create_faiss_index(query, FAISS_INDEX_PATH, DOWNLOAD_FOLDER)\\n    relevant_docs = search_similar_documents(faiss_index, question)\\n    response = generate_response(relevant_docs, question)\\n\\n    print(\\"\\\\nGenerated Response:\\\\n\\", response)\\n\\n    bibliography = \\"\\\\n\\\\n### References\\\\n\\"\\n    sorted_docs = sorted(relevant_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\"))\\n\\n    for doc_key, documents in itertools.groupby(sorted_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\")):\\n        doc = next(documents)\\n        bibliography += (\\n            f\\"{doc.metadata.get(\'ref\', \'Unknown\')} {doc.metadata.get(\'title\', \'Unknown\')}, \\"\\n            f\\"{\', \'.join(doc.metadata.get(\'authors\', \'Unknown\'))}, arXiv {doc.metadata.get(\'entry_id\', \'Unknown\')}, \\"\\n            f\\"{doc.metadata.get(\'year\', \'Unknown\')}. {doc.metadata.get(\'url\', \'Unknown\')}\\\\n\\"\\n        )\\n    print(bibliography)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\n\\nThis workflow uses LangChain, OpenAI\'s models, FAISS for vector storage, and arXiv\'s Python client. Let\u2019s break it down:\\n\\n### Key Steps:\\n\\n#### 1. **Environment Setup and Imports**\\nThe code imports standard libraries (`os`, `itertools`, `pathlib`, etc.), third-party tools (`arxiv`, `dotenv`), and key LangChain modules for PDF loading, text splitting, embeddings, LLM interaction, and vector storage. It also loads API keys securely via environment variables.\\n\\n#### 2. **Query arXiv for Relevant Papers**\\n```python\\nfetch_arxiv_papers()\\n```\\nUses the `arxiv` Python package to search for academic papers matching a query string. Returns a list of arXiv `Result` objects.\\n\\n#### 3. **Download and Process Papers**\\n```python\\nprocess_papers()\\n```\\nDownloads the PDF files, splits them into semantically meaningful text chunks using OpenAI embeddings, and attaches rich metadata such as title, authors, publication year, and a reference number.\\n\\n#### 4. **Create or Load FAISS Index**\\n```python\\nload_or_create_faiss_index()\\n```\\nChecks whether a local FAISS index already exists. If not, it creates one by embedding the paper chunks and saving the index locally.\\n\\n#### 5. **Semantic Search over the Indexed Chunks**\\n```python\\nsearch_similar_documents()\\n```\\nTakes a user query and searches for the most semantically similar document chunks using vector similarity (k-nearest neighbors search).\\n\\n#### 6. **Generate an LLM-Based Answer with Citations**\\n```python\\ngenerate_response()\\n```\\nUses a prompt template and an OpenAI Chat model (`gpt-4o-mini`) to generate a response grounded in the top retrieved documents, including source references inline using numbered citations.\\n\\n#### 7. **Main Program Logic**\\n```python\\nmain()\\n```\\nPuts all the above steps together to:\\n- Search and download papers about *hallucinations in LLMs*\\n- Answer the question: *How to mitigate hallucination?*\\n- Print the generated response\\n- Print a formatted bibliography of the cited papers\\n\\n### LlamaIndex example <a href=\\"#llamaindex:1\\">[3]</a>\\n\\nFollowing with hands-on implementations, let\'s explore how to build a citation-aware query engine using LlamaIndex. This implementation starts by retrieving relevant text chunks from a set of documents, splits them into citable segments, and uses a Large Language Model (LLM) to synthesize a well-cited answer to a given query.\\n\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport asyncio\\nimport logging\\nfrom typing import List, Union\\n\\nfrom dotenv import load_dotenv\\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\\nfrom llama_index.core.workflow import Context, Workflow, StartEvent, StopEvent, step, Event\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.embeddings.openai import OpenAIEmbedding\\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, TextNode\\nfrom llama_index.core.response_synthesizers import ResponseMode, get_response_synthesizer\\nfrom llama_index.core.node_parser import SentenceSplitter\\nfrom llama_index.core.prompts import PromptTemplate\\n\\n# Load environment variables\\nload_dotenv(verbose=True)\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Prompt templates for citation-based QA\\nCITATION_QA_TEMPLATE = PromptTemplate(\\n    \\"Please provide an answer based solely on the provided sources. \\"\\n    \\"When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. \\"\\n    \\"Every answer should include at least one source citation. \\"\\n    \\"Only cite a source when you are explicitly referencing it. \\"\\n    \\"If none of the sources are helpful, you should indicate that. \\\\n\\"\\n    \\"Example:\\\\n\\"\\n    \\"Source 1:\\\\nThe sky is red in the evening and blue in the morning.\\\\n\\"\\n    \\"Source 2:\\\\nWater is wet when the sky is red.\\\\n\\"\\n    \\"Query: When is water wet?\\\\n\\"\\n    \\"Answer: Water will be wet when the sky is red [2], which occurs in the evening [1].\\\\n\\"\\n    \\"Now it\'s your turn. Below are several numbered sources:\\\\n\\"\\n    \\"{context_str}\\\\nQuery: {query_str}\\\\nAnswer: \\"\\n)\\n\\nCITATION_REFINE_TEMPLATE = PromptTemplate(\\n    \\"Please refine the existing answer based solely on the provided sources. \\"\\n    \\"Cite sources where necessary, following this format:\\\\n\\"\\n    \\"Example:\\\\n\\"\\n    \\"Existing answer: {existing_answer}\\\\n\\"\\n    \\"{context_msg}\\\\n\\"\\n    \\"Query: {query_str}\\\\nRefined Answer: \\"\\n)\\n\\nDEFAULT_CITATION_CHUNK_SIZE = 512\\nDEFAULT_CITATION_CHUNK_OVERLAP = 20\\n\\n\\nclass RetrieverEvent(Event):\\n    \\"\\"\\"Event triggered after document retrieval.\\"\\"\\"\\n\\n    nodes: List[NodeWithScore]\\n\\n\\nclass CreateCitationsEvent(Event):\\n    \\"\\"\\"Event triggered after creating citations.\\"\\"\\"\\n\\n    nodes: List[NodeWithScore]\\n\\n\\nclass CitationQueryEngineWorkflow(Workflow):\\n    \\"\\"\\"Workflow for processing queries with retrieval-augmented generation (RAG).\\"\\"\\"\\n\\n    @step\\n    async def retrieve(self, ctx: Context, ev: StartEvent) -> Union[RetrieverEvent, None]:\\n        \\"\\"\\"Retrieve relevant nodes based on the query.\\"\\"\\"\\n        query = ev.get(\\"query\\")\\n        if not query:\\n            logger.warning(\\"No query provided.\\")\\n            return None\\n\\n        logger.info(f\\"Querying database: {query}\\")\\n\\n        await ctx.set(\\"query\\", query)\\n\\n        if ev.index is None:\\n            logger.error(\\"Index is empty. Load documents before querying!\\")\\n            return None\\n\\n        retriever = ev.index.as_retriever(similarity_top_k=2)\\n        nodes = retriever.retrieve(query)\\n\\n        logger.info(f\\"Retrieved {len(nodes)} nodes.\\")\\n        return RetrieverEvent(nodes=nodes)\\n\\n    @step\\n    async def create_citation_nodes(self, ev: RetrieverEvent) -> CreateCitationsEvent:\\n        \\"\\"\\"Create granular citation nodes from retrieved text chunks.\\"\\"\\"\\n        nodes = ev.nodes\\n        new_nodes: List[NodeWithScore] = []\\n\\n        text_splitter = SentenceSplitter(\\n            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\\n            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\\n        )\\n\\n        for node in nodes:\\n            text_chunks = text_splitter.split_text(\\n                node.node.get_content(metadata_mode=MetadataMode.NONE)\\n            )\\n\\n            for idx, text_chunk in enumerate(text_chunks, start=len(new_nodes) + 1):\\n                text = f\\"Source {idx}:\\\\n{text_chunk}\\\\n\\"\\n\\n                new_node = NodeWithScore(\\n                    node=TextNode.model_validate(node.node), score=node.score\\n                )\\n                new_node.node.text = text\\n                new_nodes.append(new_node)\\n\\n        logger.info(f\\"Created {len(new_nodes)} citation nodes.\\")\\n        return CreateCitationsEvent(nodes=new_nodes)\\n\\n    @step\\n    async def synthesize(self, ctx: Context, ev: CreateCitationsEvent) -> StopEvent:\\n        \\"\\"\\"Generate an AI response based on retrieved citations.\\"\\"\\"\\n        llm = OpenAI(model=\\"gpt-4o-mini\\")\\n        query = await ctx.get(\\"query\\", default=None)\\n\\n        synthesizer = get_response_synthesizer(\\n            llm=llm,\\n            text_qa_template=CITATION_QA_TEMPLATE,\\n            refine_template=CITATION_REFINE_TEMPLATE,\\n            response_mode=ResponseMode.COMPACT,\\n            use_async=True,\\n        )\\n\\n        response = await synthesizer.asynthesize(query, nodes=ev.nodes)\\n        return StopEvent(result=response)\\n\\n\\nasync def run_workflow():\\n    \\"\\"\\"Initialize the index and run the query workflow.\\"\\"\\"\\n    logger.info(\\"Loading documents...\\")\\n    documents = SimpleDirectoryReader(\\"downloads\\").load_data()\\n\\n    index = VectorStoreIndex.from_documents(\\n        documents=documents,\\n        embed_model=OpenAIEmbedding(model_name=\\"text-embedding-3-small\\"),\\n    )\\n\\n    logger.info(\\"Running citation query workflow...\\")\\n    workflow = CitationQueryEngineWorkflow()\\n    result = await workflow.run(query=\\"Write a blog post about agents?\\", index=index)\\n\\n    bibliography = \\"\\\\n\\\\n### References\\\\n\\"\\n    for node in result.source_nodes:\\n        bibliography += f\\"{node.get_text()}\\\\n\\"\\n    print(bibliography)\\n\\n    return result\\n\\n\\nif __name__ == \\"__main__\\":\\n    result = asyncio.run(run_workflow())\\n    print(result)\\n```\\n\\n### Key Steps:\\n\\n#### 1. **Environment Setup and Logging Configuration**\\nThe code loads environment variables from `.env` using `dotenv`, and configures logging to help track events during the workflow execution.\\n\\n```python\\nload_dotenv(verbose=True)\\nlogging.basicConfig(level=logging.INFO)\\n```\\n\\n#### 2. **Define Prompt Templates**\\nTwo prompt templates are defined for instructing the LLM:\\n- `CITATION_QA_TEMPLATE`: Generates answers with numbered citations based on the provided context.\\n- `CITATION_REFINE_TEMPLATE`: Refines an existing answer with additional citation context.\\n\\nThese templates ensure the model cites only relevant sources and produces trustworthy output.\\n\\n#### 3. **Declare Custom Events**\\nCustom `Event` classes (`RetrieverEvent`, `CreateCitationsEvent`) are defined to structure the flow of data across the steps of the workflow.\\n\\n\\n#### 4. **Build the Citation Query Workflow**\\n`CitationQueryEngineWorkflow` is a subclass of `Workflow` with three main `@step`s:\\n\\n- **`retrieve()`**:  \\n  Retrieves top-k relevant document nodes from a vector index based on the user\'s query using similarity search.\\n\\n- **`create_citation_nodes()`**:  \\n  Splits the retrieved chunks into smaller, clearly numbered sources (e.g., `Source 1`, `Source 2`), ensuring each text chunk can be referenced independently.\\n\\n- **`synthesize()`**:  \\n  Generates a final, citation-rich response using the GPT-4o-mini model via the LlamaIndex synthesizer tools.\\n\\n#### 5. **Execute the Workflow**\\nThe `run_workflow()` function loads all documents from the `downloads/` directory and builds a `VectorStoreIndex` using OpenAI embeddings. It then runs the query engine on the question:\\n**\\"Write a blog post about agents?\\"**\\n\\nFinally, it prints both the result and a nicely formatted bibliography of all cited text chunks.\\n\\n\\n### Using Grounding with Google Search in the Gemini API example\\n\\nFinally, in this last example, you will learn how to leverage Google Search capabilities within the Gemini API to generate content with inline citations. This approach combines the power of Gemini\u2019s 2.0 with real-time web search to produce informative, grounded responses with proper attributions.\\n\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport itertools\\nfrom typing import Optional\\n\\nfrom dotenv import load_dotenv\\nfrom pydantic import BaseModel\\nfrom rich import print\\n\\nfrom google import genai\\nfrom google.genai.types import (\\n    GenerateContentConfig,\\n    Tool,\\n    GoogleSearch,\\n    GroundingChunk\\n)\\n\\nload_dotenv()\\n\\nclass Citation(BaseModel):\\n    \\"\\"\\"Represents a citation extracted from the Gemini grounding metadata.\\"\\"\\"\\n    title: str\\n    score: float\\n    link: str\\n    chunk_index: int\\n    chunk_text: Optional[str] = None\\n    start_index: Optional[int] = None\\n    end_index: Optional[int] = None\\n\\n\\ndef generate_content(prompt: str, model: str) -> genai.types.GenerateContentResponse:\\n    client = genai.Client()\\n    return client.models.generate_content(\\n        model=model,\\n        contents=prompt,\\n        config=GenerateContentConfig(\\n            response_modalities=[\\"TEXT\\"],\\n            tools=[Tool(google_search=GoogleSearch())],\\n        ),\\n    )\\n\\n\\ndef extract_citations(response: genai.types.GenerateContentResponse) -> list[Citation]:\\n    citations = []\\n    grounding_metadata = response.candidates[0].grounding_metadata\\n    for support in grounding_metadata.grounding_supports:\\n        for idx, score in zip(support.grounding_chunk_indices, support.confidence_scores):\\n            chunk: GroundingChunk = grounding_metadata.grounding_chunks[idx]\\n            citations.append(\\n                Citation(\\n                    title=chunk.web.title,\\n                    link=chunk.web.uri,\\n                    score=score,\\n                    chunk_index=idx,\\n                    chunk_text=support.segment.text,\\n                    start_index=support.segment.start_index,\\n                    end_index=support.segment.end_index,\\n                )\\n            )\\n    return citations\\n\\n\\ndef inject_citations_into_text(text: str, citations: list[Citation]) -> str:\\n    citations.sort(key=lambda x: (x.start_index, x.end_index))\\n    offset = 0\\n    for (start, end), group in itertools.groupby(citations, key=lambda x: (x.start_index, x.end_index)):\\n        group_list = list(group)\\n        indices = \\",\\".join(str(c.chunk_index + 1) for c in group_list)\\n        citation_str = f\\"[{indices}]\\"\\n        text = text[:end + offset] + citation_str + text[end + offset:]\\n        offset += len(citation_str)\\n    return text\\n\\n\\ndef format_citation_section(citations: list[Citation]) -> str:\\n    result = \\"\\\\n\\\\n**Citations**\\\\n\\\\n\\"\\n    sorted_citations = sorted(citations, key=lambda x: x.chunk_index)\\n    for chunk_index, group in itertools.groupby(sorted_citations, key=lambda x: x.chunk_index):\\n        citation = list(group)[0]\\n        result += f\\"[{chunk_index + 1}] {citation.title} - {citation.link}\\\\n\\"\\n    return result\\n\\n\\ndef main():\\n    MODEL_NAME = \\"gemini-2.0-flash\\"\\n    response = generate_content(\\"Write a blog post about Agents\\", MODEL_NAME)\\n    citations = extract_citations(response)\\n\\n    generated_text = response.text\\n    final_text = inject_citations_into_text(generated_text, citations)\\n    final_text += format_citation_section(citations)\\n    print(final_text)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n```\\n\\n### Key Steps:\\n\\n#### 1. **Set Up and Define the Citation Schema**\\n```python\\nclass Citation(BaseModel)\\n```\\nDefines a schema for handling citation information, including title, score, link, and text span indices for precise inline placement.\\n\\n#### 2. **Call Gemini\u2019s Multimodal API with Google Search Tooling**\\n```python\\ngenerate_content()\\n```\\nThis function generates content using Gemini (e.g., `gemini-2.0-flash`) and includes a `GoogleSearch` tool, enabling grounded web references.\\n\\n#### 3. **Extract Grounded Citations**\\n```python\\nextract_citations()\\n```\\nPulls out metadata like source titles, URLs, and confidence scores from the model\'s response using Gemini\u2019s `grounding_supports`.\\n\\n#### 4. **Inject Inline Citations**\\n```python\\ninject_citations_into_text()\\n```\\nAdds numbered citation markers like `[1]`, `[2]`, etc., directly into the generated text using the start and end positions returned by Gemini.\\n\\n\\n#### 5. **Format the Citation Section**\\n```python\\nformat_citation_section()\\n```\\nGenerates a clean, markdown-style bibliography list at the end of the post, matching each inline marker with its source.\\n\\n#### 6. **Main Execution**\\n```python\\nmain()\\n```\\nCombines everything: generates content about \\"Agents\\", extracts and injects citations, and prints the final, publication-ready blog post with proper attributions.\\n\\n## Key Differences and Summary\\n\\n- **Grounding with Google Search in the Gemini API** enhances the accuracy and freshness of model responses by leveraging Google\u2019s real-time search and grounding capabilities. It\u2019s straightforward to implement but is tightly integrated with Google\u2019s Gemini models family.\\n\\n- **LangChain:** Provides a modular framework for building RAG pipelines. Offers flexibility in choosing components (document loaders, text splitters, vector stores, LLMs). Requires more manual setup, but allows for greater customization. Focuses on creating a reference string in the metadata and using that in the prompt.\\n- **LlamaIndex:** Offers higher-level abstractions for RAG, including workflows and specialized components for citation handling. Emphasizes creating granular citation nodes for precise referencing. Uses very explicit prompt templates to guide the LLM.\\n\\nAll three approaches achieve the same goal \u2013 generating responses with citations \u2013 but they use different mechanisms and levels of abstraction. The choice of which to use depends on your specific needs and preferences. The most important common thread is the careful management of metadata to track the source of information.\\n\\n## Conclusion\\n\\nIn the era of AI-generated content, trust is everything. Retrieval-Augmented Generation has unlocked new levels of intelligence and context-awareness, but without clear, faithful citations, even the most accurate answers remain suspect. Citations are not just a safeguard against hallucinations\u2014they are the bridge between AI and human understanding, offering transparency, accountability, and traceability. This post has walked through practical, hands-on implementations using LangChain, LlamaIndex, and Google\u2019s Gemini API to demonstrate that citation-aware RAG isn\'t just a research ideal\u2014it\u2019s an achievable standard. As builders and researchers, the responsibility is ours to push beyond plausible-sounding responses and deliver outputs that are grounded, explainable, and verifiably true. The future of reliable AI starts with showing your sources.\\n\\nAll the code snippets and examples in this post are available on GitHub following this link:\\n[llm-app-patterns](https://github.com/haruiz/llm-app-patterns/tree/main)\\n\\n\\n**References**\\n<ul>\\n <li><a id=\\"arvix:1\\" href=\\"https://arxiv.org/abs/2410.11217\\" target=\\"_blank\\">[1] Qian, H., Fan, Y., Zhang, R., & Guo, J. (2024, October 15). On the Capacity of Citation Generation by Large Language Models</a></li>\\n <li><a id=\\"arvix:2\\" href=\\"https://arxiv.org/abs/2412.18004\\" target=\\"_blank\\">[2] Wallat, J., Heuss, M., Maarten, D. R., & Anand, A. (2024, December 23). Correctness is not Faithfulness in RAG Attributions</a></li>\\n<li>\\n<a id=\\"llamaindex:1\\" href=\\"https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/\\" target=\\"_blank\\">[3] LlamaIndex Documentation: Build RAG with in-line citations</a>\\n</li>\\n</ul>"},{"id":"intro-to-system-design","metadata":{"permalink":"/blog/intro-to-system-design","source":"@site/blog/2024-03-18-intro-to-system-design/index.mdx","title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","description":"System design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.","date":"2024-03-18T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":34.7,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","slug":"intro-to-system-design","description":"System design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Building Trustworthy RAG Systems with In Text Citations","permalink":"/blog/improve-rag-systems-reliability-with-citations"},"nextItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\n\\n## Introduction\\n\\nSystem design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. It is a multidisciplinary field that requires a wide range of skills and knowledge, including software engineering, computer science, network engineering, and project management. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.\\n\\n\\n## System design process overview\\n\\n### 1. Defining the requirements of the system\\n\\nDefining the requirements of the system is the first step in the system design process, and it involves gathering, analyzing, and documenting the requirements for the system. The requirements analysis phase is crucial, as it provides the foundation for the rest of the system design process. It helps to ensure that the system will meet the needs of its users and stakeholders and that it will be developed within the constraints of time, budget, and resources. The requirements analysis phase typically involves the following activities:\\n\\n1. **Gathering Requirements**: This involves collecting information about the needs, goals, and constraints of the system from its users and stakeholders. This information can be gathered through interviews, surveys, questionnaires, and workshops.\\n\\n2. **Analyzing Requirements**: This involves analyzing the gathered information to identify the key features, functions, and constraints of the system. It also involves identifying any conflicts or inconsistencies in the requirements.\\n\\n3. **Documenting Requirements**: This involves documenting the requirements in a clear, concise, and unambiguous manner. The requirements should be documented in a way that is understandable to all stakeholders, including developers, testers, and project managers.\\n\\n4. **Validating Requirements**: This involves validating the requirements with the users and stakeholders to ensure that they accurately reflect their needs and goals. It also involves ensuring that the requirements are complete, consistent, and feasible.\\n\\n5. **Managing Requirements**: This involves managing changes to the requirements throughout the system design process. It involves tracking changes, resolving conflicts, and ensuring that the requirements are kept up-to-date.\\n\\n#### Types of Requirements\\n\\nThere are several types of requirements that need to be considered when designing a system. These include:\\n\\n1. **Functional Requirements**: These are the requirements that describe the functions, features, and capabilities of the system. They specify what the system should do, and they are typically expressed as use cases, user stories, or functional specifications.\\n\\n2. **Non-Functional Requirements**: These are the requirements that describe the quality attributes of the system, such as performance, reliability, availability, security, and usability. They specify how well the system should perform, and they are typically expressed as performance requirements, security requirements, and usability requirements.\\n\\n3. **Business Requirements**: These are the requirements that describe the business goals, objectives, and constraints of the system. They specify why the system is being developed, and they are typically expressed as business cases, business rules, and business process models.\\n\\n4. **User Requirements**: These are the requirements that describe the needs, goals, and constraints of the users of the system. They specify who will use the system, and they are typically expressed as user profiles, user scenarios, and user interface designs.\\n\\n5. **System Requirements**: These are the requirements that describe the technical constraints and dependencies of the system. They specify how the system will be developed, deployed, and maintained, and they are typically expressed as system architecture, system interfaces, and system dependencies.\\n\\n6. **Regulatory Requirements**: These are the requirements that describe the legal, ethical, and regulatory constraints of the system. They specify how the system should comply with laws, regulations, and standards, and they are typically expressed as compliance requirements, privacy requirements, and security requirements.\\n\\n7. **Data Requirements**: These are the requirements that describe the data needs, constraints, and dependencies of the system. They specify what data the system will use, store, and process, and they are typically expressed as data models, data flows, and data storage.\\n\\n### 2. Selecting the appropriate methodology\\n\\nLike any other software solution, ML systems require a well-structured methodology to maximize the success rate of the implementation. ML algorithms are the less challenging part. The hard part is making algorithms work with other software components to solve real-world problems.\\n\\nThere are several **software development methodologies** that can be used to develop a ML system, such as the waterfall model, the agile model, and the iterative model. Each of these methodologies has its own strengths and weaknesses, and they are suitable for different types of projects and teams. These methodologies also provided a framework for gathering, analyzing, and documenting the requirements the system.\\n\\n1. **Waterfall Model**: The waterfall model is a linear and sequential software development methodology that divides the development process into distinct phases, such as requirements analysis, design, implementation, testing, and maintenance. Each phase must be completed before the next phase can begin, and the process is difficult to change once it has started. The waterfall model is suitable for projects with well-defined requirements and stable technologies, but it is not suitable for projects with changing requirements and emerging technologies.\\n\\n2. **Agile Model**: The agile model is an iterative and incremental software development methodology that focuses on delivering working software in short iterations, typically two to four weeks. It emphasizes collaboration, flexibility, and customer feedback, and it is suitable for projects with changing requirements and emerging technologies. The agile model is based on the principles of the Agile Manifesto, which emphasizes individuals and interactions, working software, customer collaboration, and responding to change.\\n\\n3. **Iterative Model**: The iterative model is a software development methodology that divides the development process into small, incremental, and iterative cycles, each of which produces a working prototype of the system. The iterative model is suitable for projects with evolving requirements and complex technologies, and it is based on the principles of the spiral model, which emphasizes risk management, prototyping, and incremental development.\\n\\n#### Adaptations for ML and Data Science Projects\\n\\n**Considerations**\\n\\nWhile these methodologies offer frameworks for managing work, ML projects may require specific adaptations:\\n\\n1. **Iterative Experimentation:** Embrace the iterative nature of ML, where initial models often serve as baselines for further experimentation and refinement.\\n\\n2. **Flexible Planning:** Allow for adjustments in project scope and direction based on intermediate results and discoveries.\\n\\n3. **Model Versioning and Experiment Tracking:** Implement tools and practices for tracking different model versions, experiment parameters, and results to ensure reproducibility and facilitate decision-making.\\n\\n4. **Collaboration between Data Scientists and Domain Experts:** Foster close collaboration to ensure that models are developed with a deep understanding of the domain and are aligned with business needs.\\n\\n**Adaptations**\\n\\n1. **Scrum:** Scrum is a popular Agile framework that organizes work into small, manageable pieces delivered in short cycles called sprints, typically lasting 2-4 weeks. For ML projects, Scrum can facilitate rapid experimentation and iteration. Teams can define sprints for different phases of ML development, such as data preparation, model training, evaluation, and deployment. Daily stand-ups can help track progress and address blockers quickly.\\n\\n2. **Kanban:** Kanban emphasizes continuous delivery without overburdening the team, using a visual board to track work items through various stages of completion. In ML projects, Kanban can be used to manage the flow of tasks like data annotation, feature engineering, model experimentation, and performance tuning. Its flexibility is particularly useful for projects where priorities shift frequently based on experimental results or business needs.\\n\\n3. **Extreme Programming (XP):** XP focuses on technical excellence and high customer involvement, with practices like pair programming, test-driven development (TDD), and frequent releases. While some XP practices may not directly translate to ML projects (e.g., TDD is challenging due to the probabilistic nature of ML outcomes), the emphasis on quality and collaboration can be beneficial. For instance, pair programming can be adapted for collaborative model development and code reviews, ensuring high-quality code and model architecture.\\n\\n4. **Lean Development:** Lean development aims to reduce waste and focus on delivering value. For ML projects, this can mean focusing on high-value tasks, such as feature engineering, model experimentation, and deployment, while minimizing time spent on less critical activities. Techniques like value stream mapping can help identify bottlenecks and streamline the ML workflow.\\n\\n5. **Feature-Driven Development (FDD):** FDD is an iterative and incremental approach that focuses on building features in short iterations. For ML projects, this can translate to developing specific features or components of the ML pipeline in short cycles, ensuring that each iteration delivers tangible value.\\n\\nOnce we have gathered, analyzed, and documented the requirements for the system, we can move on to the next phase of the system design process, which is architecturing the system.\\n\\n### 3. Architecturing your solution\\n\\nArchitecturing a system is the act of decomposing a system into multiple building blocks so that we can identify how each building block can be developed, deployed, and maintained to achieve a high level of modularity, flexibility, and scalability. The architecture of a system provides a high-level view of how these components are arranged and interact with each other to achieve the desired functionality and performance. Several architectural styles and patterns can be used to design a system, including the client-server architecture, the microservices architecture, and the event-driven architecture. However, selecting the appropriate architectural style depends on the type of application to be developed and the system\'s requirements.\\n\\nClassifying applications involves categorizing them based on criteria such as functionality, deployment methods, technology used, target user base, and platform. Here, we are providing a classification based on the deployment model.\\n\\n- **Web Applications:** Accessed via web browsers, e.g., Google Docs, Salesforce.\\n- **Desktop Applications:** Installed on a personal computer or laptop, e.g., Microsoft Word, Adobe Photoshop.\\n- **Mobile Applications:** Designed for smartphones and tablets, e.g., Instagram, Uber.\\n- **Cloud Applications:** Hosted on cloud services and accessible over the Internet, e.g., Dropbox, Slack.\\n\\nAn ML model can be deployed in any of these application types. For example, a web application can use machine learning to provide personalized recommendations to users; a desktop application can use a machine learning model to automate repetitive tasks; and a mobile application can use a machine learning model to recognize speech or images, etc.\\n\\n#### Client-\\"Server\\" Architecture\\n\\nFor web applications, cloud applications, and sometimes mobile applications, the **client-server architecture** is a common choice for the system architecture. The client-server architecture is a distributed computing architecture that divides the system into two major components: the client and the server. The client is the end-user device or application that requests and consumes the services provided by the server. The server is the remote computer or service that provides the resources, services, or data to the client. The client-server architecture provides a scalable, flexible, and reliable way to distribute and manage resources and services across a network. This architecture consists of the following components:\\n\\n1. **Client**: The client is a device or a program that requests resources and services from the server. The client can be a web browser, a mobile app, or a desktop application.\\n\\n2. **Server**: The server is a device or a program that provides resources and services to the client. The server can be a web server, an application server, or a database server.\\n\\n3. **Network**: The network is the medium through which the client and server communicate with each other. The network can be a local area network (LAN), a wide area network (WAN), or the internet.\\n\\n4. **Protocol**: The protocol is a set of rules and conventions that govern the communication between the client and server. The protocol can be HTTP, HTTPS, TCP, or UDP.\\n\\nIn software development, the component of your app running in the client side is called the **frontend** and the component running in the server side is called the **backend**. The frontend is responsible for the user interface and user experience, while the backend is responsible for the business logic, data storage, and integration with external systems. The frontend and backend communicate with each other using APIs, such as RESTful APIs, GraphQL APIs, or WebSocket APIs. I will discuss more about APIs and Webservices in a future post.\\n\\n![client-server-architecture](client-server.png)\\n\\nClient-Server-based applications can be deployed in different ways, following different architectural patterns. The most common ones are:\\n\\n#### Server-Based architecture patterns\\n\\n#### Monolithic Architecture\\n\\nThe monolithic architecture is a traditional software architecture pattern that consists of a single, self-contained application that contains all the components, modules, and services of the system. The monolithic architecture is based on the principles of tight coupling, where the components of the system are tightly integrated and dependent on each other. The monolithic architecture is suitable for small to medium-sized applications with simple requirements and low complexity. It is also suitable for applications with stable technologies and well-defined requirements. The monolithic architecture consists of the following components:\\n\\n1. **User Interface**: The user interface is the front-end component of the system that interacts with the user. It can be a web interface, a mobile interface, or a desktop interface.\\n2. **Business Logic**: The business logic is the core component of the system that implements the business rules, processes, and workflows. It can be implemented as a set of classes, functions, or procedures.\\n3. **Data Storage**: The data storage is the back-end component of the system that stores and manages the data. It can be a relational database, a NoSQL database, or a file system.\\n4. **Integration**: The integration is the component of the system that integrates with external systems, services, and APIs. It can be implemented as a set of connectors, adapters, or gateways.\\n\\n#### Microservices Architecture\\n\\nThe microservices architecture is a modern software architecture that consists of a collection of small, independent, and loosely-coupled services that are developed, deployed, and maintained independently. The microservices architecture is based on the principles of loose coupling, where the components of the system are decoupled and independent of each other. The microservices architecture is suitable for large-scale applications with complex requirements and high complexity. It is also suitable for applications with changing requirements and emerging technologies. The microservices architecture consists of the following components:\\n\\n1. **Service**: The service is a small, independent, and loosely-coupled component of the system that provides a specific set of functions and capabilities. It can be implemented as a RESTful API, a message queue, or a microservice.\\n2. **Container**: The container is a lightweight, portable, and self-contained environment that hosts the service. It can be implemented as a Docker container, a Kubernetes pod, or a serverless function.\\n3. **Orchestration**: The orchestration is the component of the system that manages the deployment, scaling, and monitoring of the services. It can be implemented as a container orchestrator, a service mesh, or a serverless platform.\\n4. **Gateway**: The gateway is the component of the system that provides a single entry point for the services. It can be implemented as an API gateway, a message broker, or a load balancer\\n5. **Database**: The database add to each service the capability to store and manage the data. It can be a relational database, a NoSQL database, or a distributed database.\\n\\nThese are the most common architectural patterns for server-based applications. Each pattern has its own strengths and weaknesses, and the choice of pattern depends on the requirements and constraints of the system. However, with the rise of cloud computing and serverless computing, the **serverless architecture** has become an alternative to the develop and deploy applications.\\n\\n#### Serverless-Based Architecture patterns\\n\\n The serverless architecture is a cloud computing model that abstracts the infrastructure and runtime environment from the developer, allowing them to focus on writing code and deploying applications without managing servers. The serverless architecture is based on the principles of event-driven computing, where events, such as HTTP requests, database changes, or file uploads, trigger the execution of code. The serverless architecture consists of the following components:\\n\\n1. **Function**: The function is a small, stateless, and event-driven piece of code that performs a specific task or function. It can be implemented as a serverless function, a lambda function, or a cloud function.\\n2. **Event**: The event is a trigger that initiates the execution of the function. It can be an HTTP request, a database change, or a file upload.\\n3. **Cloud**: The cloud is the infrastructure and runtime environment that hosts and manages the functions. It can be a cloud provider, such as AWS, Azure, or GCP.\\n4. **API**: The API is the interface that exposes the functions to the client. It can be a RESTful API, a GraphQL API, or a message queue.\\n\\n#### Server-based vs Serverless-based architectures\\n\\n- **Monolithic architectures** are best suited for small to medium-sized applications where simplicity and ease of management are key. However, they can become cumbersome to update and scale as the application grows.\\n- **Microservices** offer a highly scalable and flexible architecture that is suitable for complex applications that need to rapidly evolve. They require a significant upfront investment in design and infrastructure management but provide long-term benefits in scalability and maintainability.\\n- **Serverless architectures** abstract the management of servers, making it easier for developers to deploy code that scales automatically with demand. This model is cost-effective for applications with fluctuating workloads but introduces new challenges in managing application state and understanding cloud provider limitations.\\n\\n\\n![Architectural-styles](architectural-styles.png)\\n\\n| Factor | Monolithic | Microservices | Serverless |\\n| :---: | :---: | :---: | :---: |\\n| Definition | A software development <br/> approach where an <br/> application is built as a <br/> single and indivisible unit. | An architecture that <br/> structures an application <br/> as a collection of small, <br/> autonomous services <br/> modeled around a <br/> business domain. | A cloud-computing <br/> execution model where the <br/> cloud provider dynamically <br/> manages the allocation and <br/> provisioning of servers. |\\n| Complexity | Simple to develop and <br/> deploy initially but <br/> becomes more complex <br/> and unwieldy as the <br/> application grows. | Complex to design and <br/> implement due to its <br/> distributed nature, but <br/> easier to manage, <br/> understand, and update <br/> in the long term. | Low operational complexity <br/> for developers as the cloud <br/> provider manages the <br/> infrastructure, but can have <br/> complex architecture <br/> patterns. |\\n| Scalability | Scaling requires <br/> duplicating the entire <br/> application, which can be <br/> inefficient for resources. | Services can be scaled <br/> independently based on <br/> demand, leading to <br/> efficient use of resources. | Automatically scales based <br/> on the workload by running <br/> code in response to events, <br/> without provisioning or <br/> managing servers. |\\n| Development | Development is <br/> straightforward in the <br/> early phases but can slow <br/> down as the application <br/> grows due to tightly <br/> coupled components. | Enables the use of <br/> different technologies <br/> and programming <br/> languages for different <br/> services, potentially <br/> increasing development <br/> speed. | Development focuses on <br/> individual functions, <br/> potentially speeding up <br/> development cycles, but <br/> requires understanding of <br/> serverless patterns and <br/> limits. |\\n| Deployment | Deploying updates <br/> requires redeploying the <br/> entire application, which <br/> can be slow and risky. | Services can be deployed <br/> independently, allowing <br/> for faster and less risky <br/> updates. | Code is deployed to the <br/> cloud provider, which then <br/> takes care of deployment, <br/> scaling, and management, <br/> simplifying deployment <br/> processes. |\\n| Maintenance | Maintenance can be <br/> challenging as fixing a <br/> bug or making an update <br/> requires redeploying the <br/> entire application. | Easier to maintain and <br/> update individual <br/> services without <br/> impacting the entire <br/> application. | Maintenance of the <br/> infrastructure is handled by <br/> the cloud provider, but <br/> developers must manage <br/> their code\'s scalability and <br/> performance within the <br/> serverless environment. |\\n| Cost | Costs can be predictable <br/> but may not efficiently <br/> utilize resources due to <br/> the need to scale the <br/> entire application. | Potentially more cost- <br/> efficient as resources are <br/> used more effectively by <br/> scaling services <br/> independently. | Cost-effective for <br/> applications with variable <br/> traffic but can become <br/> expensive if not managed <br/> properly, due to the pay-per- <br/> use pricing model. |\\n| Use Cases | Suitable for small <br/> applications or projects <br/> where simplicity and ease <br/> of deployment are <br/> prioritized. | Ideal for large, complex <br/> applications requiring <br/> scalability, flexibility, and <br/> rapid iteration. <br/> darr | Best for event-driven <br/> scenarios, sporadic <br/> workloads, and rapid <br/> development cycles, where <br/> managing infrastructure is <br/> not desirable. |\\n\\nIt is important to be aware that these are just some architectural patterns that can be used to design a system. There are many other patterns and styles that can be used to design your system architecture, such as event-driven architecture, service-oriented architecture, and peer-to-peer architecture. As this article attempts to introduce the topic, only some of the most common architectures were mentioned.\\n\\n### 3.Developing and building your application\\n\\nThe process of developing a system involves the selection of the appropriate technologies, tools, and frameworks to implement the system. It also consists of designing and developing the system components, such as the user interface, business logic, and data storage. The development process should be straightforward after clearly defining the systems\' requirements.\\n\\n### 4. Testing your application\\n\\nTesting is an essential part of the development process, as it helps to ensure that the system meets its requirements and performs as expected before going into production. Testing involves verifying and validating the system\'s functionality, performance, reliability, and security. It also involves identifying and fixing defects, bugs, and issues in the system. There are several types of testing that can be used to test a system, including:\\n\\n- **Unit Testing**: This involves testing individual components, modules, or functions of the system to ensure that they work as expected. It is typically performed by developers using testing frameworks, such as JUnit, NUnit, or Mocha.\\n- **Integration Testing**: This involves testing the interactions and dependencies between the components, modules, or services of the system to ensure that they work together as expected. It is typically performed by developers using testing frameworks, such as TestNG, Cucumber, or Postman.\\n- **System Testing**: This involves testing the system as a whole to ensure that it meets its requirements and performs as expected. It is typically performed by testers using testing tools, such as Selenium, JMeter, or SoapUI.\\n- **Performance Testing**: This involves testing the performance and scalability of the system to ensure that it can handle the expected load and stress. It is typically performed by testers using performance testing tools, such as Apache JMeter, LoadRunner, or Gatling.\\n- **Security Testing**: This involves testing the security and reliability of the system to ensure that it is protected from unauthorized access and malicious attacks. It is typically performed by security experts using security testing tools, such as OWASP ZAP, Burp Suite, or Nessus.\\n- **User Acceptance Testing**: This involves testing the system with real users to ensure that it meets their needs and expectations. It is typically performed by users using acceptance testing tools, such as UserTesting, UsabilityHub, or UserZoom.\\n- **Stress Testing**: This involves testing the system under extreme conditions to ensure that it can handle the maximum load and stress. It is typically performed by testers using stress testing tools, such as Apache JMeter, LoadRunner, or Gatling.\\n\\n### 5. Deploying your App to production\\n\\nDeploying a system can be a complex and time-consuming process, and it requires careful planning and coordination to minimize the risk of downtime and data loss. Several deployment strategies and strategies exist. However, in today\'s world, Docker is generally the most suitable alternative to simplify this task. Docker is a platform for developing, shipping, and running applications using containerization. Containers are lightweight, portable, and self-contained environments that can run on any machine with the Docker runtime installed. They provide a consistent and reliable way to package and deploy applications, and they are widely compatible with cloud computing environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Docker provides several benefits for deploying systems, including:\\n\\n- **Portability**: Containers can run on any machine with the Docker runtime installed, making them highly portable and compatible with different environments.\\n- **Consistency**: Containers provide a consistent and reliable way to package and deploy applications, ensuring that they run the same way in development, testing, and production environments.\\n- **Isolation**: Containers provide a high level of isolation between applications, ensuring that they do not interfere with each other and that they are secure and reliable.\\n- **Scalability**: Containers can be easily scaled up or down to meet the demands of the system, making them highly scalable and flexible.\\n- **Efficiency**: Containers are lightweight and efficient, requiring minimal resources and providing fast startup times and high performance.\\n- **Security**: Containers provide a high level of security, ensuring that applications are protected from unauthorized access and malicious attacks.\\n- **Automation**: Containers can be easily automated using tools and platforms, such as Kubernetes, Docker Swarm, and Amazon ECS, making them easy to manage and maintain.\\n- **Cost-Effectiveness**: Containers are cost-effective, requiring minimal resources and providing high performance, making them ideal for cloud computing environments.\\n- **Flexibility**: Containers are flexible, allowing developers to use different technologies, tools, and frameworks to develop and deploy applications.\\n- **Reliability**: Containers are reliable, ensuring that applications run consistently and predictably in different environments.\\n- **Compatibility**: Containers are compatible with different operating systems, such as Linux, Windows, and macOS, making them highly versatile and widely used.\\n\\n![Deployment modes](deployment-modes.png)\\n\\n#### Docker\\nDocker was introduced to the world by Solomon Hykes in 2013, founder and CEO of a company called dotCloud. It provides a platform for building, shipping, and running distributed applications. Docker introduced the concept of \u201ccontainers\u201d to package software into isolated environments that can run on any system with the Docker engine installed. This deployment model makes it easy to run the same application in different environments, such as development, testing, and production, without worrying about dependencies, configurations, or compatibility issues.\\n\\n##### Docker components\\n\\nDocker consists of several components that work together to provide its functionality. These components include the Docker engine, the Docker client, and the Docker registry.\\n\\n- **Docker Engine**: The Docker engine is the core component of Docker that provides the runtime environment for containers. It consists of the Docker daemon, which is responsible for building, running, and distributing containers, and the Docker runtime, which is responsible for executing the processes of containers. The Docker engine can run on any system with the Docker runtime installed, including Linux, Windows, and macOS, and it can be managed and monitored using tools and platforms, such as Docker Swarm, Kubernetes, and Amazon ECS.\\n\\n- **Docker Client**: The Docker client is a command-line interface (CLI) that allows developers to interact with the Docker engine, providing a simple and intuitive way to build, run, and manage containers. The Docker client can also be used with graphical user interfaces (GUIs) and integrated development environments (IDEs), providing a seamless and consistent experience for developers.\\n\\n- **Docker Registry**: The Docker registry is a repository for storing and distributing containers, allowing developers to build, push, and pull containers from Docker registries, such as Docker Hub, Amazon ECR, and Google Container Registry. The Docker registry provides a high level of visibility and control over the distribution of containers, ensuring that they are secure, reliable, and efficient.\\n\\n##### Docker Hub\\n\\nDocker also provides a centralized repository called the Docker Hub, where developers can store and share their containers with others. This makes it easy to find and reuse existing containers and to collaborate with other developers on projects. Docker Hub provides a wide range of official and community-contributed containers, including base images, application images, and service images. It also provides features for managing and monitoring containers, such as versioning, tagging, and scanning. Docker Hub is widely used by developers, organizations, and cloud providers, and it provides a high level of visibility and control over the distribution of containers. We can use Docker Hub to store and share our containers so our team members can easily access and use them.\\n\\n![Docker Hub](docker-ecosystem.png)\\n\\n\\n##### Transforming my application into a container image?\\n\\nEverything start by creating a `Dockerfile` that contains the instructions to build a Docker image. The Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. The Docker image is built using the `docker build` command, which reads the `Dockerfile` and executes the instructions to create the image. The Docker image is then stored in a registry, such as Docker Hub, Amazon ECR, or Google Container Registry, where it can be shared and distributed with others. The Docker image can be run as a container using the `docker run` command, which creates an instance of the image and runs it as a container. The Docker container is a running instance of the image that can be managed and monitored using the Docker engine. The Docker container can be stopped, started, paused, and deleted using the `docker stop`, `docker start`, `docker pause`, and `docker rm` commands, respectively. The Docker container can also be managed and monitored using tools and platforms, such as Docker Compose, Docker Swarm, and Kubernetes.\\n\\n**Dockerfile example**\\n```bash\\n# Use an official Python runtime as a parent image\\nFROM python:3.8-slim\\n\\n# Set the working directory in the container\\nWORKDIR /app\\n\\n# Copy the current directory contents into the container at /app\\nCOPY . /app\\n\\n# Install any needed packages specified in requirements.txt\\nRUN pip install --trusted-host pypi.python.org -r requirements.txt\\n\\n# Make port 80 available to the world outside this container\\nEXPOSE 80\\n\\n# Define environment variable\\nENV NAME World\\n\\n# Run app.py when the container launches\\nCMD [\\"python\\", \\"app.py\\"]\\n```\\n\\n##### How does Docker work?\\n\\nUnder the hood, Docker uses a client-server architecture, where the Docker client communicates with the Docker daemon, which is responsible for building, running, and distributing containers. The Docker client and daemon can run on **the same system** or on **different systems**, and they communicate with each other using a **REST API** over a Unix socket or a network interface. As it was mentioned, the Docker daemon is responsible for managing the containers, images, volumes, networks, and other resources of the system. However, it also provides a high-level API for interacting with the Docker engine, allowing developers to build, run, and manage containers using simple commands and scripts.\\nThe Docker daemon is also responsible for managing the lifecycle of containers, including creating, starting, stopping, pausing, and deleting containers, as well as managing their resources, such as CPU, memory, and storage.\\n\\n![Docker](docker.png)\\n\\n##### The underlying technology\\n\\nDocker is written in the `Go programming language` and takes advantage of several features of the Linux kernel to deliver its functionality. The Linux kernel provides the core features and capabilities of the Docker engine, such as process isolation, resource management, and networking.\\n\\n**Linux kernel features** that Docker relies on include:\\n\\n- **Cgroups (control groups)** provide the ability to limit and prioritize the resources of containers, such as CPU, memory, and storage.\\n- **Namespaces** provide the ability to isolate and control the processes, users, and network of containers, ensuring that they do not interfere with each other.\\n- **Union file systems** provide the ability to create and manage the file systems of containers, allowing them to share and reuse the same files and directories.\\n\\n:::tip\\n\\nThe Linux kernel is the main component of the Linux operating system (OS). It\'s a computer program that acts as the interface between a computer\'s hardware and its processes. The kernel manages resources as efficiently as possible and enables communication between applications and hardware.\\n\\n:::\\n\\nSo the question you probably have, how can Docker run containers on Windows and MacOS if Docker relies on the Linux kernel?\\n\\n**On Windows** you can run Docker containers using the following approaches:\\n\\n**Windows Subsystem for Linux (WSL) 2:** With the introduction of WSL 2, Docker can run Linux containers natively on Windows. WSL 2 provides a full Linux kernel built into Windows, allowing Docker to interface directly with the kernel without the need for a virtual machine (VM). This approach is efficient and integrates well with Windows environments.\\n\\n**Docker Desktop for Windows:** Before WSL 2, Docker Desktop for Windows used a lightweight VM to host a Linux kernel. This VM then runs the Docker Engine and, by extension, Docker containers.\\n\\n**On macOS**, docker also utilizes a lightweight virtual machine to run a Linux kernel. Docker Desktop for Mac leverages macOS\'s native virtualization frameworks (such as Hypervisor.framework for Intel processors and the Virtualization framework for Apple silicon) to run this VM efficiently. This setup allows Docker containers to run in a Linux-like environment on Mac, with Docker Desktop handling the complexities of managing the VM.\\n\\n![Docker  on Linux and Windows](docker-linux-win.png)\\n\\n**Docker was designed to run Linux-based docker containers**, as it was developed on top of some of the Linux kernel features. However, Microsoft offers four container-based Windows images from which users can build. Each base image is a different type of the Windows or Windows Server operating system, has a different on-disk footprint, and has a different set of the Windows API set.All Windows container base images are discoverable through Docker Hub. The Windows container base images themselves are served from mcr.microsoft.com, the Microsoft Container Registry (MCR). This is why the pull commands for the Windows container base images look like the following:\\n\\n```bash\\ndocker pull mcr.microsoft.com/windows/servercore:ltsc20229\\n```\\n\\nFor more information, you can check the [official documentation](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-base-images).\\n\\n\\n**Most common Docker commands**\\n\\n```bash\\n# Pull an image from Docker Hub\\ndocker pull <image-name>\\n\\n# List all images on the system\\ndocker images\\n\\n# Run a container from an image\\ndocker run <image-name>\\n\\n# List all running containers\\ndocker ps\\n\\n# List all containers\\ndocker ps -a\\n\\n# Stop a running container\\ndocker stop <container-id>\\n\\n# Start a stopped container\\ndocker start <container-id>\\n\\n# Remove a container\\ndocker rm <container-id>\\n\\n# Remove an image\\ndocker rmi <image-name>\\n```\\n\\n\\n##### Running Multi-Container Applications\\n\\nOne of the challenges of running multi-container applications is managing the dependencies and interactions between them. Docker Compose, Swarm and  Kubernetes they are all tools that can be used to define, configure, and run multi-container applications. Cloud providers, such as AWS, Azure, and GCP, also provide managed services for running multi-container applications, such as Amazon ECS, Azure Container Instances, and Google Cloud Run. In the next video you can see 3 alternatives to run multi-container applications on GCP.\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/jh0fPT-AWwM\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n\\nNow, lets dive into the introduction of Docker Compose, Docker Swarm and Kubernetes.\\n\\n- **Docker Compose** is a tool for defining and running multi-container applications on a single host. It is easy to use and requires minimal setup, making it a popular choice for developers who want to quickly set up and test their applications locally. Docker Compose provides a simple and convenient way to define, configure, and run multi-container applications, but it is limited to a single host and does not provide the same level of scalability and resource management as Kubernetes.\\n\\n```yaml\\nversion: \'3\'\\nservices:\\n    frontend:\\n        image: frontend:latest\\n        ports:\\n        - \\"80:80\\"\\n        depends_on:\\n        - backend\\n    backend:\\n        image: backend:latest\\n        ports:\\n        - \\"8080:8080\\"\\n        environment:\\n        - DATABASE_URL=postgres://user:password@db:5432/db\\n    db:\\n        image: postgres:latest\\n        environment:\\n        - POSTGRES_USER=user\\n        - POSTGRES_PASSWORD=password\\n        - POSTGRES_DB=db\\nnetworks:\\n    default:\\n      external:\\n        name: my-network\\n```\\n\\n- **Kubernetes**, on the other hand, is a production-ready platform for deploying, scaling, and managing containerized applications. It provides a powerful and flexible architecture for managing multi-container applications at scale, and it includes features for automatic scaling, rolling updates, self-healing, and resource management. Kubernetes is designed for large, complex, and mission-critical applications, and it provides a high degree of availability and resilience. If this technology catch your attention, and you want to learn more about it, I recommend you to check the next video. It provides a great overview about how kubernetes comes to life into the world of multi-container applications.\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/BE77h7dmoQU\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n- **Docker Swarm** is a native clustering and orchestration tool for Docker. It allows you to create and manage a cluster of Docker hosts, and it provides features for scaling, load balancing, and service discovery. Docker Swarm is easy to use and integrates well with Docker, making it a good choice for developers who want to manage multi-container applications without the complexity of Kubernetes.\\n\\n\\n### 6. Scaling your App to meet the demands of its users\\n\\nWhen deploying a system, it is essential to consider how the system will scale to meet the demands of its users. Scalability describes the ability of a system to handle an increasing amount of work without compromising its performance, reliability, and availability. It is also related to system elasticity, which is the ability of a system to adapt to changes in the workload by adding (scaling up) or removing (scaling out) resources. Systems can be scaled in two ways: `vertically` and `horizontally`.\\n\\n#### Horizontal Scaling (Scaling Out/In)\\n\\nHorizontal scaling involves adding more machines or nodes to a pool of resources to manage increased load. It\'s like adding more lanes to a highway to accommodate more traffic. This approach is common in distributed systems, such as cloud computing environments, where you can add more instances or servers to handle more requests.\\n\\n**Advantages:**\\n\\n- **Scalability:** It\'s easier to scale applications indefinitely by simply adding more machines into the existing infrastructure.\\n\\n- **Flexibility:** You can scale the system up or down by adding or removing resources as demand changes, often automatically.\\n\\n- **Fault Tolerance:** Horizontal scaling can improve the reliability and availability of a system. If one node fails, others can take over, reducing the risk of system downtime.\\n\\n**Disadvantages:**\\n\\n- **Complexity:** Managing a distributed system with many nodes can be more complex, requiring sophisticated software and tools for load balancing, distributed data management, and failover mechanisms.\\n\\n- **Data Consistency:** Ensuring data consistency across nodes can be challenging, especially in databases or systems requiring real-time synchronization.\\n\\n#### Vertical Scaling (Scaling Up/Down)\\n\\nVertical scaling involves increasing the capacity of an existing machine or node by adding more resources to it, such as CPU, RAM, or storage. It\'s akin to upgrading the engine in a car to achieve higher performance.\\n\\n**Advantages:**\\n\\n- **Simplicity:** It is often simpler to implement as it may require just upgrading existing hardware. It doesn\'t involve the complexity of managing multiple nodes.\\n\\n- **Immediate Performance Boost:** Upgrading hardware can provide an immediate improvement in performance for applications that can utilize the extra resources.\\n\\n**Disadvantages:**\\n\\n- **Limited Scalability:** There is a physical limit to how much you can upgrade a single machine, and eventually, you might hit the maximum capacity of what a single server can handle.\\n\\n- **Downtime:** Upgrading hardware might require downtime, which can be a significant drawback for systems that require high availability.\\n\\n- **Cost:** Beyond certain points, vertical scaling can become prohibitively expensive as high-end hardware components can cost significantly more.\\n\\nThe choice between horizontal and vertical scaling depends on the specific requirements, architecture, and constraints of the system in question. Horizontal scaling is favored for applications designed for cloud environments and those requiring high availability and scalability. Vertical scaling might be chosen for applications with less demand for scalability or where simplicity and immediate performance improvement are prioritized. Often, a hybrid approach is used, combining both strategies to leverage the advantages of each.\\n\\nScaling your system is something you can control,and plan ahead with the support of your infrastructure team. However, doing this manually is rather time-consuming, especially when the increased load only sustains for a short period of time. In other words, you\u2019re always too late. This is where autoscaling comes in, by automatically scaling either horizontally or vertically when the current incoming load requires it.\\n\\n#### Autoscaling\\n\\nAuto-scaling, or automatic scaling, is a technique that dynamically adjusts the amount of computational resources in a server farm or a cloud environment based on the current demand. It is closely related to both horizontal and vertical scaling, but it primarily leverages horizontal scaling due to its flexibility and the ease with which resources can be added or removed in cloud-based environments. I encorage you to check the next video to understand how Kubernetes relies on autoscaling to manage the resources of your system in cloud or on-premises environments.\\n\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/XpeAITE4uqA\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### 7. Monitoring and Logging\\n\\nMonitoring and logging are essential for understanding the behavior and performance of a system in a production environment. It involves collecting and analyzing data about the system\'s performance, availability, and reliability, while logging involves recording and storing data about the system\'s activities, events, and errors. Monitoring and logging are crucial for identifying and diagnosing issues, optimizing performance, and ensuring the system meets its service level objectives (SLOs) and service level agreements (SLAs). There are several tools and platforms that can be used for monitoring and logging. The selection of the appropriate tools and platforms depends on the requirements and constraints of the system.\\n\\nThis is the end of the introduction to system design for data scientists and ML engineers. I hope you have enjoyed it and learned something new. If you have any questions, feel free to ask in the comments section."},{"id":"python-for-data-science-exploring-the-syntax","metadata":{"permalink":"/blog/python-for-data-science-exploring-the-syntax","source":"@site/blog/2022-10-26-python-for-data-science-exploring-the-syntax/index.mdx","title":"Python for Data Science Series - Exploring the syntax","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","date":"2022-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":19.475,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Exploring the syntax","slug":"python-for-data-science-exploring-the-syntax","image":"https://haruiz.github.io/img/2022-10-26-python-for-data-science-exploring-the-syntax-og-image.jpg","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","permalink":"/blog/intro-to-system-design"},"nextItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"\x3c!--truncate--\x3e\\n\\n### Introduction\\n\\nIn the [last post](python-for-data-science-part-getting-started) we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python by creating a simple program that uses Google Cloud Vision API to detect faces in an image.\\n\\nYou will learn today:\\n- What is a computer program?\\n- How to write a program in Python?\\n  - Python syntax\\n  - How to organize your code in python using functions\\n- What is a REST API, and how to use it?\\n- How to use Google Cloud Vision API to detect faces in an image?\\n\\nSo lets started!!!\\n\\n# What is a computer program?\\n\\nA computer program is a sequence of instructions we write using a programming language to tell the computer what to do for us. This sequence of instructions contains but is not limited to:\\nShow information to the user, ask the user for input, save and recover data in memory/disk, and perform calculations. So, programming languages provide a set of built-in functions and instructions that can be used to accomplish these tasks.\\n\\n:::tip\\nIf we think about programming languages, we can compare them to different idioms we use to communicate with others. We choose the appropriate language based on the application or where our program will run.\\n:::\\n\\n# How to write a program in Python?\\n\\nWhen we write programs independent of the programming language we decide to use, writing down our algorithm in simple words is always helpful. In a way, we can have a mental model of what our program will be doing and how it will be executed. To do so, we can use Pseudocode, a simplified version of computer programs written in natural or human-readable language that can be easily interpreted. You can check this [**cheat sheet**](https://cheatography.com/lcheong/cheat-sheets/pseudocode/) that will help you to write your program in Pseudocode.\\n\\n:::info Algorithm\\nFinite set of rules to be followed in calculations or other problem-solving operations, especially by a computer.\\n:::\\n\\nSo, let\'s define our pseudocode for our face detection program:\\n\\n```pseudocode showLineNumbers\\nVAR image_path as STRING = INPUT(\\"Please provide the path of the image: \\")\\nIF image_path is empty or image_path dont exist THEN\\n    PRINT(\\"image path could not be empty or the image does not exist\\")\\n    EXIT\\nENDIF\\n\\nFUNCTION read_image(image_path) as STRING\\n    VAR image_bytes as BYTES = read(image_path)\\n    RETURN image_bytes\\nENDFUNCTION\\n\\nFUNCTION detect_faces_on_image(image_bytes as BYTES) as LIST:\\n    api_response as LIST = call_face_detection_gcp_api(image_bytes)\\n    IF api_response is empty THEN\\n        PRINT(\\"No faces found\\")\\n        RETURN\\n    faces as LIST = []\\n    FOR json_entity in api_response THEN\\n        face as DICT = {\\n            \\"confidence\\": json_entity.confidence,\\n            \\"bounding_box\\": json_entity.bounding_box,\\n            \\"is_happy\\" : json_entity.joy_likelihood == \\"VERY_LIKELY\\"\\n        }\\n        faces.append(face)\\n    ENDFOR\\n    RETURN faces\\nENDFUNCTION\\n\\nimage_bytes = read_image(image_path)\\nfaces_list = detect_faces(image_bytes)\\ndisplay_detect_faces(faces_list)\\n```\\n\\nAs you can see in Pseudocode, we can skip the implementation details of our program. We write down our algorithm using a high-level language, so in this way, we have a big picture of the tasks we need to perform that we can use later to translate our algorithm into a programming language. In line 13, for instance,  we need to call the Google Cloud Vision API to detect the faces in the image, but we have yet to determine how it will be implemented.\\n\\n## Python syntax\\n\\nTo learn about python syntax, we will navigate through the Pseudocode and convert it into a python script.\\n\\n### Variables\\n\\nProgramming is all about data and manipulating it to solve problems. So, we need to have a way to store data on our computer that we can access later during execution. To do so, we use variables. Variables are a way to store data in memory where we can save almost any data. In Python, it is straightforward to define a variable; we need to use the assignment operator `=` followed by the value we want to store, and the Python interpreter will take care of the rest. Under the hood, it will allocate memory for the variable and store its value. We can save strings, integers, floats, booleans, lists, dictionaries, and other data types. Let\'s see an example:\\n\\n```python\\n# String\\nmy_string = \\"Hello World\\"\\n# Integer\\nmy_integer = 1\\n# Float\\nmy_float = 1.0\\n# Boolean\\nmy_boolean = True\\n# List\\nmy_list = [1, 2, 3]\\n# Dictionary\\nmy_dict = {\\"key\\": \\"value\\"}\\n```\\n\\nWe can obtain the memory address and type of a variable using the `id()` and `type()` functions respectively.\\n\\n```python\\nmy_string = \\"Hello World\\"\\nmy_string_2 = \\"Hello World\\"\\nprint(id(my_string))\\nprint(id(my_string_2))\\nprint(type(my_string))\\nprint(type(my_string_2))\\n```\\n\\nIn our program, in line 1, we define a variable `image_path` and assign it the value of the user input. In Python, the `input()` function allows us to grab information from the user so we can save the value into a variable. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimage_path = input(\\"Please provide the path of the image:\\")\\n```\\n\\nThe syntax is very similar to the Pseudocode. However, you can notice that in Python, we don\'t specify the variable type. That is because Python is a dynamically typed language, meaning that the variable type is inferred during the execution. In terms of productivity, this is very convenient because we don\'t need to worry about specifying the type of the variables when we define them. However, it can sometimes be a source of errors if we are not carefully doing operations.\\n\\nPython will raise an error if we try to perform an operation that is not supported by the type of the variable. Let\'s see an example:\\n\\n```python\\na = 2   \\nb = \\"2\\"\\n# error-line-next\\nprint(a + b)\\n# TypeError: unsupported operand type(s) for +: \'int\' and \'str\'\\n```\\n\\n\\n:::warning Rules for creating variables across languages\\n- A variable name must start with a letter or an underscore character.\\n- A variable name cannot start with a number.\\n- A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ ).\\n- Variable names are case-sensitive (name, Name and NAME are three different variables).\\n- The reserved words(keywords) cannot be used naming the variable.\\n:::\\n\\n### Conditional blocks\\n\\nA common task in programming is to execute a block of code only if a condition is met. In Python, we can use the `if` statement to do so. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\n```\\n\\nIn the example above, we check if the variable `a` is equal to 2. If that is the case, we print the message \\"a is equal to 2\\". We can also use the `else` statement to execute a block of code if the condition is not met. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\nelse:\\n    print(\\"a is not equal to 2\\")\\n```\\n\\nIn our program, we need to check if the user input is empty or if the image path does not exist. We can use the `if` statement to do so. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimport os\\n\\nimage_path = input(\\"Please provide the path of the image:\\")\\nif image_path == \\"\\" or not os.path.exists(image_path):\\n    print(\\"image path could not be empty or the image does not exist\\")\\n    exit()\\n```\\n\\nAgain, we can observe that the syntax is very similar to the Pseudocode, just with the addition of the `os.path.exists()` function in the condition to check whether a image exists or not. The os module it is included in the Python standard library, and it provides a way to interact with the operating system. We also use the `exit()` function to exit the program in case the condition is met. We are going to discuss about modules later in this article.\\n\\n:::info Python Standard Library\\nThe Python Standard Library is a set of modules that comes with the Python installation. It provides a wide range of built-in functions and classes that we can use in our programs for different purposes. You can find more information about the Python Standard Library [here](https://docs.python.org/3/library/index.html).\\n\\nSome of the most used modules are:\\n- **os:** provides a way to interact with the operating system.\\n- **sys:** provides a way to interact with the Python interpreter.\\n- **json:** provides a way to work with JSON data.\\n- **re:** provides a way to work with regular expressions.\\n- **math:** provides a way to work with mathematical operations.\\n- **random:** provides a way to work with random numbers.\\n- **datetime:** provides a way to work with dates and times.\\n- **urllib:** provides a way to work with URLs.It is a very useful module to work with APIs. We are going to use it in the next section to call the Google Cloud Vision API. \\n:::\\n\\n### Functions\\n\\nFunctions are a way to encapsulate a block of code that we can reuse in our program. In Python, we can define a function using the `def` keyword followed by the function name and the parameters. Let\'s see an example:\\n\\n```python\\ndef add(a, b):\\n    \\"\\"\\"\\n    This function adds two numbers\\n    :param a: first number\\n    :param b: second number\\n    :return: sum of the two numbers\\n    \\"\\"\\"\\n    return a + b\\n\\nif __name__ == \\"__main__\\":\\n    print(add(1, 2))\\n```\\n\\nThe paremeters are the variables that we need to pass to the function to perform the task. In the example above, we define a function called `add` that takes two parameters `a` and `b`, and the function returns the sum of the values of the two parameters. We can call the function by using the function name followed by the parameters. In the example above, we call the function `add` with the parameters `1` and `2`. The function returns the value `3` and we print it in the console.\\n\\nIn our program, we have two main functions that we need to implement, `read_image` and `call_face_detection_gcp_api.` The first takes the image path as a parameter and returns the image data. The second takes the image data as a parameter, requests the Google Cloud Vision API to detect faces in the image, and returns the face annotations in JSON format. Let\'s see how we can translate the `read_image`  function from Pseudocode into Python:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n```\\n\\nThere is a new syntax here in the `read_image` function to be discussed. \\n\\n- **Function annotations:** Although not mandatory, we can specify the parameters\' type and the functions\' return value in Python. These are called function annotations. Although the Python interpreter does not enforce them, and we still have to check the type of the parameters programmatically,  annotations are extremely useful for other project contributors to navigate through the code and understand how the function must be called. In the example above, we specify that the function takes a string as a parameter and returns a bytes object.\\n\\nAnother good thing about function annotations is that It makes the function more readable and will also helps to avoid errors when function is called in other parts of the program.\\n\\n- **Context Managers:** It can also be noticed that we use the `with` statement to open the file in the `read_image` function. These blocks of code are called context managers in Python, and in this case, it ensures that the file is closed after the block of code is executed. We will discuss context managers later in other articles since this is an advanced topic. For more information about context managers, you can check the [Python documentation](https://docs.python.org/3/reference/compound_stmts.html#with).\\n\\n- **Encoding:** We can also see that we use the `rb` mode to open the file. This mode allows us to read the file as bytes so we can encode it in base64 to send it to the Google Cloud Vision API. That is required because the API only accepts images encoded in this format. For more information about the `rb` mode, you can check the [Python documentation](https://docs.python.org/3/library/functions.html#open), and face detection API documentation [here](https://cloud.google.com/vision/docs/detecting-faces).\\n\\n:::info Encoding data\\nEncodings are a way to represent a sequence of bytes in a different format. The most common encodings are ASCII, UTF-8, and base64. ASCII is a 7-bit encoding that represents the first 128 characters of Unicode. UTF-8 is a variable-length encoding that represents the first 1,112,064 characters of Unicode. Base64 is a way to represent binary data in ASCII characters and it is used to send binary data in text-based protocols such as HTTP. For more information about encodings, you can check the [Python documentation](https://docs.python.org/3/library/codecs.html#standard-encodings).\\n:::\\n\\n- **Error handling:** In order to catch the errors in our python functions we can use the `try` and `except` block. The `try` statement allows us to execute a block of code and catch the errors that can happen in the `except` statement. A `finally` block can also be used to execute a block of code after the `try` and `except` blocks.\\nLet\'s see how to do this:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    try:\\n        # read and load the image into memory\\n        with open(image_path, \\"rb\\") as f:\\n            return f.read() # read the image\'s bytes\\n    except Exception as e:\\n        raise Exception(\\"Error reading the image: \\", e)\\n    finally:\\n        print(\\"finally block\\")\\n```\\n\\n### Modules\\n\\nModules are a way to group a set of functions and classes in our programs. In Python, we can import a module using the `import` keyword followed by the module name.\\n\\n```python\\nimport os # import the os module\\nif __name__ == \\"__main__\\":\\n    print(os.path.exists(\\"image.jpg\\"))\\n```\\n\\nIn the example above, we import the `os` module and use the `os.path.exists()` function to check if the file `image.jpg` exists. We can also import a specific function from a module using the `from` keyword. Let\'s see an example:\\n\\n```python\\nfrom os import path\\nif __name__ == \\"__main__\\":\\n    print(path.exists(\\"image.jpg\\"))\\n```\\n\\nFollowing with our example, to implement the `call_face_detection_gcp_api` function, we need to import the `urllib` module. This module provides a set of function we can use to call the Google Cloud Vision API. Let\'s see how to do this:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string so it can be sent to the API\\n    :param image_bytes:\\n    :return: base64 string\\n    \\"\\"\\"\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call GCP Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: response the face annotations as JSON\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    image_base64 = image_to_base64(image_bytes)\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # Create request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n```\\n\\nFor more information about using the `urlib` module, you can check the [Python documentation](https://docs.python.org/3/library/urllib.request.html).\\n\\n:::tip What is an REST API?\\nREST stands for Representational State Transfer. It is an architectural style for designing networked applications, that allows to expose data and functionality to external clients in public(wan) or private(lan) networks. Clients could be web applications, mobile applications, or even other services. For more information about REST APIs, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Representational_state_transfer).REST APIs are implemented using HTTP methods. The most common methods are `GET`, `POST`, `PUT`, `PATCH`, and `DELETE`, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods). It also provides standard data formats to send and receive data, for instance `JSON` and `XML`. More information [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages).\\n:::\\n\\nThe video below give you a quick overview of how REST APIs work:\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/7YcW25PHnAA\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Loops\\n\\nLoops are a way to execute a block of code multiple times. In Python, we can use the `for` loop to iterate over a list of elements. Let\'s see an example:\\n\\n```python\\nfor i in range(10):\\n    print(i)\\n```\\n\\nThe `range` function returns a list of numbers from 0 to 10. The `for` loop iterates over the list and prints each element. The `range` function can also receive a start and end value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10):\\n    print(i)\\n```\\n\\nThe `range` function can also receive a step value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10, 2):\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a list of elements. Let\'s see an example:\\n\\n```python   \\nfor i in [1, 2, 3, 4, 5]:\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a dictionary. Let\'s see an example:\\n\\n```python\\nfor key, value in {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3}.items():\\n    print(f\\"key: {key}, value: {value}\\")\\n```\\n\\nThe `for` loop can also be used to iterate over a string. Let\'s see an example:\\n\\n```python\\nfor char in \\"Hello World\\":\\n    print(char)\\n```\\n\\nThe `while` loop is used to execute a block of code while a condition is true. Let\'s see an example:\\n\\n```python\\ni = 0\\nwhile i < 10:\\n    print(i)\\n    i += 1\\n```\\n\\nIn our code in the last function of our script, we need to iterate over the list of faces returned by the API. Let\'s see how we do this in line `18`\\n\\n```python showLineNumbers {18-23}\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: list of faces found\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n```\\n\\nI skipped the `call_face_detection_gcp_api` function explanation since it was supposed to be an introductory tutorial. However, I have tried my best to comment on the code so you can develop an intuition on what the function does. To get more information about how to call the `GCP face detection API,` you can check the official documentation [here](https://cloud.google.com/vision/docs/detecting-faces). You must create a Google Cloud Platform account to use the API. To see how to create the project and get the API key, you can check the [official documentation](https://cloud.google.com/vision/docs/libraries#client-libraries-install-python).\\n\\nIn the next section, we will see how to do more advance things with Python using third party packages and libraries. For now I will leave you with the full code of the script:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string\\n    :param image_bytes:\\n    :return:\\n    \\"\\"\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call Google Cloud Platform Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    image_base64 = image_to_base64(image_bytes)\\n    # Create request body\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # make request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n\\n\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n\\n\\ndef main():\\n    try:\\n        image_path = input(\\"Please provide the path of the image:\\")\\n        assert image_path != \\"\\" and os.path.exists(image_path), \\"image path could not be empty or the image does not exist\\"\\n        # read and return image as bytes\\n        image = read_image(image_path)\\n        # pass the image to the face detection function to detect faces\\n        faces = detect_faces_on_image(image, API_KEY=\\"<GCP API KEY>\\")\\n        # print the number of faces found\\n        print(\\"number of faces found:\\", len(faces))\\n        # iterate over the faces and do something\\n        for face in faces:\\n            print(face[\\"is_happy\\"])\\n    except Exception as e:\\n        print(f\\"Error running the script: {e}\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\nThis is all for this tutorial. I hope you enjoyed it. If you have any questions, please leave a comment below or contact me on LinkedIn. If you want to see more tutorials like this, please subscribe to my newsletter (See top menu). To access the code for this tutorial, you can check the [GitHub repository](https://github.com/haruiz/blog-code/blob/main/python-for-data-science-exploring-the-syntax/main.py)\\n\\n\\n## Useful Links\\n- [GCP Face Detection API](https://cloud.google.com/vision/docs/detecting-faces)\\n- [GCP Machine Learning APIs](https://cloud.google.com/products/ai)\\n- [Python Cheat Sheet](https://www.pythoncheatsheet.org/)\\n- [Python Documentation](https://docs.python.org/3/)\\n- [Python Tutorial](https://docs.python.org/3/tutorial/index.html)\\n- [Python Standard Library](https://docs.python.org/3/library/index.html)\\n- [Python Package Index](https://pypi.org/)\\n- [Python for Data Science](https://www.python.org/about/apps/)\\n- [What is a REST API?](https://www.youtube.com/watch?v=lsMQRaeKNDk&ab_channel=IBMTechnology)\\n- [What is JSON?](https://www.youtube.com/watch?v=iiADhChRriM&ab_channel=IBMTechnology)"},{"id":"python-for-data-science-part-getting-started","metadata":{"permalink":"/blog/python-for-data-science-part-getting-started","source":"@site/blog/2022-08-27-python-for-data-science-getting-started/index.mdx","title":"Python for Data Science Series - Getting started","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","date":"2022-08-27T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":6.3,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Getting started","slug":"python-for-data-science-part-getting-started","image":"https://haruiz.github.io/img/2022-08-27-python-for-data-science-getting-started-og-image.jpg","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"},"nextItem":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","permalink":"/blog/python-environments-with-pyenv-and-poetry"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\nimport VideoPlayer from \\"@site/src/components/VideoPlayer\\";\\n\\n## Introduction\\n\\nProgramming is an essential skill for data scientists. If you are considering starting a data science career, the sooner you learn how to code, the better it will be. Most data sciences jobs rely on programming to automate cleaning and organizing data sets, design databases, fine-tune machine learning algorithms, etc. Therefore, having some experience in programming Languages such as Python, R, and SQL makes your life easier and will allow you to automate your analysis pipelines.\\n\\nIn this week\'s post, we will focus on Python. A general-purpose programming language that allows us to work with data and explore different algorithms and techniques that would be extremely useful to add to our analysis toolbox.\\n\\n### Why should I learn how to program?\\n\\n\\nTo help organizations make better decisions,  a data scientist is a technical expert who uses mathematical and statistical techniques to manipulate, analyze and extract patterns from raw/noisy data to produce information. Those tools include but are not limited to statistical inference, pattern recognition, machine learning, deep learning, etc. \\n\\nData Scientist\'s responsibilities involve:\\n\\n- Work closely with business stakeholders to understand their goals and determine how data can be used to achieve them.  \\n- Fetching information from various sources and analyzing it to get a clear understanding of how an organization performs\\n- Undertaking data collection, preprocessing, and analysis\\n- Building models to address business problems\\n-  Presenting information in a way that your audience can understand using different data visualization techniques\\n\\nAlthough programming is not required to be a data scientist, taking advantage of the power of computers, most of these tasks can be automated. So, programming skills provide data scientists with the superpowers to manipulate, process, and analyze big datasets, automate and develop computational algorithms to produce results (faster and more effectively), and create neat visualizations to present the data more intuitively.\\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/dU1xS07N-FA\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n## Programming languages for data science\\n\\nThere are hundreds of programming languages out there, built for diverse purposes. Some are better suited for web or mobile development, others for data analysis, etc. Choosing the correct language to use will depend on your level of experience, role, and/or project goals. In the last few years, Python has been ranked as one of the top programming languages data scientists use to manipulate, process, and analyze big datasets.\\n\\nBut why is Python so popular? Well, I will list some reasons why data scientists love Python and what makes this language suitable for high productivity and performance in processing large amounts of data.\\n\\n### Why Python?\\n\\n- Python is **open source**, so is freely available to everyone.You can even use it to develop commercial applications.\\n- Python is **Multi-Platform**. It can be run on any platform, including Windows, Mac, Linux, and Raspberry Pi.\\n- Python is a **Multi-paradigm** language, which means it can be used for both object-oriented and functional programming. It comes from you writing code in a way that is easy to read and understand.\\n- Python is **Multi-purpose**, so you can use it to develop almost any kind of application. You can use it to develop web applications, game development, data analysis, machine learning, and much more.\\n- Python syntax is **easy to read** and **easy to write**. So the learning curve is low in comparison to other languages.\\n- Data Science **packages ecosystem**: Python also has [PyPI package index,a python package repository](https://pypi.org/), where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages.Some of the most used libraries for data science and machine learning are:\\n\\n  - [Tensorflow](https://www.tensorflow.org/), a high-performance numerical programming library for deep learning.\\n  - [Pandas](https://pandas.pydata.org/), a Python library for data analysis and manipulation.\\n  - [NumPy](https://www.numpy.org/), a Python library for scientific computing ( that offers an extensive collection of advanced mathematical functions, including linear algebra, Fourier transforms, random number generation, etc.)\\n  - [Matplotlib](https://matplotlib.org/), a Python library for plotting graphs and charts.\\n  - [Scikit-learn](https://scikit-learn.org/stable/index.html), a Python library for machine learning.\\n  - [Seaborn](https://seaborn.pydata.org/), a Python library for statistical data visualization.\\n\\n- **High performance:** Although some people complain about performance in Python (see [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)), mainly caused by some features such as dynamic typing, it is also simple to extend developing modules in other compiled languages like C++ or C which could [speed up your code by 100x.](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)\\n  \\nThe following section will introduce you to the Python programming language, and we will start learning its syntax.\\n\\n## Hands-on Tutorial\\n\\n:::tip\\nTo set up our python environment, we will use `pyenv` and `poetry.` You can learn more about these tools in the previous post.\\n[Python environments with pyenv and poetry](/blog/python-environments-with-pyenv-and-poetry)\\n:::\\n\\nWe will start with a simple program that prints \\"Hello World\\" on the screen, and from there, we will begin navigating into the python syntax, learning some of its keywords and essential building blocks. Start creating a folder called \\"python_demo\\" and a file called \\"hello_world.py.\\" To do so, run the following commands in the terminal:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'cd workspace\' , comment: \\"moving into the workspace directory. It could be any folder in your machine where you want to have your python_demo folder\\"},\\n{ type: \'input\', value: \'mkdir python_demo\' , comment: \\"creating the python_demo folder inside the workspace folder\\" },\\n{ type: \'input\', value: \'cd python_demo\' , comment: \\"going into the python_demo folder\\" },\\n{ type: \'input\', value: \'touch hello_world.py\' , comment: \\"creating the hello_world.py file inside the python_demo folder, in windows use the command type\\" },\\n{ type: \'input\', value: \\"pyenv version\\", comment: \\"checking the python version being used by pyenv to create the Python environment\\"},\\n{ type: \\"output\\", value: \\"python 3.10.0\\"},\\n{ type: \\"input\\", value: \\"poetry init\\", comment: \\"initialize poetry project into the python_demo directory\\"},\\n{ type: \\"input\\", value: \\"poetry install\\", comment: \\"create python environment within the folder\\"}\\n]} />\\n\\nIf all the command runs successfully, you should see the following folder structure:\\n```bash\\n\u251c\u2500\u2500 python_demo\\n\u2502   \u251c\u2500\u2500 hello_world.py\\n\u2502   \u251c\u2500\u2500 poetry.lock\\n\u2502   \u2514\u2500\u2500 pyproject.toml\\n```\\nAnd the `pyproject.toml` file should look like this:\\n```toml\\n[tool.poetry]\\nname = \\"python_demo\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [`Henry Ruiz  <henryruiz22@gmail.com>`]\\n\\n[tool.poetry.dependencies]\\npython = \\"^3.10\\"\\n\\n[tool.poetry.dev-dependencies]\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\nYou can see that the Python version was set to 3.10.0. that will depend on the version of Python you are using with pyenv.\\n\\n:::tip\\nTo check the python version run the command `pyenv version` in the terminal.\\n:::\\n\\nTo open our python_demo folder in pycharm check the animation below.\\n\\n<VideoPlayer videoUrl={require(\\"./open-folder-pycharm.mp4\\").default} />\\n\\nAt this point, you should know how to create and run python files. So, in the coming tutorials, we will be working on the hello_world.py file, exploring the python syntax, and learning cool things about Python and data science.\\n\\nThanks for reading!, and I hope this tutorial helped you to get started with Python.\\n\\n\\n**Some useful resources**\\n\\n- [Python Tutorial](https://docs.python.org/3/tutorial/)\\n- [Python Language Reference](https://docs.python.org/3/reference/)\\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\\n- [Why Coding is important in Data Science](https://www.dqindia.com/coding-important-data-science/)\\n- [Python for Data Science](https://www.geeksforgeeks.org/python-for-data-science/)\\n- [Top programming languages for data scientists in 2022](https://www.datacamp.com/blog/top-programming-languages-for-data-scientists-in-2022)\\n- [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)\\n- [Write Your Own C-extension to Speed Up Python by 100x](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)"},{"id":"python-environments-with-pyenv-and-poetry","metadata":{"permalink":"/blog/python-environments-with-pyenv-and-poetry","source":"@site/blog/2022-08-07-Python-environments-with-pyenv-and-poetry/index.mdx","title":"Python for Data Science Series - Python environments with pyenv and poetry","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","date":"2022-08-07T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":18.075,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","slug":"python-environments-with-pyenv-and-poetry","hide_table_of_contents":false,"image":"https://haruiz.github.io/img/2022-08-07-Python-environments-with-pyenv-and-poetry-og_image.png","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\n\\nimport TOCInline from \'@theme/TOCInline\';\\n\\nimport Image from \'@theme/IdealImage\';\\n\\nimport Tabs from \'@theme/Tabs\';\\n\\nimport TabItem from \'@theme/TabItem\';\\n\\n[//]: # ()\\n[//]: # (:::tip In this post you will learn)\\n\\n[//]: # ()\\n[//]: # (<TOCInline toc={toc} />)\\n\\n[//]: # ()\\n[//]: # (:::)\\n\\n## Introduction\\n\\nPython, a versatile programming language widely embraced in fields such as web development, data science, machine learning, and scientific computing. However, navigating through different Python installations and dependencies can often become overwhelming. On this post we will explore how tools like pyenv and poetry can simplify this process by effectively managing project dependencies. Let\'s embark on this journey of optimizing code environments together!\\n\\n### Why Python?\\n\\nAccording to the [**2022 stack overflow developer survey**](https://survey.stackoverflow.co/2022/#technology-most-loved-dreaded-and-wanted), Python is one of the most widely used programming languages today. Of 71,467 responses, 68% of developers expressed that they love the language and are planning to continue working with Python, and approximately 12.000 of those who haven\'t got the chance to use it have expressed their interest in starting developing with it. Its popularity is mainly due to its simplicity in syntax, expressiveness, and versatility. We can use Python to create any kind of software, from web applications to scientific computing.\\n\\n\\nPython also has [**PyPI package index**](https://pypi.org/),a python package repository, where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages. \\n\\n:::info\\nThe Python Package Index, abbreviated as PyPI (/\u02ccpa\u026api\u02c8a\u026a/) and also known as the Cheese Shop (a reference to the Monty Python\'s Flying Circus sketch \\"Cheese Shop\\"), is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl and to the CRAN repository for R.<a href=\\"#wikipedia:1\\">[1]</a>\\n:::\\n### Python Dependency hell\\n\\n\\nWell, it sounds like Python is amazing! However, if you have been using Python for a while, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! An issue commonly known as dependency hell, which is a term associated with the frustration arising from problems managing our project\'s dependencies. \\n\\nDependency hell in Python often happens because pip does not have a dependency resolver and because all dependencies are shared across projects. So, other projects could be affected when a given dependency may need to be updated or uninstalled. \\n\\nOn top of it, since Python doesn\'t distinguish between different versions of the same library in the `/site-packages` directory, this leads to many conflicts when you have two projects requiring different versions of the same library or the global installation doesn\'t match.\\n\\nThus, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./dependency-hell.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\n### Virtual environments to the rescue!\\n\\nA Python virtual environment is a separate folder where only your project\'s dependencies(packages) are located. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and its own independent set of installed Python packages in its site directories. That is a very convenient way to prevent `Dependency Hell.`\\n\\n:::tip\\nPython virtual environment allows multiple versions of Python to coexist in the same machine, so you can test your application using different Python versions. It also keeps your project\'s dependencies isolated, so they don\'t interfere with the dependencies of others projects.\\n:::\\n\\nThere are different tools out there that can be used to create Python virtual environments. In this post, I will show you how to use pyenv and poetry. However, you can also try other tools, such as [virtualenv](https://virtualenv.pypa.io/en/latest/) or anaconda, and based on your experience, you can choose that one you feel most comfortable with.\\nthe video below will provide you with more information about these kinds of tools.\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/3J02sec99RM\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Pyenv\\npyenv is a command line tool which allows you to install and run multiple versions of Python in the same machine. For those who come from a javascript background, pyenv is a very similar tool to nvm.\\n\\n**Setup & get started with pyenv**\\n\\nYou can follow the steps below for installing `pyenv` on macOS or check the [documentation](https://github.com/pyenv/pyenv) for alternative installation methods. \\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl https://pyenv.run | bash\', comment: \\"Install pyenv\\"},\\n{type: \'output\', value: \'Installing pyenv...\'},\\n{type: \'output\', value: \'Installation complete!\'}\\n]} />\\n\\n\\nAfter having installed pyenv, you can then install any python version running the command `pyenv install <version>`.\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv install 3.9.0\', comment: \\"Install python 3.9.0 in my machine\\"},\\n{type: \'output\', value: \'Downloading Python-3.9.0.tar.xz...\'},\\n{type: \'output\', value: \'-> https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xz\', delay: 1000},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installed Python-3.9.0 to /Users/haruiz/.pyenv/versions/3.9.0\'}\\n]} />\\n\\n:::tip\\nif you are not sure about which versions are available to be installed in your machine, you can run the command `pyenv install --list`.\\n:::\\n\\nYou can run the command `pyenv versions` to check which Python versions have been installed.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv versions\' , comment: \\"Check which versions of Python are installed\\"},\\n{type: \'output\', value: \'system\'},\\n{type: \'output\', value: \'* 3.10.0 (set by /Users/haruiz/.pyenv/version)\'},\\n{type: \'output\', value: \'3.9.0\'}\\n]} />\\n\\nTo set the default version of Python to be used, you can run the command `pyenv global <version>`. This version will be used when you run `python` or `python3` in your terminal.\\n\\n<TermynalReact lines ={[\\n{type: \'input\', value: \'pyenv global 3.10.0\', comment: \\"Set python 3.10.0 as the default version\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nAlternatively to the `pyenv global` command, Sometimes you want to set a specific version of Python to be used within a specific folder. You can create a `.python-version` file in the folder and set the version you want to use,  or by running the command `pyenv local <version>`. pyenv will then use this version when you run `python` or `python3` in the folder.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'mkdir myproject\', comment: \\"Create a folder called myproject\\"},\\n{type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n{type: \'input\', value: \'pwd\',  comment: \\"Check the current directory after the cd command\\"},\\n{type: \'output\', value: \'/Users/haruiz/myproject\'},\\n{type: \'input\', value: \'pyenv local 3.9.0\', comment: \\"Set python 3.9.0 as the default version in myproject\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nTo make sure what python version is being used by pyenv, you can run the command `pyenv version`.\\n\\n### Poetry\\n\\nPoetry is a tool that allows you to manage your project\'s dependencies and facilitates the process of packaging for distribution. It resolves your project dependencies and makes sure that there are no conflicts between them.\\n\\nPoetry integrates with the [PyPI](https://pypi.org/) package index to find and install your environment dependencies, and pyenv to set your project python runtime.\\n\\nTo install poetry we follow the steps below:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\', comment: \\"Install poetry\\"},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installation complete!\'},\\n{type: \\"input\\", value: `export PATH=\\"\\\\$HOME/.poetry/bin:\\\\$PATH\\"`, comment: \\"Add poetry to the PATH\\"},\\n{type: \'input\', value: \'poetry --version\', comment: \\"Check the version of poetry after installing it\\"},\\n{type: \'output\', value: \'Poetry version 1.1.13\'},\\n{type: \'input\', value: \'poetry help completions\', comment: \\"Check the completions of poetry\\"},\\n{type: \'output\', value: \'poetry completions bash\'},\\n{type: \'input\', value: \'poetry config virtualenvs.in-project true\', comment: \\"Configure poetry to create virtual environments inside the project\'s root directory\\"}\\n]} />\\n\\nIf you were able to run the previous commands, we can then move forward with the rest of the tutorial.\\n\\nTo ask poetry to create a new project, we use the command `poetry new <project name>`. \\nThis will create a new folder with the name `<project name>` and a `pyproject.toml` folder inside it.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry new myproject\', comment: \\"Create a new project called myproject\\"},\\n    {type: \'output\', value: \'Created package myproject in myproject\'}\\n]} />\\n\\nIf you already have a project, and you want to use poetry to manage the dependencies, you can use the command `poetry init`. So, poetry will add the `pyproject.toml` file to your project.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n    { type: \'input\', value: \'poetry init\', comment: \\"Initialize poetry in myproject\\"},\\n]} />\\n\\n\\nThe main file of your poetry project is the `pyproject.toml` file. This file defines your project\'s dependencies(python packages) and holds the required metadata for packaging. Poetry updates this file every time a new python package is installed. By sharing this file with others, they can recreate your project environment and run your application. To do so, they will need to have poetry installed and run the command `poetry install` within the same folder where the `pyproject.toml` file is located.\\n\\nNow we can start adding dependencies to our project. To do so, we use the command `poetry add <package name>`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry add numpy pandas\', comment: \\"Add numpy and pandas to the project, this command replaces the pip install command\\"},\\n    {type: \'output\', value: \'Installed requests\'}\\n]} />\\n\\nNow our `pyproject.toml` file looks like:\\n\\n```toml\\n    [tool.poetry]\\n    name = \\"myproject\\"\\n    version = \\"0.1.0\\"\\n    description = \\"\\"\\n    authors = [`Henry Ruiz  <henry.ruiz.tamu@gmail.com>`]\\n    \\n    [tool.poetry.dependencies]\\n    python = \\"^3.10\\"\\n    numpy = \\"^1.23.1\\"\\n    pandas = \\"^1.4.3\\"\\n    \\n    [tool.poetry.dev-dependencies]\\n    pytest = \\"^5.2\\"\\n    \\n    [build-system]\\n    requires = [\\"poetry-core>=1.0.0\\"]\\n    build-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nLest review that file sections:\\n\\n- **\\\\[tool.poetry\\\\]:** This section contains informational metadata about our package, such as the package name, description, author details, etc. Most of the config values here are optional unless you\'re planning on publishing this project as an official PyPi package. \\n- **\\\\[tool.poetry.dependencies\\\\]:** This section defines the dependencies of your project. Here is where you define the python packages that your project requires to run. We can update this file manually if it is needed.\\n- **\\\\[tool.poetry.dev-dependencies\\\\]:** This section defines the dev dependencies of your project. These dependencies are not required for your project to run, but they are useful for development.\\n- **\\\\[build-system\\\\]:** This is rarely a section you\'ll need to touch unless you upgrade your version of Poetry.\\n\\nTo see in a nicer format the dependencies of your project, you can use the command `poetry show --tree`. This command draws a graph of all of our dependencies as well as the dependencies of our dependencies.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --tree\', comment: \\"Show the dependencies of our project\\"}]} />\\n\\nIf we are not sure at some point that we have the latest version of a dependency, we can tell poetry to check on our package repository if there is a new version by using \u201c\u2014 latest\u201d option\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --latest\', comment: \\"Show the latest version of our dependencies\\"}]} />\\n\\nIf we list our folder content, we will see that not only the `pyproject.toml` file is created, but also some other folders and files. So, let\'s take a look at the contents of the `myproject` folder.\\n\\n```bash\\n\u251c\u2500\u2500 .venv\\n\u2502\xa0\xa0 \u251c\u2500\u2500 .gitignore\\n\u2502\xa0\xa0 \u251c\u2500\u2500 bin\\n\u2502\xa0\xa0 \u251c\u2500\u2500 lib\\n\u2502\xa0\xa0 \u2514\u2500\u2500 pyvenv.cfg\\n\u251c\u2500\u2500 README.rst\\n\u251c\u2500\u2500 myproject\\n\u2502\xa0\xa0 \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 poetry.lock\\n\u251c\u2500\u2500 pyproject.toml\\n\u2514\u2500\u2500 tests\\n    \u251c\u2500\u2500 __init__.py\\n    \u2514\u2500\u2500 test_myproject.py\\n\\n5 directories, 11 files\\n```\\n\\n- **\\\\`.venv\\\\`**: This folder is created by poetry when it creates a virtual environment.It isolates the project from the system environment and provides a clean environment for your project. It contains the Python interpreter and your projects dependencies. \\n- **poetry.lock**: When Poetry finished installing the dependencies, it writes all of the packages and the exact versions of them to the poetry.lock file, locking the project to those specific versions. \\n\\n:::note\\n Notice that this folder structure is created only if the `poetry new myproject` was executed. When poetry is initialized within a folder that already exists ( using the `poetry init` command), only the `pryproject.toml` and the .env folder are created.\\n:::\\n\\n:::tip\\nYou should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies. For more info, check this link : [Poetry basic usage](https://python-poetry.org/docs/basic-usage/)\\n:::\\n\\nBuilding our project and publishing it is just running the ```poetry build``` and ```poetry publish``` commands, so it is pretty intuitive. The publish command will submit our application to pip, so other developers can easily install it.\\n\\n### Hands-on tutorial \\n\\n**Creating a python package using poetry**\\n\\nIn this section, you will learn how to create a simple python package named `style_image` with poetry. This simple python package takes two images, the style image, and the content image, and performs style transfer. \\"Style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together, so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.\\"<a href=\\"#tensorflow-docs:1\\">[2]</a>\\n\\nFor our `style_image` package we will use the `magenta/arbitrary-image-stylization-v1-256` model available in TensorflowHub under-the-hood.\\n\\nSo, let\'s do it!!\\n\\nWe will start by creating a new project called `style_image` using the command `poetry new style_image`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'pyenv version\', comment: \\"Check the version of python that is being used by pyenv, it would be the python version that will be used by poetry\\"},\\n    { type: \'input\', value: \'poetry new style_image\', comment: \\"Create a new project called style_image\\"},\\n]} />\\n\\n**Installing package dependencies**\\n\\nNext we are going to install the dependencies of our project, so we run the commands:\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd style_image\', comment: \\"Move into the style_image folder where the `pyproject.toml` file is located\\"},\\n    { type: \'input\', value: \'poetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\', comment: \\"Add the dependencies to the project\\", lineDelay: 10.0},\\n    {type: \\"output\\", value: \\"Updating dependencies\\"}, \\n    {type: \\"output\\", value: \\"Resolving dependencies...\\"}, \\n    {type: \\"progress\\", progressPercent: 50},\\n    { type: \'output\', value: `Solver Problem : \\\\n \\nThe current project\\\\\'s Python requirement (>=3.10,<4.0) \\nis not compatible with \\nsome of the required packages Python requirement..... \\\\n\\nFor tensorflow, a possible solution would be to set the \'python\' property to \\">=3.10,<3.11\\"\\n                    `, color: \\"red\\"},\\n]} />\\n\\nWe will see that there is an error trying to install tensorflow:\\n```bash showLineNumbers {14-15}\\nCreating virtualenv style-image in /Users/haruiz/temp/style_image/.venv\\nUsing version ^0.12.0 for tensorflow-hub\\nUsing version ^2.9.1 for tensorflow\\nUsing version ^1.23.1 for numpy\\nUsing version ^9.2.0 for Pillow\\nUsing version ^0.20.0 for validators\\nUsing version ^0.6.1 for typer\\n\\nUpdating dependencies\\nResolving dependencies... (4.2s)\\n\\nSolverProblemError\\n\\nThe current project\'s Python requirement `(>=3.10,<4.0)` is not compatible with some of the required packages Python requirement:\\n- tensorflow-io-gcs-filesystem requires Python `>=3.7, <3.11`, so it will not be satisfied for Python `>=3.11,<4.0`\\n```\\n\\nThe great thing is that poetry generally provides information on how to fix them. For the error above, poetry suggests restricting the python property to `>=3.10,<3.11` in the pyproject.toml file.\\nFor tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to `>=3.10,<3.11`\\n\\n:::tip\\nMake sure you always check the output in the terminal.\\n:::\\n\\nSo the `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nWe can then try to install the dependencies again:\\n\\n```bash\\npoetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\\n```\\n\\nAfter installing the dependencies, our `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9-15}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\ntensorflow-hub = \\"^0.12.0\\"\\nnumpy = \\"^1.23.1\\"\\nPillow = \\"^9.2.0\\"\\ntensorflow = \\"^2.9.1\\"\\nvalidators = \\"^0.20.0\\"\\ntyper = {extras = [\\"all\\"], version = \\"^0.6.1\\"}\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\n**Coding our `style_image` package**\\n\\nAt this point, we are ready to start coding, let\'s create the folder structure below and replace the code in each .py file with the code on this repository [https://github.com/haruiz/style_image](https://github.com/haruiz/style_image):\\n```bash\\n    \u251c\u2500\u2500 README.md\\n    \u251c\u2500\u2500 README.rst\\n    \u251c\u2500\u2500 data\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 content_image.jpg\\n    \u251c\u2500\u2500 main.py\\n    \u251c\u2500\u2500 poetry.lock\\n    \u251c\u2500\u2500 pyproject.toml\\n    \u251c\u2500\u2500 style_image\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 core\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 style_image.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 main.py\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 util\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __pycache__\\n    \u2502\xa0\xa0     \u2514\u2500\u2500 image_utils.py\\n    \u251c\u2500\u2500 stylized_image.png\\n    \u2514\u2500\u2500 tests\\n        \u251c\u2500\u2500 __init__.py\\n        \u2514\u2500\u2500 test_style_image.py\\n```\\nCode :\\n\\n<Tabs>\\n  <TabItem value=\\"main.py\\" label=\\"main.py\\" default>\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\n\\nif __name__ == \\"__main__\\":\\n\\n    content_image_path = \\"data/content_image.jpg\\"\\n    style_image_path = \\"data/style_image.jpg\\"\\n\\n    stylized_image = (\\n        StyleImage(style_image_path)\\n        .transfer(content_image_path, output_image_size=800)\\n        .save(\\"stylized_image.jpg\\")\\n    )\\n```\\n\\n</TabItem>\\n  <TabItem value=\\"core/style_image.py\\" label=\\"core/style_image.py\\">\\n\\n```python showLineNumbers \\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\n\\nfrom style_image.util import ImageUtils\\nfrom PIL import Image as PILImage\\n\\n\\nclass StyleImage:\\n    def __init__(self, style_image_path):\\n        self._style_image_path = style_image_path\\n        hub_handle = (\\n            \\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\\"\\n        )\\n        self._hub_module = hub.load(hub_handle)\\n\\n    def transfer(\\n        self, content_image_path, output_image_size=384, style_img_size=(256, 256)\\n    ):\\n        \\"\\"\\"\\n        transfer the style of the style image to the content image\\n        :param content_image_path: image path of the content image :\\n        :param output_image_size: The content image size can be arbitrary.\\n        :param style_img_size: The style prediction model was trained with image size 256 and it\'s the\\n        recommended image size for the style image (though, other sizes work as\\n        well but will lead to different results).\\n        Recommended to keep it at 256.\\n        :return:\\n        \\"\\"\\"\\n        content_img_size = (output_image_size, output_image_size)\\n        # Load the content and style images.\\n        content_image = ImageUtils.load_image(content_image_path, content_img_size)\\n        style_image = ImageUtils.load_image(self._style_image_path, style_img_size)\\n        # Stylize image.\\n        stylized_image_tensor = self._hub_module(\\n            tf.constant(content_image), tf.constant(style_image)\\n        )[0]\\n        stylized_image_arr = tf.image.convert_image_dtype(\\n            stylized_image_tensor, tf.uint8\\n        ).numpy()\\n        stylized_image_arr = stylized_image_arr[0]  # Remove batch dimension.\\n        stylized_image = PILImage.fromarray(stylized_image_arr)\\n        return stylized_image\\n```\\n\\n  </TabItem>\\n  <TabItem value=\\"util/image_utils.py\\" label=\\"util/image_utils.py\\">\\n\\n```python showLineNumbers \\nimport functools\\nimport tensorflow as tf\\nimport os\\nimport validators\\n\\n\\nclass ImageUtils:\\n    @staticmethod\\n    def crop_center(image):\\n        \\"\\"\\"Returns a cropped square image.\\"\\"\\"\\n        shape = image.shape\\n        new_shape = min(shape[1], shape[2])\\n        offset_y = max(shape[1] - shape[2], 0) // 2\\n        offset_x = max(shape[2] - shape[1], 0) // 2\\n        image = tf.image.crop_to_bounding_box(\\n            image, offset_y, offset_x, new_shape, new_shape\\n        )\\n        return image\\n\\n    @classmethod\\n    @functools.lru_cache(maxsize=None)\\n    def load_image(cls, image_path, image_size=(256, 256), \\n                   preserve_aspect_ratio=True):\\n        \\"\\"\\"Loads and preprocesses images.\\"\\"\\"\\n        # Cache image file locally.\\n        if validators.url(image_path):\\n            image_path = tf.keras.utils.get_file(\\n                os.path.basename(image_path)[-128:], image_path\\n            )\\n        # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\\n        img = tf.io.decode_image(\\n            tf.io.read_file(image_path), channels=3, dtype=tf.float32\\n        )[tf.newaxis, ...]\\n        img = cls.crop_center(img)\\n        img = tf.image.resize(\\n            img, image_size, preserve_aspect_ratio=preserve_aspect_ratio\\n        )\\n        return img\\n\\n``` \\n\\n  </TabItem>\\n<TabItem value=\\"style_image/main.py\\" label=\\"style_image/main.py\\">\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\nimport typer\\n\\napp = typer.Typer()\\n\\n\\ndef style_image_callback(value: str):\\n    style_urls = dict(\\n        kanagawa_great_wave=\\"https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\\",\\n        kandinsky_composition_7=\\"https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\\",\\n        hubble_pillars_of_creation=\\"https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\\",\\n        van_gogh_starry_night=\\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\\",\\n        turner_nantes=\\"https://upload.wikimedia.org/wikipedia/commons/b/b7/JMW_Turner_-_Nantes_from_the_Ile_Feydeau.jpg\\",\\n        munch_scream=\\"https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg\\",\\n        picasso_demoiselles_avignon=\\"https://upload.wikimedia.org/wikipedia/en/4/4c/Les_Demoiselles_d%27Avignon.jpg\\",\\n        picasso_violin=\\"https://upload.wikimedia.org/wikipedia/en/3/3c/Pablo_Picasso%2C_1911-12%2C_Violon_%28Violin%29%2C_oil_on_canvas%2C_Kr%C3%B6ller-M%C3%BCller_Museum%2C_Otterlo%2C_Netherlands.jpg\\",\\n        picasso_bottle_of_rum=\\"https://upload.wikimedia.org/wikipedia/en/7/7f/Pablo_Picasso%2C_1911%2C_Still_Life_with_a_Bottle_of_Rum%2C_oil_on_canvas%2C_61.3_x_50.5_cm%2C_Metropolitan_Museum_of_Art%2C_New_York.jpg\\",\\n        fire=\\"https://upload.wikimedia.org/wikipedia/commons/3/36/Large_bonfire.jpg\\",\\n        derkovits_woman_head=\\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Derkovits_Gyula_Woman_head_1922.jpg\\",\\n        amadeo_style_life=\\"https://upload.wikimedia.org/wikipedia/commons/8/8e/Untitled_%28Still_life%29_%281913%29_-_Amadeo_Souza-Cardoso_%281887-1918%29_%2817385824283%29.jpg\\",\\n        derkovtis_talig=\\"https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg\\",\\n        amadeo_cardoso=\\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Amadeo_de_Souza-Cardoso%2C_1915_-_Landscape_with_black_figure.jpg\\",\\n    )\\n    if value in style_urls:\\n        return style_urls[value]\\n    return value\\n\\n\\n@app.command()\\ndef main(\\n    style_image: str = typer.Option(\\n        ..., \\"--style_image\\", \\"-s\\", callback=style_image_callback\\n    ),\\n    content_image: str = typer.Option(..., \\"--content_image\\", \\"-c\\"),\\n    output_image_size: int = typer.Option(384, \\"--output_image_size\\", \\"-sz\\"),\\n    output_image_path: str = typer.Option(\\"stylized_image.png\\", \\"--output_image_path\\", \\"-o\\"),\\n):\\n    style_image = StyleImage(style_image)\\n    stylized_image = style_image.transfer(content_image, output_image_size=output_image_size)\\n    stylized_image.save(output_image_path)\\n\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n:::note running your scripts using the virtual environment\\nNotice that if you want to execute the `main.py` file or any other file/script using the python environment you just created, you need to run the command `poetry run python main.py.` So, poetry knows that you are running the `main.py` file with the python environment created for the `style_image` package.\\nIf you feel more comfortable running `python main.py,` instead of running `poetry run ...` you can permanently activate the environment running the command `poetry shell.`. So it will be activated for all the commands you run.\\n:::\\n\\nPoetry and pyenv are integrated with `visual studio` code and `Pycharm`. In fact, they will automatically recognize the python environment created by poetry.\\n\\n**Publishing our package to PyPi**\\n\\nPublishing our package in Pypi should be straightforward. We just run the `poetry publish` command. Since this is just a demo, we are going to publish our package to the pypi test repository `https://test.pypi.org/.` However, the steps should be the same in production `https://pypi.org/.`\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry build\', comment: \\"build package\\" },\\n    { type: \'input\', value: \'poetry config repositories.testpypi https://test.pypi.org/legacy/\', comment: \\"add repository\\" },\\n    { type: \'input\', value: \'poetry config repositories\', comment: \\"list repositories\\" },\\n    { type: \'output\', value: `{\'testpypi\': {\'url\': \'https://test.pypi.org/\'}}`, color: \\"gray\\" },\\n    { type: \'input\', value: \'poetry publish -r testpypi\', comment: \\"publish package to testpypi repository\\" },\\n    { type: \'prompt\', value: \'username : haruiz\'},\\n    { type: \'prompt\', value: \'password\'},\\n    { type: \\"output\\", value: \\"Publishing style_image (0.1.0) to testpypi..\\", color:\\"green\\"}\\n  ]} />\\n\\nIf the `publish` command is successful, you will be able to find the package in the testpypi repository.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./pypi-test.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\nThat is all!! We are done!!. You can check the links below for more information about poetry. \\n\\nThanks for your support and don\'t forget to share,\\n\\n\\n**Some useful resources**\\n- [Poetry Documentation](https://python-poetry.org/)\\n- [Pyenv Documentation](https://github.com/pyenv/pyenv)\\n- [Great talk about poetry](https://www.youtube.com/watch?v=QX_Nhu1zhlg&ab_channel=PyGotham2019)\\n- [Package Python Projects the Proper Way with Poetry](https://hackersandslackers.com/python-poetry-package-manager/)\\n- [Poetry: Finally an all-in-one tool to manage Python packages](https://medium.com/analytics-vidhya/poetry-finally-an-all-in-one-tool-to-manage-python-packages-3c4d2538e828)\\n- [Making Python Packages Part 2: How to Publish & Test Your Package on PyPI with Poetry](https://towardsdatascience.com/packages-part-2-how-to-publish-test-your-package-on-pypi-with-poetry-9fc7295df1a5)\\n- [Publishing to a private Python repository with Poetry](https://medium.com/packagr/publishing-to-a-private-python-repository-with-poetry-23b660484471)\\n- [Python Virtual Environments tutorial using Virtualenv and Poetry](https://serpapi.com/blog/python-virtual-environments-using-virtualenv-and-poetry/)\\n- [The Nine Circles of Python Dependency Hell](https://medium.com/knerd/the-nine-circles-of-python-dependency-hell-481d53e3e025)\\n- [Get started with pyenv & poetry. Saviours in the python chaos!](https://blog.jayway.com/2019/12/28/pyenv-poetry-saviours-in-the-python-chaos/)\\n\\n**References**\\n<ul>\\n <li><a id=\\"wikipedia:1\\" href=\\"https://en.wikipedia.org/wiki/Python_Package_Index\\" target=\\"_blank\\">[1] Python Package Index</a></li>\\n <li><a id=\\"tensorflow-docs:1\\" href=\\"https://www.tensorflow.org/tutorials/generative/style_transfer\\" target=\\"_blank\\">[2] Neural style transfer</a></li>\\n</ul>"}]}}')}}]);