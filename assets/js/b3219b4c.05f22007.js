"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3099],{3890:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"accelerating-science-with-jax-simulations-physics-and-beyond","metadata":{"permalink":"/blog/accelerating-science-with-jax-simulations-physics-and-beyond","source":"@site/blog/2025-06-05-accelerating-science-with-jax-simulations-physics-and-beyond/index.mdx","title":"Accelerating Science with JAX - Simulations, Physics, and Beyond","description":"This post explores how JAX is reshaping modern scientific computing by enabling high-performance, differentiable, and hardware-accelerated workflows across a wide range of applications. From modeling complex systems and solving differential equations to training large-scale geospatial and machine learning models, we introduce powerful libraries like Diffrax, fdtdx, jax-md, Equinox, and Jeo. Learn how JAX supports scalable, composable research pipelines that span domains such as engineering, biology, Earth observation, and AI.","date":"2025-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"},{"inline":true,"label":"jax","permalink":"/blog/tags/jax"},{"inline":true,"label":"scientific-computing","permalink":"/blog/tags/scientific-computing"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"simulations","permalink":"/blog/tags/simulations"},{"inline":true,"label":"physics","permalink":"/blog/tags/physics"},{"inline":true,"label":"electromagnetics","permalink":"/blog/tags/electromagnetics"},{"inline":true,"label":"geosciences","permalink":"/blog/tags/geosciences"},{"inline":true,"label":"earth-observation","permalink":"/blog/tags/earth-observation"}],"readingTime":9.17,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Accelerating Science with JAX - Simulations, Physics, and Beyond","slug":"accelerating-science-with-jax-simulations-physics-and-beyond","description":"This post explores how JAX is reshaping modern scientific computing by enabling high-performance, differentiable, and hardware-accelerated workflows across a wide range of applications. From modeling complex systems and solving differential equations to training large-scale geospatial and machine learning models, we introduce powerful libraries like Diffrax, fdtdx, jax-md, Equinox, and Jeo. Learn how JAX supports scalable, composable research pipelines that span domains such as engineering, biology, Earth observation, and AI.","authors":["haruiz"],"tags":["python","data-science","jax","scientific-computing","machine-learning","simulations","physics","electromagnetics","geosciences","earth-observation"]},"unlisted":false,"nextItem":{"title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","permalink":"/blog/implementing-anthropic\'s-agent-design-patterns-with-google-adk"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!-- truncate --\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\n\\nAfter working hands-on with [JAX](https://github.com/google/jax)\u2014particularly through libraries like [`fdtdx`](https://github.com/ymahlau/fdtdx) for accelerating electromagnetic simulations\u2014I\'ve come to see it not just as a tool for deep learning, but as a core engine for modern scientific computing. What makes JAX stand out is its elegant fusion of NumPy-like syntax, automatic differentiation, and hardware-agnostic acceleration across CPUs, GPUs, and TPUs\u2014all wrapped in a functional programming model that encourages composability and clarity.\\n\\nBeyond its foundational capabilities, high-level libraries like NNX (see example below) further extend JAX\u2019s flexibility, making it easy to build modular, expressive, and differentiable models with minimal boilerplate. This powerful ecosystem positions JAX as a go-to framework not only for machine learning, but also for developing high-performance simulations across disciplines such as physics, electromagnetics, and scientific optimization\u2014unlocking workflows that demand both numerical precision and differentiable programming.\\n\\n```python\\nfrom flax import nnx\\nimport optax\\n\\n\\nclass Model(nnx.Module):\\n  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\\n    self.linear = nnx.Linear(din, dmid, rngs=rngs)\\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\\n    self.dropout = nnx.Dropout(0.2, rngs=rngs)\\n    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\\n\\n  def __call__(self, x):\\n    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\\n    return self.linear_out(x)\\n\\nmodel = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\\noptimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\\n\\n@nnx.jit  # automatic state management for JAX transforms\\ndef train_step(model, optimizer, x, y):\\n  def loss_fn(model):\\n    y_pred = model(x)  # call methods directly\\n    return ((y_pred - y) ** 2).mean()\\n\\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\\n  optimizer.update(grads)  # in-place updates\\n\\n  return loss\\n\\nnnx.display(optimizer)\\n```\\n\\n\\nBuilding on these capabilities, in this post I\u2019ll highlight some of the most exciting open-source projects built on JAX and demonstrate how they can be used in scientific computing to model complex physical systems\u2014ranging from solving ordinary and partial differential equations (ODEs/PDEs), to simulating molecular dynamics, electromagnetic wave propagation, fluid dynamics, and geophysical processes. These tools empower researchers to explore phenomena such as heat diffusion, quantum interactions, wave mechanics, subsurface imaging, and environmental transport models with high performance and end-to-end differentiability. I\u2019ll also discuss where these tools can be effectively applied across domains such as physics, engineering, geosciences, earth and environmental systems, and computational biology to optimize and accelerate research workflows.\\n\\nTo run the code examples in this post, you\u2019ll need to install JAX. You can do this by running the following command in your terminal or in the environment where you plan to run the examples:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pip install -U \\"jax[cuda12]\\" -q\' },\\n]} />\\n\\n\\n:::tip\\nUse the following code snippet to verify that JAX is installed correctly and to determine which hardware accelerator (CPU, GPU, or TPU) is being used:\\n```python\\nimport jax\\n\\n# List all available devices\\ndevices = jax.devices()\\nprint(devices)\\nprint(jax.default_backend())\\nprint(jax.lib.xla_bridge.get_backend().platform)\\n```\\n:::\\n\\n##  1. Differentiable ODEs with [`Diffrax`](https://github.com/patrick-kidger/diffrax)\\n\\nJAX\u2019s powerful automatic differentiation system enables seamless integration of differential equations into loss functions and optimization routines. Building on this foundation, **Diffrax** provides state-of-the-art ODE solvers that are fully compatible with `jax.grad`, enabling end-to-end differentiability. This extends JAX\u2019s capabilities far beyond neural network training, transforming it into a versatile tool for solving both ordinary differential equations (ODEs) and partial differential equations (PDEs) within a unified, gradient-based framework.\\n\\n### Applications\\n\\nDiffrax can be applied across a wide range of scientific and engineering domains, including:\\n\\n* **Physics-informed neural networks (PINNs)** \u2013 Solve ODEs and PDEs by incorporating physical laws directly into model training.\\n* **Parameter estimation and system identification** \u2013 Learn unknown parameters of dynamical systems using gradient-based optimization.\\n* **Optimal control and trajectory planning** \u2013 Optimize control inputs for physical systems such as robots or autonomous vehicles.\\n* **Biological and ecological modeling** \u2013 Fit and simulate population dynamics, gene regulation networks, or epidemiological models.\\n* **Financial modeling** \u2013 Solve stochastic differential equations for pricing, forecasting, or risk assessment.\\n\\nThe following example demonstrates how to solve a simple ODE using Diffrax:\\n\\n### Example: Solve\\n\\n$$\\ny(0)=1 \\\\quad \\\\frac{\\\\mathrm{~d} y}{\\\\mathrm{~d} t}(t)=-y(t)\\n$$\\n\\nover the interval $[0,3]$.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pip install -U diffrax -q\' },\\n]} />\\n\\n\\n```python\\nfrom diffrax import diffeqsolve, Dopri5, ODETerm, SaveAt, PIDController\\n\\nvector_field = lambda t, y, args: -y\\nterm = ODETerm(vector_field)\\nsolver = Dopri5()\\nsaveat = SaveAt(ts=[0., 1., 2., 3.])\\nstepsize_controller = PIDController(rtol=1e-5, atol=1e-5)\\n\\nsol = diffeqsolve(term, solver, t0=0, t1=3, dt0=0.1, y0=1, saveat=saveat,\\n                  stepsize_controller=stepsize_controller)\\n\\nprint(sol.ts)\\nprint(sol.ys)\\n```\\n\\n\\n## 2. Electromagnetic Simulation with [`fdtdx`](https://github.com/ymahlau/fdtdx)\\n\\n\\n[`fdtdx`](https://github.com/ymahlau/fdtdx) is a JAX-based solver for simulating electromagnetic wave propagation using the finite-difference time-domain (FDTD) method\u2014a widely adopted numerical approach for solving Maxwell\u2019s equations. The library supports 1D, 2D, and 3D simulations with GPU acceleration through JAX\'s `jit` compilation.\\n\\nWhat makes `fdtdx` particularly powerful is its ability to run **differentiable simulations**, enabling gradient-based optimization and control. This feature opens the door to integrating electromagnetic simulations directly into learning and optimization pipelines, dramatically accelerating scientific and engineering workflows.\\n\\n### Applications\\n\\n`fdtdx` can be used in a wide range of scientific and engineering domains, including:\\n\\n* **Antenna design** \u2013 Automatically optimize shape, size, and placement to achieve desired radiation patterns.\\n* **Waveguide and photonic circuit analysis** \u2013 Simulate and refine structures for efficient light propagation.\\n* **Metamaterial and photonic crystal engineering** \u2013 Design custom materials with tailored electromagnetic properties.\\n* **Inverse design problems** \u2013 Use gradients to iteratively improve device geometry or material distribution.\\n* **Differentiable physics-informed ML** \u2013 Integrate Maxwell-compliant simulations into neural network training.\\n\\n\\n## 3. Molecular Simulation with [`jax-md`](https://github.com/jax-md/jax-md)\\n\\njax-md is a high-performance, differentiable molecular dynamics engine built with JAX, designed for simulating particles, materials, and custom potentials. Molecular dynamics is a key tool in computational condensed matter physics, used to explore how microscopic interactions give rise to macroscopic behavior.\\n\\n### Applications\\n\\n`jax-md` can be applied in various domains, including:\\n* **Materials science** \u2013 Simulate crystal structures, defects, and phase transitions.\\n* **Biophysics** \u2013 Model protein folding, ligand binding, and molecular interactions.\\n* **Chemistry** \u2013 Study reaction dynamics, solvent effects, and catalysis.\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Bkm8tGET7-w?si=mshSBt_3T4R_ha9E\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n\\n## 4. Scientific ML with [`Equinox`](https://github.com/patrick-kidger/equinox)\\n\\nEquinox is a flexible JAX library designed for building models\u2014including neural networks and physical systems\u2014using a PyTorch-like syntax. It integrates seamlessly with JAX\'s functional paradigm, offering features like filtered transformation APIs, PyTree manipulation utilities, and support for runtime errors. Equinox models are pure PyTrees, enabling smooth compatibility with JAX transformations such as jit, grad, and vmap. It\'s a great choice for scientific computing workflows that require combining neural and physics-based models within the same differentiable pipeline\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'pip install -U equinox -q\' },\\n]} />\\n\\n\\n```python\\nimport equinox as eqx\\nimport jax\\n\\nclass Linear(eqx.Module):\\n    weight: jax.Array\\n    bias: jax.Array\\n\\n    def __init__(self, in_size, out_size, key):\\n        wkey, bkey = jax.random.split(key)\\n        self.weight = jax.random.normal(wkey, (out_size, in_size))\\n        self.bias = jax.random.normal(bkey, (out_size,))\\n\\n    def __call__(self, x):\\n        return self.weight @ x + self.bias\\n\\n@jax.jit\\n@jax.grad\\ndef loss_fn(model, x, y):\\n    pred_y = jax.vmap(model)(x)\\n    return jax.numpy.mean((y - pred_y) ** 2)\\n\\nbatch_size, in_size, out_size = 32, 2, 3\\nmodel = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\\nx = jax.numpy.zeros((batch_size, in_size))\\ny = jax.numpy.zeros((batch_size, out_size))\\ngrads = loss_fn(model, x, y)\\n```\\n\\n## 5. Model training and inference for geospatial remote sensing and Earth Observation with [`Jeo`](https://github.com/google-deepmind/jeo)\\n\\nJeo is an open-source library developed by Google DeepMind for training machine learning models in geospatial remote sensing and Earth Observation (EO) applications. Built on JAX and Flax, Jeo offers a flexible and scalable framework designed to run seamlessly on CPUs, GPUs, and Google Cloud TPU VMs.\\nJeo provides a comprehensive set of tools for building, training, and evaluating models on large-scale geospatial datasets. It includes support for data loading, preprocessing, model definition, training loops, and evaluation metrics\u2014all optimized for high performance and scalability.\\n\\n![Jeo](jeo_geeflow_processing.png)\\n\\n## Applications\\n\\nJeo is particularly well-suited for applications in Earth Observation and remote sensing, including:\\n\\n* **Land cover classification** \u2013 Classify satellite imagery into different land cover types (e.g., urban, forest, water).\\n* **Change detection** \u2013 Identify changes in land use or land cover over time using multi-temporal satellite data.\\n* **Tracking and monitoring** \u2013 Monitor environmental changes such as deforestation, urbanization, or natural disasters.\\n    - [NeurIPS 2024 Workshop on Tackling Climate Change with Machine Learning](https://www.climatechange.ai/events/neurips2024)\\n* **Object detection** \u2013 Detect and classify objects in satellite imagery, such as buildings, roads, or vehicles.\\n    - [Planted: a dataset for planted forest identification from multi-satellite time series](https://arxiv.org/pdf/2406.18554)\\n\\n## Looking Ahead\\n\\nJAX is transforming scientific programming by:\\n\\n* Unifying simulation, optimization, and differentiation\\n* Supporting GPU/TPU acceleration\\n* Enabling end-to-end differentiable workflows\\n\\n**Coming soon**: A beginner-friendly guide on how to get started with JAX\u2014from installation to writing your first `jit`-compiled function, understanding `grad`, and building your first differentiable simulator.\\n\\n> \u26a1 The future of research is fast, differentiable, and written in JAX.\\n\\n## Conclusion\\n\\nAX goes beyond accelerating model training and inference\u2014it redefines what\u2019s possible in the machine learning context by seamlessly integrating scientific computing into ML workflows. With NumPy-like syntax, automatic differentiation, and XLA-compiled execution across CPUs, GPUs, and TPUs, JAX empowers researchers to build high-performance, end-to-end differentiable systems. Its functional programming paradigm and composable transformations\u2014such as jit, grad, vmap, and pmap\u2014make it easy to define and express complex models, simulators, and optimization routines. Unlike traditional ML libraries focused primarily on neural networks, JAX supports a broader class of models that incorporate physics, dynamics, and numerical solvers\u2014enabling more interpretable, grounded, and scientifically rigorous machine learning.\\n\\nJAX has also played a central role in training large foundation models such as Gemma and Gemini, which were developed using JAX in combination with ML Pathways, Google\u2019s scalable infrastructure for multitask learning. JAX\u2019s ability to leverage modern hardware\u2014especially TPUs\u2014enables faster, more efficient training at massive scale. As detailed in the Gemini research paper, \\"the \'single controller\' programming model of JAX and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\\" This synergy between JAX and Pathways exemplifies how cutting-edge infrastructure and functional design can accelerate the creation of general-purpose AI systems.\\n\\n### References\\n\\n- [Diffrax Documentation](https://docs.kidger.site/diffrax/)\\n- [Using JAX to accelerate our research](https://deepmind.google/discover/blog/using-jax-to-accelerate-our-research/)\\n- [A flexible framework for large-scale FDTD simulations: open-source inverse design for 3D nanostructures](https://arxiv.org/html/2412.12360v2)\\n- [JAX, M.D.: A Framework for Differentiable Physics](https://arxiv.org/abs/1912.04232)\\n- [Equinox Documentation](https://docs.kidger.site/equinox/)\\n- [Jeo Documentation](https://google-deepmind.github.io/jeo/)\\n- [Gemma 3 model card](https://ai.google.dev/gemma/docs/core/model_card_3#:~:text=to%20operate%20sustainably.-,Software,dramatically%20simplifying%20the%20development%20workflow.%22)\\n- [Deep Learning with JAX](https://www.manning.com/books/deep-learning-with-jax)"},{"id":"implementing-anthropic\'s-agent-design-patterns-with-google-adk","metadata":{"permalink":"/blog/implementing-anthropic\'s-agent-design-patterns-with-google-adk","source":"@site/blog/2025-04-30-implementing-anthropic\'s-agent-design-patterns-with-google-adk/index.md","title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","description":"Agentic systems are rapidly becoming a core design pattern for LLM-powered applications, enabling dynamic reasoning, decision-making, and tool use. Inspired by Anthropic\u2019s influential Building Effective Agents article, this post demonstrates the implementation of their proposed agent design patterns\u2014such as prompt chaining, routing, and parallelization\u2014using Google\u2019s open-source Agent Development Kit (ADK). The guide provides practical, hands-on examples that illustrate how these patterns work and where they can be effectively applied.","date":"2025-04-30T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"agentic-systems","permalink":"/blog/tags/agentic-systems"},{"inline":true,"label":"llm-agents","permalink":"/blog/tags/llm-agents"},{"inline":true,"label":"agent-development-kit","permalink":"/blog/tags/agent-development-kit"},{"inline":true,"label":"prompt-engineering","permalink":"/blog/tags/prompt-engineering"},{"inline":true,"label":"ai-workflows","permalink":"/blog/tags/ai-workflows"},{"inline":true,"label":"autonomous-agents","permalink":"/blog/tags/autonomous-agents"},{"inline":true,"label":"multi-agent-systems","permalink":"/blog/tags/multi-agent-systems"},{"inline":true,"label":"adk","permalink":"/blog/tags/adk"}],"readingTime":26.55,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","slug":"implementing-anthropic\'s-agent-design-patterns-with-google-adk","description":"Agentic systems are rapidly becoming a core design pattern for LLM-powered applications, enabling dynamic reasoning, decision-making, and tool use. Inspired by Anthropic\u2019s influential Building Effective Agents article, this post demonstrates the implementation of their proposed agent design patterns\u2014such as prompt chaining, routing, and parallelization\u2014using Google\u2019s open-source Agent Development Kit (ADK). The guide provides practical, hands-on examples that illustrate how these patterns work and where they can be effectively applied.","authors":["haruiz"],"tags":["python","agentic-systems","llm-agents","agent-development-kit","prompt-engineering","ai-workflows","autonomous-agents","multi-agent-systems","adk"]},"unlisted":false,"prevItem":{"title":"Accelerating Science with JAX - Simulations, Physics, and Beyond","permalink":"/blog/accelerating-science-with-jax-simulations-physics-and-beyond"},"nextItem":{"title":"Getting Started with Ray on Google Cloud Platform","permalink":"/blog/getting-started-with-ray-on-google-cloud-platform"}},"content":"\x3c!-- truncate --\x3e\\n\\n\\nAnthropic\u2019s <a href=\\"#anthropic:1\\">\\"Building Effective Agents\\"[1]</a> article has quickly become a widely referenced guide for designing agentic systems, particularly because it introduces the conceptual and functional differences between Workflows and Agents.\\n\\n:::quote[Anthropic]\\n\\n**Workflows** are systems where LLMs and tools are orchestrated through predefined code paths. \\n**Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\n::::\\n\\nWorkflows provide structure, determinism, and reproducibility\u2014making them ideal for applications with well-defined logic and fixed execution paths. Agents, in contrast, offer flexibility and autonomy, enabling LLMs to direct their own reasoning, planning, and tool use to make decisions and solve problems. This autonomy makes agent-based systems especially effective in open-ended, interactive, or partially observable environments.\\n\\nThis post explores Anthropic\u2019s proposed agent design patterns and shows how to implement them using Google\u2019s open-source Agent Development Kit (ADK). ADK provides a flexible and extensible framework for building, orchestrating, and deploying LLM-powered agents, making it a practical choice for bringing these concepts into real-world applications.\\n\\n:::tip[In This Post You Will Learn]\\n\\n- The core differences between **Workflows** and **Agents**, as introduced by Anthropic, and when to use each approach\\n- How to implement six of the foundational agent design patterns from Anthropic\u2019s article using Google ADK:\\n    - \ud83d\udd01 **Prompt Chaining** for step-by-step task decomposition\\n    - \ud83d\udd00 **Routing** for directing inputs to specialized agents\\n    - \u26a1 **Parallelization** to run tasks concurrently using sectioning and voting\\n    - \ud83e\udde0 **Orchestrator\u2013Worker** pattern for dynamic task delegation and synthesis\\n    - \ud83e\uddea **Evaluator\u2013Optimizer** loop for iterative refinement based on structured feedback\\n    - \ud83e\udd16 **Autonomous Agents** that reason, plan, and act independently in open-ended environments\\n- How to integrate tools, memory, and execution loops using ADK framework\\n:::\\n\\n## Hands-on Examples\\n\\nThe foundation for all the agent patterns explored and implemented in this post begins with a core building block: an augmented LLM equipped with tools, memory, and retrieval capabilities. This setup enables the agent to access external knowledge, retain contextual awareness over time, and interact with its environment through purposeful actions. These foundational capabilities will be integrated into the implementation of each agent pattern described in the following sections. The code snippet below demonstrates how to augment an LLM with a tool that retrieves the current weather and time\u2014effectively transforming it into a functional agent.\\n\\n```python\\nimport datetime\\nfrom zoneinfo import ZoneInfo\\n\\nfrom google.adk import Runner\\nfrom google.adk.agents import Agent\\nfrom dotenv import load_dotenv\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\nfrom rich.panel import Panel\\nfrom rich import print\\n\\nload_dotenv(verbose=True)\\n\\ndef get_weather(city: str) -> dict:\\n    \\"\\"\\"Retrieves the current weather report for a specified city.\\n\\n    Args:\\n        city (str): The name of the city for which to retrieve the weather report.\\n\\n    Returns:\\n        dict: status and result or error msg.\\n    \\"\\"\\"\\n    if city.lower() == \\"new york\\":\\n        return {\\n            \\"status\\": \\"success\\",\\n            \\"report\\": (\\n                \\"The weather in New York is sunny with a temperature of 25 degrees\\"\\n                \\" Celsius (41 degrees Fahrenheit).\\"\\n            ),\\n        }\\n    else:\\n        return {\\n            \\"status\\": \\"error\\",\\n            \\"error_message\\": f\\"Weather information for \'{city}\' is not available.\\",\\n        }\\n\\n\\ndef get_current_time(city: str) -> dict:\\n    \\"\\"\\"Returns the current time in a specified city.\\n\\n    Args:\\n        city (str): The name of the city for which to retrieve the current time.\\n\\n    Returns:\\n        dict: status and result or error msg.\\n    \\"\\"\\"\\n\\n    if city.lower() == \\"new york\\":\\n        tz_identifier = \\"America/New_York\\"\\n    else:\\n        return {\\n            \\"status\\": \\"error\\",\\n            \\"error_message\\": (\\n                f\\"Sorry, I don\'t have timezone information for {city}.\\"\\n            ),\\n        }\\n\\n    tz = ZoneInfo(tz_identifier)\\n    now = datetime.datetime.now(tz)\\n    report = (\\n        f\'The current time in {city} is {now.strftime(\\"%Y-%m-%d %H:%M:%S %Z%z\\")}\'\\n    )\\n    return {\\"status\\": \\"success\\", \\"report\\": report}\\n\\n\\nroot_agent = Agent(\\n    name=\\"weather_time_agent\\",\\n    model=\\"gemini-2.0-flash\\",\\n    description=(\\n        \\"Agent to answer questions about the time and weather in a city.\\"\\n    ),\\n    instruction=(\\n        \\"You are a helpful agent who can answer user questions about the time and weather in a city.\\"\\n    ),\\n    tools=[get_weather, get_current_time],\\n)\\n\\n# Example usage\\nif __name__ == \'__main__\':\\n\\n    # --- Constants ---\\n    APP_NAME = \\"code_refinement_app\\"\\n    USER_ID = \\"user_123\\"\\n    SESSION_ID = \\"session_456\\"\\n\\n    # --- Services ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n\\n    # Create session once\\n    session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    # --- Runner Setup ---\\n    runner = Runner(\\n        agent=root_agent,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service,\\n    )\\n    prompt = \\"What is the current weather in New York?\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt[/bold white]: {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            response = event.content.parts[0].text\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green] {response}\\", title=\\"\ud83e\udd16\\"))\\n\\n```\\n\\n## Workflow: Prompt chaining\\n\\nThe prompt chaining pattern consists of breaking a complex task into a series of smaller, manageable steps, where each step involves an LLM processing the output of the previous one. This sequential structure allows for greater control and reliability, as you can introduce programmatic checks\u2014or \u201cgates\u201d\u2014at any stage to validate intermediate results and keep the process aligned with the intended goal.\\n\\n**When to use this pattern:**\\n\\nPrompt chaining is ideal for scenarios where a task can be cleanly divided into discrete, well-ordered subtasks. It intentionally trades off some latency to gain higher accuracy by reducing the cognitive load on each individual model invocation.\\n\\n:::tip[Example applications]\\n- Creating marketing content and then translating it into another language\\n- Drafting an outline, validating it against predefined criteria, and using it to generate a full document\\n:::\\n\\n**ADK Implementation:**\\n\\nIn the following example, we implement a simple joke generation workflow that consists of three steps: generating a joke, improving it, and polishing it. Each step is handled by a separate agent, and the output of one agent is passed as input to the next.\\n\\n```python\\nfrom typing import Optional, List, Dict\\nfrom dotenv import load_dotenv\\nfrom rich import print\\nfrom rich.panel import Panel\\n\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent, SequentialAgent\\nfrom google.adk.agents.callback_context import CallbackContext\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.models import LlmRequest, LlmResponse\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\n\\n# --- Load environment variables ---\\nload_dotenv()\\n\\n# --- Constants ---\\nBLOCKED_KEYWORDS = [\\"apple\\"]  # Extendable\\n\\n# --- Agent Configs ---\\nAGENT_CONFIGS: List[Dict] = [\\n    {\\n        \\"name\\": \\"joke_generator\\",\\n        \\"description\\": \\"Generate a joke\\",\\n        \\"instruction\\": \\"Generate a joke based on the user prompt\\",\\n        \\"output_key\\": \\"joke\\",\\n        \\"temperature\\": 1.0\\n    },\\n    {\\n        \\"name\\": \\"joke_improver\\",\\n        \\"description\\": \\"Improve the joke\\",\\n        \\"instruction\\": \\"Make the joke funnier and more engaging\\",\\n        \\"output_key\\": \\"improved_joke\\",\\n        \\"temperature\\": 0.7\\n    },\\n    {\\n        \\"name\\": \\"joke_polisher\\",\\n        \\"description\\": \\"Polish the joke\\",\\n        \\"instruction\\": \\"Polish the joke, add a surprise twist at the end\\",\\n        \\"output_key\\": \\"polished_joke\\",\\n        \\"temperature\\": 0.5\\n    },\\n]\\n\\n# --- Guardrail Callback ---\\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\\n    \\"\\"\\"\\n    Guardrail function to block inappropriate prompts.\\n    \\"\\"\\"\\n    prompt = llm_request.contents[0].parts[0].text.lower()\\n    print(Panel.fit(f\\"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\\\n[bold cyan]Prompt:[/bold cyan] {prompt}\\"))\\n\\n    for word in BLOCKED_KEYWORDS:\\n        if word in prompt:\\n            raise ValueError(f\\"\u274c Prompt contains forbidden word: \'{word}\'. Please rephrase.\\")\\n\\n    return None\\n\\n# --- Agent Factory ---\\ndef create_llm_agent(config: Dict) -> LlmAgent:\\n    return LlmAgent(\\n        name=config[\\"name\\"],\\n        description=config[\\"description\\"],\\n        model=\\"gemini-2.0-flash\\",\\n        global_instruction=f\\"You are a {config[\'description\'].lower()}.\\",\\n        instruction=config[\\"instruction\\"],\\n        output_key=config[\\"output_key\\"],\\n        generate_content_config=types.GenerateContentConfig(temperature=config[\\"temperature\\"]),\\n        before_model_callback=on_before_model_callback\\n    )\\n\\n# --- Create Sequential Workflow ---\\njoke_agents = [create_llm_agent(cfg) for cfg in AGENT_CONFIGS]\\njoke_workflow = SequentialAgent(\\n    name=\\"joke_generator_workflow\\",\\n    description=\\"Generate, improve, and publish a joke\\",\\n    sub_agents=joke_agents\\n)\\n\\n# --- Set root agent for the web user interface ---\\nroot_agent = joke_workflow\\n\\n# --- Execution Handlers ---\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Run the agent workflow with a user prompt.\\n    \\"\\"\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt:[/bold white] {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green] {event.content.parts[0].text}\\", title=\\"\ud83e\udd16\\"))\\n\\ndef inspect_state():\\n    \\"\\"\\"\\n    Inspect and print the internal session state.\\n    \\"\\"\\"\\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\\n    print(Panel.fit(\\"[bold yellow]Session State[/bold yellow]\\"))\\n    for key, value in state.items():\\n        print(f\\"[cyan]{key}[/cyan]:\\\\n{value}\\\\n\\")\\n\\n# --- Main Execution ---\\nif __name__ == \'__main__\':\\n    APP_NAME = \\"joke_generator_app\\"\\n    USER_ID = \\"dev_user_01\\"\\n    SESSION_ID = \\"dev_user_session_01\\"\\n\\n    # --- Session & Runner Setup ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n\\n    session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    runner = Runner(\\n        agent=joke_workflow,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service\\n    )\\n\\n    try:\\n        call_agent(\\"Tell me a robot joke\\")\\n        inspect_state()\\n    except Exception as e:\\n        print(Panel.fit(f\\"[bold red]Error:[/bold red] {str(e)}\\", title=\\"\u274c\\"))\\n\\n```\\n\\n## Workflow: Routing\\n\\nThe routing pattern involves classifying the incoming task/prompt and directing them to the most appropriate follow-up LLM/Agent call. This approach promotes modularity and specialization, enabling more tailored prompts for each category. Without routing, attempts to optimize for one input type often degrade performance for others due to overly generalized prompts.\\n\\n**When to use this pattern:**\\n\\nRouting is particularly effective for tasks involving clearly distinguishable input categories that benefit from separate handling. It works best when accurate classification can be achieved\u2014whether through an LLM, rule-based logic, or a traditional machine learning classifier.\\n\\n:::tip[Example applications]\\n- Handling customer service inquiries by directing general questions, refund requests, and technical issues to different prompt flows or tools\\n- Forwarding simple, frequently asked questions to lightweight models like Claude 3.5 Haiku, while sending complex or edge cases to more capable models like Claude 3.5 Sonnet to balance speed and cost\\n:::\\n\\n**ADK Implementation:**\\n\\nIn this example, we implement a routing agent that delegates tasks to sub-agents based on the topic of the user prompt. The routing agent uses a simple keyword-based classification to determine which sub-agent to invoke.\\n\\n```python\\nfrom dotenv import load_dotenv\\nfrom rich import print\\nfrom rich.panel import Panel\\nfrom typing import Optional, List, Dict\\n\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent\\nfrom google.adk.agents.callback_context import CallbackContext\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.models import LlmRequest, LlmResponse\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\n\\n# --- Load environment ---\\nload_dotenv()\\n\\n# --- Constants ---\\nBLOCKED_KEYWORDS = [\\"apple\\"]\\n\\n# --- Router Config: Define Routing Sub-Agents ---\\nROUTER_CONFIG: List[Dict] = [\\n    {\\n        \\"name\\": \\"joke_generator\\",\\n        \\"description\\": \\"Generate a joke\\",\\n        \\"instruction\\": \\"Generate a joke based on the user prompt\\",\\n        \\"output_key\\": \\"joke\\",\\n        \\"temperature\\": 1.0\\n    },\\n    {\\n        \\"name\\": \\"song_generator\\",\\n        \\"description\\": \\"Generate a song\\",\\n        \\"instruction\\": \\"Generate a song based on the user prompt\\",\\n        \\"output_key\\": \\"song\\",\\n        \\"temperature\\": 1.0\\n    },\\n    {\\n        \\"name\\": \\"poem_generator\\",\\n        \\"description\\": \\"Generate a poem\\",\\n        \\"instruction\\": \\"Generate a poem based on the user prompt\\",\\n        \\"output_key\\": \\"poem\\",\\n        \\"temperature\\": 1.0\\n    }\\n]\\n\\n# --- Guardrail Callback ---\\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\\n    prompt = llm_request.contents[0].parts[0].text.lower()\\n    print(Panel.fit(f\\"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\\\n[bold cyan]Prompt:[/bold cyan] {prompt}\\"))\\n\\n    for word in BLOCKED_KEYWORDS:\\n        if word in prompt:\\n            raise ValueError(f\\"\u274c Prompt contains forbidden word: \'{word}\'. Please rephrase.\\")\\n\\n    return None\\n\\n\\n# --- Helper: Agent Factory from Router Config ---\\ndef create_llm_agent(config: Dict) -> LlmAgent:\\n    return LlmAgent(\\n        name=config[\\"name\\"],\\n        description=config[\\"description\\"],\\n        model=\\"gemini-2.0-flash\\",\\n        global_instruction=f\\"You are a {config[\'description\'].lower()}.\\",\\n        instruction=config[\\"instruction\\"],\\n        output_key=config[\\"output_key\\"],\\n        generate_content_config=types.GenerateContentConfig(temperature=config.get(\\"temperature\\", 1.0)),\\n        before_model_callback=on_before_model_callback\\n    )\\n\\n# Create sub-agents from config\\nsub_agents = [create_llm_agent(cfg) for cfg in ROUTER_CONFIG]\\n\\n# --- Router Agent ---\\nrouter_instruction = (\\n    \\"You are a router agent.\\\\n\\"\\n    \\"Given the user prompt, decide whether it\'s a request for a joke, song, or poem, and delegate accordingly.\\\\n\\"\\n    \\"Use only the appropriate sub-agent based on the topic.\\\\n\\"\\n)\\n\\nrouter_agent = LlmAgent(\\n    name=\\"root_router\\",\\n    model=\\"gemini-2.0-flash\\",\\n    description=\\"Router agent that delegates to joke, song, or poem generators.\\",\\n    instruction=router_instruction,\\n    sub_agents=sub_agents,\\n    output_key=\\"final_response\\",\\n    before_model_callback=on_before_model_callback\\n)\\n\\n# --- Set root agent for the web user interface ---\\nroot_agent = router_agent\\n\\n# --- Execution Helpers ---\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Call the router agent with a user prompt and print the response.\\n    \\"\\"\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt:[/bold white] {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            response = event.content.parts[0].text\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green] {response}\\", title=\\"\ud83e\udd16\\"))\\n\\n\\ndef inspect_state():\\n    \\"\\"\\"\\n    Print the internal session state.\\n    \\"\\"\\"\\n    state = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\\n    print(Panel.fit(\\"[bold yellow]Session State[/bold yellow]\\"))\\n    for key, value in state.items():\\n        print(f\\"[cyan]{key}[/cyan]: {value}\\")\\n\\n\\n# --- Entry Point ---\\nif __name__ == \'__main__\':\\n    APP_NAME = \\"joke_generator_app\\"\\n    USER_ID = \\"dev_user_01\\"\\n    SESSION_ID = \\"dev_user_session_01\\"\\n\\n    # --- Session & Runner Setup ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n\\n    session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    runner = Runner(\\n        agent=router_agent,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service\\n    )\\n\\n    try:\\n        topic = \\"robots\\"\\n        call_agent(f\\"write a poem about {topic}\\")\\n        inspect_state()\\n    except Exception as e:\\n        print(Panel.fit(f\\"[bold red]Error:[/bold red] {str(e)}\\", title=\\"\u274c\\"))\\n\\n```\\n\\n## Workflow: Parallelization\\n\\nThe parallelization pattern allows multiple LLMs to work concurrently on a task, with their outputs combined through programmatic aggregation. This approach typically takes two main forms:\\n\\n**Sectioning:** Dividing a task into distinct, independent components that can be handled in parallel\\n**Voting:** Running the same task multiple times to gather diverse perspectives and aggregate results for higher confidence\\n\\n**When to use this pattern:**\\n\\nParallelization is ideal when tasks can be broken into parts that are either independent or benefit from multiple viewpoints. It\u2019s especially effective for increasing speed by distributing workload or for improving accuracy in complex evaluations by isolating specific dimensions of a problem across separate LLM calls.\\n\\n:::tip[Example applications]\\n**Sectioning:**\\n\\n* Generating a multi-part report where each section (e.g., summary, key findings, recommendations) is written by a different LLM call\\n* Creating localized versions of a product description, with one model instance handling cultural adaptation per target market in parallel\\n* Conducting a multi-faceted sentiment analysis, where separate LLM calls assess tone, emotional intensity, and subjectivity individually\\n\\n**Voting:**\\n\\n* Rewriting a paragraph for clarity and running several prompts with slight variations, then selecting the best version via ranking or consensus\\n* Classifying user feedback as actionable or non-actionable using multiple interpretations, then aggregating to reduce classification errors\\n* Diagnosing possible root causes from system logs by prompting the model with different framing strategies and combining insights\\n:::\\n\\n\\n**ADK Implementation:**\\n\\nIn this example, we implement a parallel agent that generates a joke, song, and poem concurrently based on the user prompt. The outputs are then merged into a structured response.\\n\\n```python\\nfrom dotenv import load_dotenv\\nfrom rich import print\\nfrom rich.panel import Panel\\nfrom typing import List, Dict, Optional\\n\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent, ParallelAgent, SequentialAgent\\nfrom google.adk.agents.callback_context import CallbackContext\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.models import LlmRequest, LlmResponse\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\n\\n# --- Load Environment ---\\nload_dotenv()\\n\\n# --- Constants ---\\nBLOCKED_KEYWORDS = [\\"bruno\\"]\\n\\n# --- Task Definitions ---\\nTASK_CONFIGS: List[Dict[str, str]] = [\\n    {\\n        \\"name\\": \\"joke_generator\\",\\n        \\"description\\": \\"Generate a joke\\",\\n        \\"instruction\\": \\"Generate a joke based on the user prompt\\",\\n        \\"output_key\\": \\"joke\\"\\n    },\\n    {\\n        \\"name\\": \\"song_generator\\",\\n        \\"description\\": \\"Generate a song\\",\\n        \\"instruction\\": \\"Generate a song based on the user prompt\\",\\n        \\"output_key\\": \\"song\\"\\n    },\\n    {\\n        \\"name\\": \\"poem_generator\\",\\n        \\"description\\": \\"Generate a poem\\",\\n        \\"instruction\\": \\"Generate a poem based on the user prompt\\",\\n        \\"output_key\\": \\"poem\\"\\n    },\\n]\\n\\n# --- Callback Guardrail ---\\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\\n    \\"\\"\\"\\n    Guardrail to block LLM execution for specific banned phrases.\\n    \\"\\"\\"\\n    prompt = llm_request.contents[0].parts[0].text.lower()\\n    print(Panel.fit(f\\"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\\\n[bold cyan]Prompt:[/bold cyan] {prompt}\\"))\\n\\n    for banned in BLOCKED_KEYWORDS:\\n        if banned in prompt:\\n            return LlmResponse(\\n                content=types.Content(\\n                    role=\\"model\\",\\n                    parts=[types.Part(text=f\\"LLM call blocked. We don\'t talk about {banned.capitalize()}!!\\")]\\n                )\\n            )\\n    return None\\n\\n\\n# --- Helper: Create Agent from Task Config ---\\ndef create_task_handler_agent(task: Dict[str, str]) -> LlmAgent:\\n    return LlmAgent(\\n        name=task[\\"name\\"],\\n        description=task[\\"description\\"],\\n        model=\\"gemini-2.0-flash\\",\\n        global_instruction=f\\"You are a {task[\'description\'].lower()} generator.\\",\\n        instruction=task[\\"instruction\\"],\\n        output_key=task[\\"output_key\\"],\\n        generate_content_config=types.GenerateContentConfig(temperature=1.0),\\n        before_model_callback=on_before_model_callback\\n    )\\n\\n# --- Create Sub-agents ---\\nsub_agents = [create_task_handler_agent(task) for task in TASK_CONFIGS]\\n\\n# --- Aggregator (Parallel Execution) ---\\naggregator_agent = ParallelAgent(\\n    name=\\"ParallelGenerator\\",\\n    sub_agents=sub_agents,\\n    description=\\"Run joke, song, and poem generators in parallel based on the user prompt.\\"\\n)\\n\\n# --- Merger Agent ---\\nmerger_agent = LlmAgent(\\n    name=\\"merger_agent\\",\\n    description=\\"Merge the outputs of the sub-agents into a structured response.\\",\\n    model=\\"gemini-2.0-flash\\",\\n    global_instruction=\\"You are a merger agent.\\",\\n    instruction=(\\n        \\"Your task is to merge the outputs of multiple sub-agents into a single, coherent, and structured response.\\\\n\\\\n\\"\\n        \\"- Do **not** add any external information, context, or commentary.\\\\n\\"\\n        \\"- Use **only** the provided inputs: {joke}, {song}, and {poem}.\\\\n\\"\\n        \\"- Maintain the exact order and structure specified below.\\\\n\\\\n\\"\\n        \\"### Joke:\\\\n{joke}\\\\n\\\\n\\"\\n        \\"### Song:\\\\n{song}\\\\n\\\\n\\"\\n        \\"### Poem:\\\\n{poem}\\\\n\\\\n\\"\\n        \\"Instructions:\\\\n\\"\\n        \\"- Do **not** include any introductory or concluding phrases.\\\\n\\"\\n        \\"- Do **not** modify, interpret, or enhance the content of the inputs.\\\\n\\"\\n        \\"- Strictly follow the format above and output only the merged content as shown.\\"\\n    ),\\n    output_key=\\"merged_response\\",\\n    generate_content_config=types.GenerateContentConfig(temperature=0.5),\\n)\\n\\n# --- Root Agent (Sequential Flow) ---\\nroot_agent = SequentialAgent(\\n    name=\\"root_agent\\",\\n    sub_agents=[aggregator_agent, merger_agent],\\n    description=\\"Coordinates generation and merging of joke, song, and poem.\\"\\n)\\n\\n\\n# --- Interaction Functions ---\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Send a prompt to the root agent and print structured results.\\n    \\"\\"\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt:[/bold white] {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green]\\\\n{event.content.parts[0].text}\\", title=\\"\ud83e\udd16\\"))\\n\\ndef inspect_state():\\n    \\"\\"\\"\\n    Print the internal state of the session.\\n    \\"\\"\\"\\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\\n    print(Panel.fit(\\"[bold yellow]Session State[/bold yellow]\\"))\\n    for key, value in state.items():\\n        print(f\\"[cyan]{key}[/cyan]: {value}\\")\\n\\n# --- Main ---\\nif __name__ == \'__main__\':\\n    APP_NAME = \\"joke_generator_app\\"\\n    USER_ID = \\"dev_user_01\\"\\n    SESSION_ID = \\"dev_user_session_01\\"\\n\\n    # --- Session & Runner Setup ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n\\n    session_service.create_session(\\n        app_name=APP_NAME,\\n        user_id=USER_ID,\\n        session_id=SESSION_ID\\n    )\\n\\n    runner = Runner(\\n        agent=root_agent,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service\\n    )\\n\\n    call_agent(\\"Please generate something funny and poetic.\\")\\n    inspect_state()\\n```\\n\\n## Workflow: Orchestrator-workers\\n\\nThe **orchestrator\u2013worker pattern** involves a central LLM Agent (the orchestrator) that interprets the input, dynamically decomposes the task into subtasks, assigns these to specialized worker LLMs, and then integrates their outputs into a final result.\\n\\n**When to use this pattern:**\\n\\nThis workflow is ideal for complex, open-ended tasks where the structure and number of subtasks can\u2019t be known in advance. Unlike parallelization, where subtasks are predefined and run concurrently, the orchestrator\u2013worker setup offers greater adaptability\u2014the orchestrator decides what needs to be done based on the specific input context.\\n\\n:::tip[Example applications]\\n- Writing a grant proposal where the orchestrator delegates background research, impact justification, and formatting to different LLM calls based on the application requirements\\n- Preparing a competitive market analysis, where the orchestrator identifies key competitors, then assigns each to a worker to extract data, analyze strategies, and compare strengths\\n- Automating legal document review, where the orchestrator dynamically assigns sections (e.g., clauses, terms, jurisdictional elements) to worker agents depending on the document type and structure\\n:::\\n\\n**ADK Implementation:**\\n\\nIn this example, we implement an orchestrator agent that coordinates the generation of a joke, song, and poem based on the user prompt/topic. The orchestrator dynamically assigns tasks to worker agents and merges their outputs into a final response.\\n\\n```python\\nfrom typing import AsyncGenerator, List, Dict, Optional\\nfrom dotenv import load_dotenv\\nfrom rich import print\\nfrom rich.panel import Panel\\n\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent, LoopAgent, BaseAgent\\nfrom google.adk.agents.callback_context import CallbackContext\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.models import LlmRequest, LlmResponse\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.events import Event, EventActions\\nfrom google.genai import types\\n\\n# Load .env file\\nload_dotenv()\\n\\n\\n# --- Task Definitions ---\\nTASK_CONFIGS: List[Dict[str, str]] = [\\n    {\\n        \\"name\\": \\"joke_generator\\",\\n        \\"instruction\\": \\"Generate a joke based on the user prompt\\",\\n        \\"output_key\\": \\"joke\\"\\n    },\\n    {\\n        \\"name\\": \\"song_generator\\",\\n        \\"instruction\\": \\"Generate a song based on the user prompt\\",\\n        \\"output_key\\": \\"song\\"\\n    },\\n    {\\n        \\"name\\": \\"poem_generator\\",\\n        \\"instruction\\": \\"Generate a poem based on the user prompt\\",\\n        \\"output_key\\": \\"poem\\"\\n    },\\n]\\n\\n# --- Guardrail Callback ---\\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\\n    \\"\\"\\"\\n    Intercepts the prompt before sending it to the model. Useful for filtering or logging.\\n    Blocks LLM call if restricted keyword is present.\\n    \\"\\"\\"\\n    prompt = llm_request.contents[0].parts[0].text\\n    if \\"bruno\\" in prompt.lower():\\n        return LlmResponse(\\n            content=types.Content(\\n                role=\\"model\\",\\n                parts=[types.Part(text=\\"LLM call was blocked. We don\'t talk about Bruno!!\\")],\\n            )\\n        )\\n    return None\\n\\n# --- Agent Factory ---\\ndef create_task_handler_agent(task: Dict[str, str]) -> LlmAgent:\\n    \\"\\"\\"\\n    Creates an LLM Agent from a task configuration dictionary.\\n    Each task must include: name, instruction, and output_key.\\n    \\"\\"\\"\\n    return LlmAgent(\\n        name=task[\\"name\\"],\\n        description=f\\"Generate a {task[\'output_key\']}\\",\\n        model=task.get(\\"model\\", \\"gemini-2.0-flash\\"),\\n        global_instruction=task.get(\\"global_instruction\\", f\\"You are a {task[\'output_key\']} generator.\\"),\\n        instruction=task[\\"instruction\\"],\\n        output_key=task[\\"output_key\\"],\\n        generate_content_config=types.GenerateContentConfig(\\n            temperature=task.get(\\"temperature\\", 1.0)\\n        ),\\n        before_model_callback=on_before_model_callback,\\n    )\\n\\n# --- Create Agents from Configs ---\\ntask_handler_agents = [create_task_handler_agent(task) for task in TASK_CONFIGS]\\n\\n# --- Generic CheckCondition Agent ---\\nclass CheckCondition(BaseAgent):\\n    output_keys : List[str] = []\\n\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        is_done = all(ctx.session.state.get(key) is not None for key in self.output_keys)\\n        yield Event(author=self.name, actions=EventActions(escalate=is_done))\\n\\n# --- Setup Orchestrator Agent ---\\noutput_keys = [task[\\"output_key\\"] for task in TASK_CONFIGS]\\n\\norchestrator_agent = LoopAgent(\\n    name=\\"coordinator_agent\\",\\n    max_iterations=10,\\n    sub_agents=task_handler_agents + [CheckCondition(name=\\"Checker\\", output_keys=output_keys)]\\n)\\n\\n# --- Set root agent for the web user interface ---\\nroot_agent = orchestrator_agent\\n\\n\\n# --- Agent Interaction Functions ---\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Trigger the orchestrator with a prompt.\\n    \\"\\"\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt:[/bold white] {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            response = event.content.parts[0].text\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green] {response}\\", title=\\"\ud83e\udd16\\"))\\n\\ndef inspect_state():\\n    \\"\\"\\"\\n    Print session state from memory.\\n    \\"\\"\\"\\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\\n    print(Panel.fit(\\"[bold yellow]Session State[/bold yellow]\\"))\\n    for key, value in state.items():\\n        print(f\\"[cyan]{key}[/cyan]: {value}\\")\\n\\n# --- Main Entry Point ---\\nif __name__ == \'__main__\':\\n    # --- Constants ---\\n    APP_NAME = \\"joke_generator_app\\"\\n    USER_ID = \\"dev_user_01\\"\\n    SESSION_ID = \\"dev_user_session_01\\"\\n\\n    # --- Session & Runner Setup ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n    session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    runner = Runner(\\n        agent=orchestrator_agent,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service,\\n    )\\n\\n    call_agent(\\"Robots\\")\\n    # inspect_state()\\n```\\n\\n## Workflow: Evaluator-optimizer\\n\\nThe **evaluator\u2013optimizer pattern** involves two roles: one LLM generates an initial output, while another evaluates and provides structured feedback. This feedback is then used to refine the response in an iterative loop, gradually improving quality with each cycle.\\n\\n**When to use this pattern:**\\n\\nThis workflow is particularly useful when there are well-defined criteria for what makes a response \u201cgood,\u201d and when multiple iterations are likely to produce significant improvements. It\u2019s especially effective in scenarios where human feedback would clearly help\u2014but instead, the LLM can act as the evaluator itself. The process mirrors how a writer might draft, receive feedback, and revise a document to improve clarity, tone, or content.\\n\\n:::tip[Example applications]\\n- Crafting persuasive essays or position statements, where an evaluator LLM critiques argument strength, coherence, and tone before another round of revision\\n- Designing user-facing chatbot replies for customer support, where the evaluator checks for tone, helpfulness, and policy compliance\\n- Developing instructional content, such as tutorials or lesson plans, where the evaluator verifies clarity, pedagogical soundness, and alignment with learning objectives\\n:::\\n\\n**ADK Implementation:**\\n\\nIn this example, we implement a code refinement workflow where one agent generates code based on user requirements, while another evaluates the code against those requirements. The process iterates until the code meets the quality standards.\\n\\n\\n```python\\nfrom typing import AsyncGenerator\\nfrom rich import print\\nfrom rich.panel import Panel\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent, LoopAgent, BaseAgent\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\nfrom dotenv import load_dotenv\\n\\n# Load .env file\\nload_dotenv()\\n\\n# --- Agent Definitions ---\\ndef create_code_refiner() -> LlmAgent:\\n    return LlmAgent(\\n        name=\\"CodeRefiner\\",\\n        model=\\"gemini-2.0-flash\\",\\n        instruction=(\\n            \\"Read `state[\'current_code\']` (if present) and `state[\'requirements\']`. \\"\\n            \\"Generate or refine Python code to meet the requirements, \\"\\n            \\"then store the result in `state[\'current_code\']`.\\"\\n        ),\\n        output_key=\\"current_code\\"\\n    )\\n\\ndef create_quality_checker() -> LlmAgent:\\n    return LlmAgent(\\n        name=\\"QualityChecker\\",\\n        model=\\"gemini-2.0-flash\\",\\n        instruction=(\\n            \\"Evaluate the code in `state[\'current_code\']` against `state[\'requirements\']`. \\"\\n            \\"If the code meets the requirements, output \'pass\'; otherwise, output \'fail\'.\\"\\n        ),\\n        output_key=\\"quality_status\\"\\n    )\\n\\nclass CheckStatusAndEscalate(BaseAgent):\\n    \\"\\"\\"\\n    Terminates the loop when the quality check passes.\\n    \\"\\"\\"\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\\"quality_status\\", \\"fail\\")\\n        should_stop = status.strip().lower() == \\"pass\\"\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\n# --- Loop Agent ---\\nrefinement_loop = LoopAgent(\\n    name=\\"CodeRefinementLoop\\",\\n    max_iterations=5,\\n    sub_agents=[\\n        create_code_refiner(),\\n        create_quality_checker(),\\n        CheckStatusAndEscalate(name=\\"StopChecker\\")\\n    ]\\n)\\n\\n# --- Set root agent for the web user interface ---\\nroot_agent = refinement_loop\\n\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Send user input to the orchestrator agent and stream responses.\\n    \\"\\"\\"\\n    print(Panel.fit(f\\"[bold white]User Prompt[/bold white]: {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            response = event.content.parts[0].text\\n            print(Panel.fit(f\\"[bold green]{event.author}:[/bold green] {response}\\", title=\\"\ud83e\udd16\\"))\\n\\ndef inspect_state():\\n    \\"\\"\\"\\n    Print the internal session state.\\n    \\"\\"\\"\\n    state = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\\n    print(Panel.fit(\\"[bold yellow]Session State[/bold yellow]\\"))\\n    for key, value in state.items():\\n        print(f\\"[cyan]{key}[/cyan]:\\\\n{value}\\\\n\\")\\n\\n# --- Entry Point ---\\nif __name__ == \'__main__\':\\n    # --- Constants ---\\n    APP_NAME = \\"code_refinement_app\\"\\n    USER_ID = \\"user_123\\"\\n    SESSION_ID = \\"session_456\\"\\n\\n    # --- Services ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n\\n    session = session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    # --- Runner Setup ---\\n    runner = Runner(\\n        agent=refinement_loop,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service,\\n    )\\n    call_agent(\\"Write a Python function that calculates the factorial of a number.\\"\\n               \\"Make sure to add type hints to the function parameters and return type.\\")\\n    # inspect_state()\\n\\n```\\n\\n## Agents\\n\\nAs LLMs gain capabilities in reasoning, planning, tool use, and error recovery, agents are becoming practical for production use. Agents typically start from a user command or conversation, then plan and act autonomously\u2014looping through tool use, feedback, and self-correction. They may pause for human input when encountering uncertainty or reaching checkpoints, and they rely on real-time feedback (e.g., tool outputs) to stay grounded.\\nDespite their ability to tackle complex, open-ended problems, agents are often simple in implementation\u2014frequently just an LLM operating within a loop, using tools and adjusting based on observed outcomes. For this reason, careful tool design and clear documentation are critical.\\n\\n**When to use this pattern**: \\n\\nUse agents for open-ended tasks with unpredictable steps, where hardcoding logic is impractical. They\u2019re ideal in trusted environments requiring autonomy over multiple iterations.\\n\\n:::warning\\n\\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appr\\n\\n:::\\n\\n:::tip[Examples of Agent Use]\\n\\n- A **research assistant agent** that autonomously gathers, filters, and summarizes information from multiple sources\\n- A **data cleaning agent** that iteratively inspects a dataset, flags issues, and proposes structured fixes based on schema rules\\n:::\\n\\n\\n**ADK Implementation:**\\n\\nIn this example, we implement a code execution agent that generates and executes Python code based on user input. The agent uses the built-in code execution tool to run the generated code and return the results.\\n\\n```python\\nimport io\\n\\nfrom PIL import Image\\nfrom dotenv import load_dotenv\\nfrom google.adk import Runner\\nfrom google.adk.agents import LlmAgent\\nfrom google.adk.artifacts import InMemoryArtifactService\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.tools import built_in_code_execution\\nfrom google.genai import types\\nfrom rich import print\\nfrom rich.panel import Panel\\n\\n# Load .env file\\nload_dotenv()\\n\\n\\nroot_agent = LlmAgent(\\n    name=\\"CodeAgent\\",\\n    model=\\"gemini-2.0-flash\\",\\n    tools=[built_in_code_execution],\\n    instruction=\\"\\"\\"You are a calculator agent.\\n    When given a mathematical expression, function, or task, write and EXECUTE the Python code to obtain the result.\\n    \\"\\"\\",\\n    description=\\"Executes Python code to perform calculations.\\",\\n)\\n\\ndef call_agent(prompt: str):\\n    \\"\\"\\"\\n    Send user input to the orchestrator agent and stream responses.\\n    \\"\\"\\"\\n    # --- Services ---\\n    session_service = InMemorySessionService()\\n    artifact_service = InMemoryArtifactService()\\n    session = session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n    # --- Runner Setup ---\\n    runner = Runner(\\n        agent=root_agent,\\n        app_name=APP_NAME,\\n        session_service=session_service,\\n        artifact_service=artifact_service,\\n    )\\n    print(Panel.fit(f\\"[bold white]User Prompt[/bold white]: {prompt}\\", title=\\"\ud83d\udc64\\"))\\n    content = types.Content(role=\\"user\\", parts=[types.Part(text=prompt)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        # --- Check for specific parts FIRST ---\\n        has_specific_part = False\\n        if event.content and event.content.parts:\\n            for part in event.content.parts:  # Iterate through all parts\\n                if part.executable_code:\\n                    # Access the actual code string via .code\\n                    print(f\\"  Debug: Agent generated code:\\\\n```python\\\\n{part.executable_code.code}\\\\n```\\")\\n                    has_specific_part = True\\n                elif part.code_execution_result:\\n                    # Access the code execution result for debugging\\n                    print(f\\"  Debug: Code Execution Result: {part.code_execution_result.outcome} - Output:\\\\n{part.code_execution_result.output}\\")\\n                    has_specific_part = True\\n                elif part.inline_data:\\n                    # Access the inline data (e.g., image) and display it\\n                    if part.inline_data.mime_type == \\"image/png\\":\\n                        # Access the image data and display it\\n                        image_data = part.inline_data.data\\n                        image = Image.open(io.BytesIO(image_data))\\n                        image.show()\\n                # Also print any text parts found in any event for debugging\\n                elif part.text and not part.text.isspace():\\n                    print(f\\"  Text: \'{part.text.strip()}\'\\")\\n                    # Do not set has_specific_part=True here, as we want the final response logic below\\n\\n        # --- Check for final response AFTER specific parts ---\\n        # Only consider it final if it doesn\'t have the specific code parts we just handled\\n        if not has_specific_part and event.is_final_response():\\n            if event.content and event.content.parts and event.content.parts[0].text:\\n                final_response_text = event.content.parts[0].text.strip()\\n                print(f\\"==> Final Agent Response: {final_response_text}\\")\\n            else:\\n                print(\\"==> Final Agent Response: [No text content in final event]\\")\\n\\n\\n# --- Entry Point ---\\nif __name__ == \'__main__\':\\n    # --- Constants ---\\n    APP_NAME = \\"code_refinement_app\\"\\n    USER_ID = \\"user_123\\"\\n    SESSION_ID = \\"session_456\\"\\n\\n\\n    call_agent(\\"Generates an array of 1000 random numbers from a normal distribution with mean 0 and standard deviation 1, \\"\\n               \\"create a histogram of the data, and \\"\\n               \\"save the histogram as a PNG file plot.png\\")\\n\\n```\\n\\n## Key Differences and Summary\\n\\nTo help you choose the right pattern for your use case, the table below summarizes the core ideas, ideal use cases, and ADK components for each of Anthropic\u2019s agent design patterns. This comparison highlights the trade-offs between structure, flexibility, and autonomy when building LLM-powered systems.\\n\\n| **Pattern**             | **Core Idea**                                                                 | **When to Use**                                                                                     | **ADK Agent Types**              |\\n|-------------------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|----------------------------------|\\n| \ud83d\udd01 Prompt Chaining       | Break a task into sequential steps where each step builds on the last        | Tasks with clear, fixed subtasks that benefit from intermediate validation                          | `SequentialAgent`               |\\n| \ud83d\udd00 Routing               | Classify input and delegate to specialized agents                             | Inputs that fall into distinct categories requiring tailored responses                              | `LlmAgent` with `sub_agents`    |\\n| \u26a1 Parallelization        | Run tasks concurrently and aggregate results via sectioning or voting         | Tasks that benefit from speed or multiple perspectives                                               | `ParallelAgent` + merger agent  |\\n| \ud83e\udde0 Orchestrator\u2013Worker   | Dynamically break down tasks and assign to sub-agents                          | Tasks where subtasks are not known in advance and depend on the input                               | `LoopAgent` or custom loop + `LlmAgent` workers |\\n| \ud83e\uddea Evaluator\u2013Optimizer   | Use one agent to generate, and another to evaluate and refine                 | When iterative improvement is valuable and evaluation criteria are well-defined                     | `LoopAgent` + `LlmAgent` evaluator |\\n| \ud83e\udd16 Autonomous Agents     | LLMs operate independently, using tools and state in a reasoning loop         | Open-ended tasks with unclear paths or evolving context over multiple turns                         | `LoopAgent` with memory + tools |\\n\\n\\n## Conclusion\\n\\nThis post explored Anthropic\u2019s agent design patterns and demonstrated how to implement them using Google\u2019s Agent Development Kit (ADK). Each pattern was accompanied by hands-on examples\u2014from prompt chaining to complex orchestrator\u2013worker setups\u2014showcasing how to build LLM agents capable of handling a wide range of tasks.\\nWhile Anthropic\u2019s original post emphasizes that many of these patterns can be implemented in just a few lines of code using direct LLM APIs\u2014allowing developers to avoid the overhead of complex frameworks:\\n\\n:::quote[Anthropic]\\n\\nWe\'ve worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks....We suggest that developers start by using LLM APIs directly as many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what\'s under the hood are a common source of customer error.\\n::::\\n\\nIt\u2019s also important to recognize the practical advantages that frameworks can offer\u2014particularly as agentic systems grow in complexity and scale. Tools like Google\u2019s ADK, LangGraph, and others provide essential capabilities out of the box, including state persistence across sessions, streaming support for long-running interactions, built-in observability and debugging tools, along with deployment-ready infrastructure. These features can greatly accelerate development, improve reliability, and simplify the orchestration of complex, multi-agent workflows in real-world applications.\\n\\nIn short, while starting simple is often the right approach, thoughtfully leveraging a framework can help you move faster and build more robust, scalable agentic systems.\\n\\nIn future posts, we\u2019ll explore more advanced topics in agent design and implementation, including how to build agents that can learn from user interactions, adapt to changing environments, and collaborate with other agents. We\u2019ll also explore more of ADK\u2019s capabilities through real-world use cases and end-to-end implementations.\\n\\nThanks for reading! We hope this guide offered useful insights and practical tools to support your journey in building smarter, more capable LLM agents.\\nAll code examples are available in the [Agents Experiments repository.](https://github.com/haruiz/agents-experiments).\\nFor more on ADK, check out the [official documentation](https://google.github.io/adk-docs/).\\n\\n## References\\n<ul>\\n  <li>\\n    <a id=\\"anthropic:1\\" href=\\"https://www.anthropic.com/engineering/building-effective-agents\\" target=\\"_blank\\">\\n      [1] Anthropic (2024). Building Effective Agents.\\n    </a>\\n  </li>\\n  <li>\\n    <a id=\\"adk:1\\" href=\\"https://google.github.io/adk-docs/\\" target=\\"_blank\\">\\n      [2] Agent Development Kit (ADK)\\n    </a>\\n  </li>\\n  <li>\\n    <a id=\\"langgraph:1\\" href=\\"https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent\\" target=\\"_blank\\">\\n      [3] LangGraph Tutorials \u2013 Workflows and Agents\\n    </a>\\n  </li>\\n</ul>"},{"id":"getting-started-with-ray-on-google-cloud-platform","metadata":{"permalink":"/blog/getting-started-with-ray-on-google-cloud-platform","source":"@site/blog/2025-04-01-getting-started-with-ray-on-google-cloud-platform/index.md","title":"Getting Started with Ray on Google Cloud Platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"ray","permalink":"/blog/tags/ray"},{"inline":true,"label":"google-cloud","permalink":"/blog/tags/google-cloud"},{"inline":true,"label":"vertex-ai","permalink":"/blog/tags/vertex-ai"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"scalable-ai","permalink":"/blog/tags/scalable-ai"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"cloud-computing","permalink":"/blog/tags/cloud-computing"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"},{"inline":true,"label":"mlops","permalink":"/blog/tags/mlops"},{"inline":true,"label":"ray-datasets","permalink":"/blog/tags/ray-datasets"},{"inline":true,"label":"parallel-computing","permalink":"/blog/tags/parallel-computing"}],"readingTime":20.55,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Getting Started with Ray on Google Cloud Platform","slug":"getting-started-with-ray-on-google-cloud-platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","authors":["haruiz"],"tags":["ray","google-cloud","vertex-ai","distributed-computing","machine-learning","scalable-ai","python","cloud-computing","kubernetes","data-science","mlops","ray-datasets","parallel-computing"]},"unlisted":false,"prevItem":{"title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","permalink":"/blog/implementing-anthropic\'s-agent-design-patterns-with-google-adk"},"nextItem":{"title":"Building Trustworthy RAG Systems with In Text Citations","permalink":"/blog/improve-rag-systems-reliability-with-citations"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\n### Introduction \\n\\nAs AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax.\\n\\nWhen combined with the power and scalability of Google Cloud Platform (GCP), particularly Vertex AI\u2014Ray enables seamless orchestration of large-scale data science and machine learning workflows, from local prototyping to full production deployments.\\n\\nThis post introduces the fundamentals of Ray and walks you through how to deploy and manage Ray clusters on GCP using Vertex AI, empowering you to run scalable and efficient distributed workloads with minimal operational overhead.\\n\\n:::tip \\n## In This Post You Will Learn\\n- What Ray is and how it supports scalable Python and AI applications\\n- Core components of a Ray cluster: head nodes, worker nodes, autoscaler, and scheduler\\n- How to configure and manage Ray clusters on Google Cloud Platform with Vertex AI\\n- How Ray handles distributed task execution and resource allocation across nodes and GPUs\\n- How to submit jobs and interact with Ray clusters using the Ray Job API and Python SDK\\n- Best practices for using Ray Datasets to efficiently ingest, transform, and serve distributed data\\n- Practical examples for configuring autoscaling, deploying workloads, and optimizing cluster usage\\n:::\\n\\n### What is Ray?\\n\\n**Ray** is an open-source, unified framework engineered to enable scalable and distributed computing for AI and Python-based applications. It provides an efficient and extensible compute layer for executing parallel and distributed workloads, abstracting the complexities typically associated with distributed systems such as resource management, orchestration, and fault tolerance.\\n\\nBy decoupling infrastructure concerns from application logic, Ray allows developers and researchers to seamlessly scale individual components or end-to-end machine learning (ML) pipelines\u2014from prototyping on a local machine to executing large-scale workloads across multi-node, multi-GPU clusters.\\nRay\u2019s modular and composable architecture supports diverse roles across the machine learning and data science lifecycle, delivering specific advantages for each:\\n\\n- **For Data Scientists:** Ray enables transparent parallelization of data preprocessing, feature extraction, model training, and evaluation workflows across heterogeneous compute resources, without requiring in-depth knowledge of distributed systems. Its native integration with popular Python libraries (e.g., NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch) ensures smooth adoption into existing ecosystems.\\n\\n- **For Machine Learning Engineers:** Ray facilitates the development of production-grade ML platforms through unified APIs that allow the same codebase to scale seamlessly from a developer\'s laptop to large-scale clusters. This consistency accelerates the deployment lifecycle, reduces operational overhead, and ensures reproducibility in model training and experimentation workflows.\\n\\n- **For Systems Engineers and DevOps Teams:** Ray handles lower-level system functions including task scheduling, resource allocation, fault tolerance, and elastic scaling. It also supports Kubernetes-native deployments, making it easier to integrate with modern cloud-native infrastructure and CI/CD pipelines.\\n\\n### Ray\'s Core Components\\nRay enables seamless scaling of high-performance analytical workloads\u2014from a single laptop to a large, distributed cluster. You can get started locally with a simple `ray.init()` call during development. However, to scale applications across multiple nodes in production, you\'ll need to *deploy a Ray cluster*.\\n\\nA **Ray cluster** is a collection of nodes working together to execute tasks and manage data in a distributed environment. Each cluster consists of a **head node** and one or more **worker nodes**, each with distinct responsibilities that enable efficient and scalable execution.\\n\\n#### Head Node\\n\\nThe **head node** acts as the central coordinator of the cluster. It runs the core components responsible for orchestration and cluster management, including:\\n\\n- **Global Control Store (GCS):** A distributed metadata store that tracks task specifications, function definitions, object locations, and scheduling events. GCS ensures scalability and fault tolerance across the cluster.\\n- **Autoscaler:** A daemon process running on the head node\u2014or as a sidecar container within the head pod in Kubernetes environments\u2014that continuously monitors resource utilization and dynamically adjusts the number of worker nodes in response to real-time workload demands, enabling elastic cluster scaling both upward and downward as needed.\\n- **Driver Processes:** Entry points for Ray applications. Each driver launches and manages a job, which may include thousands of distributed tasks and actors.\\n\\nWhile the head node is capable of executing tasks and actors, in larger clusters it\'s typically configured to focus solely on coordination duties to optimize performance and avoid resource contention.\\n\\n#### Worker Nodes\\n\\n**Worker nodes** are dedicated to executing user-defined tasks and actors. They do not run any cluster management components, allowing them to fully commit their resources to computation. Worker nodes participate in distributed scheduling and contribute to Ray\'s object store, which enables efficient sharing of intermediate results across tasks.\\n\\nThe follwing figure illustrates the core components of a Ray cluster:\\n\\n![Ray Components](figure2.png)\\n***Figure 1: Ray Cluster Components***\\n\\nBy integrating these core components, Ray delivers a flexible and powerful framework for distributed computing\u2014capable of supporting diverse workloads such as large-scale data processing, machine learning pipelines, and real-time inference.\\n\\n:::info\\nRay nodes are implemented as pods when running on Kubernetes.\\n:::\\n\\n### From Cores to Clusters: Understanding Worker Distribution and Computation Scaling\\n\\nAs mentioned, in a Ray cluster, worker nodes are responsible for executing tasks and actors, utilizing the available computational resources such as CPU cores and GPUs. The distribution of workloads across these resources is managed through Ray\'s resource allocation mechanisms. The resource allocation is managed by the **Ray Scheduler**, which assigns tasks and actors to nodes based on specified resource requirements.\\n\\n- **Resource Specification:**\\n\\nDevelopers can specify worker resource requirements using the @ray.remote decorator and ray_actor_options to allocate CPUs, GPUs, and custom resources. The scheduler then ensures tasks are executed on nodes with sufficient resources to optimize cluster performance.\\n\\n```python\\nimport ray\\n\\n# defining task resources using the @ray.remote decorator\\n@ray.remote(num_cpus=2, num_gpus=1)\\ndef my_function():\\n    # Function implementation\\n    pass\\n\\n@ray.remote(num_cpus=1)\\nclass MyActor:\\n    # Actor implementation\\n    pass\\n```\\n\\nIn this example, `my_function` requires 2 CPU cores and 1 GPU, while `MyActor` requires 1 CPU core. By default, if resource requirements are not specified, Ray assigns 1 CPU core to each task or actor.\\n\\n- **Fractional Resource Allocation:**\\n\\nAdditionally, ray supports fractional resource specifications, allowing tasks or actors to utilize portions of a resource. This is particularly useful for lightweight tasks or when sharing resources among multiple processes. For example, to allocate half a CPU core to a task\\n\\n```Python\\n# defining a task with fractional CPU allocation\\n@ray.remote(num_cpus=0.5)\\ndef lightweight_task():\\n    # Task implementation\\n    pass\\n```\\n\\nSimilarly, if two actors can share a single GPU:\\n\\n```Python\\n# defining actors with shared GPU allocation\\n@ray.remote(num_gpus=0.5)\\nclass SharedGPUActor:\\n    # Actor implementation\\n    pass\\n```\\n\\nThis configuration allows for efficient utilization of resources by enabling multiple processes to share the same hardware components.\\n\\n- **Node Resource Configuration:**\\n\\nWhen initializing a Ray cluster, you can define the resources available on each node. By default, Ray detects the number of CPU cores and GPUs on a machine and assigns them as available resources. However, you can override these defaults during initialization:[Max Pumperla](https://maxpumperla.com/learning_ray/ch_02_ray_core/)\\n\\n```python\\nimport ray\\n\\nray.init(num_cpus=4, num_gpus=2, resources={\\"custom_resource\\": 1})\\n````\\nThis command starts a Ray node with 4 CPU cores, 2 GPUs, and an additional custom resource labeled \\"custom_resource\\" with a quantity of 1. \\n\\n- **Resource Management in Distributed Environments:**\\n\\nIn distributed setups, such as those using Kubernetes with KubeRay, resource requests and limits can be specified in the pod templates to ensure appropriate allocation:[Medium+1Ray+1](https://medium.com/google-cloud/simplifying-ray-and-distributed-computing-2c4b5ca72ad8)\\n\\n```yaml\\nresources:\\n  limits:\\n    nvidia.com/gpu: 1\\n  requests:\\n    memory: \\"4Gi\\"\\n    cpu: \\"2\\"\\n```\\n\\nThis configuration requests 2 CPU cores, 4 GiB of memory, and 1 GPU for the pod, ensuring that the Ray worker has the necessary resources allocated by the Kubernetes scheduler.\\n\\nBy explicitly defining resource requirements and configurations, Ray effectively manages the distribution of tasks and actors across CPU cores, processes, machines, and GPUs, optimizing the utilization of computational resources within the cluster.\\n\\n### Ray Scaling mode\\n\\nRay is designed to scale Python applications in two ways: across multiple machines (**horizontal scaling**) and within a single machine (**vertical scaling**).\\n\\n1. **Horizontal Scaling:** Ray seamlessly expands applications from a single node to a cluster of machines. By dynamically distributing tasks across nodes, it enables efficient parallel processing\u2014particularly valuable for large-scale machine learning tasks like distributed training and hyperparameter tuning.\\n2. **Vertical Scaling:** On a single machine, Ray maximizes resource utilization by parallelizing tasks across multiple CPU cores and GPUs. This optimization enhances performance for operations like data preprocessing and model inference.\\n\\nThrough these complementary scaling strategies, Ray offers the flexibility to handle varying computational demands, making it ideal for diverse AI and machine learning applications.\\n\\n### How Data Is Shared Across Worker Nodes\\n\\nIn **Ray**, efficient **data sharing and distributed computation** are foundational to its performance. At the heart of this is the **Ray Object Store**, a distributed shared-memory system designed to facilitate fast and scalable data sharing across the cluster.  Each node in a Ray cluster maintains its **own local object store**, which stores immutable objects\u2014such as datasets or model parameters\u2014used by Ray tasks and actors. This design allows for **zero-copy access** within a node, significantly reducing serialization overhead and memory duplication. When multiple workers on the same node need to access the same object, they can do so directly from shared memory, leading to highly efficient **intra-node data sharing**. (https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring)\\n\\nHowever, when data needs to be accessed across nodes (**inter-node sharing**), Ray\u2019s **distributed scheduler** comes into play. It orchestrates the transfer by serializing the object on the source node, transmitting it over the network, and deserializing it into the object store on the destination node. To avoid unnecessary data movement and associated costs, Ray incorporates **locality-aware scheduling**\u2014a strategy where tasks are preferentially scheduled on nodes where the needed data is already present. This greatly improves system performance and reduces latency.\\n\\nRay also provides a high-level API for data handling through its [**Datasets**](https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring) module, which serves as the primary user-facing Python interface for working with distributed data.\\n\\nAt its core, **Ray Datasets** represent a **distributed dataset abstraction**, where the underlying data is partitioned into blocks that are distributed across the Ray cluster and stored in distributed memory. This architecture enables parallelism by design.\\n\\nEach block of data can be loaded in parallel by worker tasks, with each task pulling a block (e.g., from cloud storage like S3) and storing it in the local object store of its node. The client-side `Dataset` object maintains lightweight references to these distributed blocks, enabling efficient tracking and manipulation without needing to move data unnecessarily. When operations are applied to the `Dataset`, they are executed in parallel across the distributed blocks\u2014allowing scalable data transformations and preprocessing workflows.\\n\\nA typical usage pattern for Ray Datasets involves:\\n\\n1. **Creating** a `Dataset` from external storage or in-memory data.\\n2. **Applying** transformations using parallel operations (e.g., `map`, `filter`, `split`, `groupby`).\\n3. **Consuming** the processed dataset by either writing it to external storage or feeding it into training and scoring pipelines.\\n\\n:::note\\nAs it is shown in the figure 3, the **Ray Datasets API** is not designed to replace general-purpose data processing frameworks like Spark. Instead, it serves as the **last-mile bridge** between upstream ETL pipelines and **distributed applications running on Ray**.\\n\\nThis bridging role becomes especially powerful when combined with **Ray-native DataFrame libraries** during the data processing stage. By keeping data in memory across stages, you can seamlessly run an **end-to-end data-to-ML pipeline entirely within Ray**, without the overhead of writing intermediate results to external storage.\\n\\nIn this architecture, **Ray acts as the universal compute substrate**, and Datasets function as the **distributed data backbone**, connecting each stage of the pipeline\u2014from data ingestion and transformation to training and inference\u2014with high efficiency and flexibility.\\n:::\\n\\n![Ray Datasets](figure3.png)\\n***Figure 2: Ray Data Ingestion Pipeline***\\n\\n### Blocks\\n\\nA **block** is the fundamental unit of data storage and transfer in **Ray Data**. Each block holds a disjoint subset of rows and is stored in Ray\u2019s **shared-memory object store**, enabling efficient parallel loading, transformation, and distribution across the cluster.\\n\\nBlocks can contain data of any modality\u2014such as text, binary data (e.g., images), or numerical arrays. However, the full capabilities of Ray Datasets are best realized with **tabular data**. In this case, each block represents a **partition of a distributed table**, internally stored as an **Apache Arrow Table**, forming a highly efficient, columnar, distributed Arrow dataset.\\n\\nThe figure below illustrates a dataset composed of three blocks, each containing 1,000 rows. The `Dataset` object itself resides in the process that initiates execution (typically the **driver process**), while the blocks are stored as immutable objects in the Ray object store, distributed across the cluster.\\n\\n![image.png](figure4.png)\\n***Figure 3: Ray Dataset Blocks***\\n\\n### **Data format compatibility**\\n\\nRay Datasets supports a wide range of **popular tabular file formats**\u2014including **CSV**, **JSON**, and **Parquet**\u2014as well as various **storage backends** like local disk, **Amazon S3**, **Google Cloud Storage (GCS)**, **Azure Blob Storage**, and **HDFS**. This broad compatibility is made possible by **Arrow\u2019s I/O layer**, which provides a unified and efficient interface for reading and writing data.\\n\\nIn addition to tabular data, Ray Datasets also supports **parallel reads and writes** of **NumPy arrays**, **text**, and **binary files**, enabling seamless ingestion of multi-modal datasets.\\n\\nTogether, this flexible I/O support and Ray\u2019s scalable execution engine make Datasets a powerful tool for **efficiently loading large-scale data** into your cluster\u2014ready for transformation, training, or serving.\\n\\n```python\\n# Read structured data from disk, cloud storage, etc.\\nray.data.read_parquet(\\"s3://path/to/parquet\\")\\nray.data.read_json(\\"...\\")\\nray.data.read_csv(\\"...\\")\\nray.data.read_text(\\"...\\")\\n\\n# Read tensor / image / file data.\\nray.data.read_numpy(\\"...\\")\\nray.data.read_binary_files(\\"...\\")\\n\\n# Create from in-memory objects.\\nray.data.from_objects([list, of, python, objects])\\nray.data.from_pandas([list, of, pandas, dfs])\\nray.data.from_numpy([list, of, numpy, arrays])\\nray.data.from_arrow([list, of, arrow, tables])\\n```\\n\\n### Seamless Data Framework Compatibility\\n\\nBeyond just storage I/O, **Ray Datasets** supports **bidirectional in-memory data exchange** with a wide range of popular data frameworks\u2014making it easy to integrate into your existing workflows.\\n\\nWhen frameworks like **Spark**, **Dask**, **Modin**, or **Mars** are run on Ray, Datasets can interact directly with them in memory, enabling efficient data sharing without unnecessary serialization or disk I/O. For smaller-scale, local data operations, Datasets also works smoothly with **Pandas** and **NumPy**.\\n\\nTo simplify data loading into machine learning models, Ray Datasets includes built-in adapters for both **PyTorch** and **TensorFlow**. These adapters allow you to convert a Ray Dataset into a framework-native structure\u2014such as `torch.utils.data.IterableDataset` or `tf.data.Dataset`\u2014so you can plug them directly into your training loops with minimal effort.\\n\\n```python\\n# Convert from existing DataFrames.\\nray.data.from_spark(spark_df)\\nray.data.from_dask(dask_df)\\nray.data.from_modin(modin_df)\\n\\n# Convert to DataFrames and ML datasets.\\ndataset.to_spark()\\ndataset.to_dask()\\ndataset.to_modin()\\ndataset.to_torch()\\ndataset.to_tf()\\n\\n# Convert to objects in the shared memory object store.\\ndataset.to_numpy_refs()\\ndataset.to_arrow_refs()\\ndataset.to_pandas_refs()\\n```\\n\\n# Deploying Ray Clusters at Scale\\n\\nRay offers native cluster deployment support on these technology stacks:\\n\\n- On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported integrations exist for Azure, Aliyun, and vSphere, and natively on GCP using vertexai.\\n- On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), through the officially supported KubeRay project.\\n- On [Anyscale](https://www.anyscale.com/ray-on-anyscale?utm_source=ray_docs&utm_medium=docs&utm_campaign=ray-doc-upsell&utm_content=ray-cluster-deployment&__hstc=152921254.1a5e5e4ba1437e11b22cfd8a9df17049.1742234597786.1743123888642.1743186807718.5&__hssc=152921254.2.1743186807718&__hsfp=4074942027), a fully managed Ray platform by Ray\'s creators. You can use existing AWS, GCP, Azure and Kubernetes clusters, or utilize Anyscale\'s hosted compute layer.\\n\\n# Deploying Ray Clusters on Vertex AI\\n\\nVertex AI offers a flexible and scalable environment for running Ray, allowing you to harness the power of Ray\u2019s distributed computing within Google Cloud\u2019s managed ML platform. Whether you\'re running training, tuning, or serving workloads, deploying Ray on Vertex AI gives you full control over cluster lifecycle and resource usage.\\n\\nRay clusters on Vertex AI are designed to **stay active** to ensure consistent capacity for critical machine learning workloads or seasonal demand spikes. **Unlike custom jobs**, which automatically release resources after completion, **Ray clusters persist** until explicitly deleted.\\n\\n**When to use long-running Ray clusters:**\\n\\n- You repeatedly submit the **same Ray job** and want to benefit from **data or image caching**.\\n- You run **many short-lived jobs** where startup time outweighs job runtime\u2014making persistent clusters more efficient.\\n\\n### Setting up Ray on VertexAI\\n\\nTo run Ray clusters on Vertex AI, follow these quick setup steps:\\n\\n1. First, **enable the Vertex AI API in your Google Cloud project**:\\n\\n```bash\\ngcloud services enable aiplatform.googleapis.com\\n```\\n\\n2. **Install the Vertex AI SDK** for Python\\n\\nThe SDK includes features such as the [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html), BigQuery integration, cluster management, and prediction APIs.\\n\\n- **Via Google Cloud Console:**\\n\\n  After [creating a Ray cluster](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster), the Vertex AI console provides access to a **Colab Enterprise notebook** that guides you through the SDK installation.\\n\\n- **Via Workbench or Custom Environment:**\\n\\n  If you\'re using **Vertex AI Workbench** or any other Python environment, install the SDK manually:\\n\\n    ```bash\\n    pip install google-cloud-aiplatform[ray]\\n    ```\\n\\n\\n:::tip \ud83d\udd10 Networking & Security Tips for Ray on Vertex AI\\n\\n- \ud83d\udd10 **(Optional)** Enable **VPC Service Controls** to reduce the risk of data exfiltration. Be aware that this restricts access to resources outside the perimeter (e.g., public Cloud Storage buckets).\\n- \ud83c\udf10 **Use an auto mode VPC network** (one per project is recommended). Avoid custom mode or multiple VPC networks in the same project, as these may cause cluster creation to fail.\\n:::\\n\\n3. **Create a Ray Cluster**. To perform this task, you can use either the Google Cloud Console or the Vertex AI SDK for Python. Ray clusters on Vertex AI support up to **2,000 total nodes**, with a maximum of **1,000 nodes per worker pool**. Although there is no limit to the number of worker pools, creating too many (e.g., 1,000 pools with a single node each) can lead to reduced performance. It\'s recommended to balance the number of worker pools and nodes per pool for optimal scalability and efficiency.\\n\\n- **Via GCP Console**\\n\\nTo create a Ray cluster on Vertex AI from the GCP console,\xa0**navigate to the Ray on Vertex AI page in the Google Cloud console, click \\"Create Cluster,\\" configure the cluster settings (name, region, machine types, etc.), and then click \\"Create\\"**\\n\\n![image.png](figure5.png)\\n***Figure 4: Ray Cluster on Vertex AI Menu***\\n\\n![image.png](figure6.png)\\n***Figure 5: Ray Cluster on Vertex AI Creation Form***\\n\\nFor detailed instructions and guidance on choosing the best configuration for your needs, follow the link: https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#console.\\n\\n- **Via Python**\\n\\nAlternatively, you can create, and manage your Ray clusters using Python with the following code snippets:\\n\\n```python\\nimport time\\nimport logging\\nimport ray\\nfrom ray.job_submission import JobSubmissionClient, JobStatus\\nfrom google.cloud import aiplatform as vertexai\\nfrom google.oauth2 import service_account\\n\\nfrom vertex_ray import create_ray_cluster, update_ray_cluster, delete_ray_cluster, list_ray_clusters, get_ray_cluster\\nfrom vertex_ray import Resources, AutoscalingSpec\\n\\n# -----------------------------\\n# Configuration\\n# -----------------------------\\nPROJECT_NAME = \\"<your-project-name>\\"\\nPROJECT_NUMBER = \\"<your-project-number>\\"\\nREGION = \\"us-central1\\"\\nCLUSTER_NAME = \\"ray-cluster\\"\\nSERVICE_ACCOUNT_FILE = \\"<path-to-your-service-account-file>.json\\"\\nRAY_VERSION = \\"2.33\\"\\nPYTHON_VERSION = \\"3.10\\"\\n\\n# -----------------------------\\n# Setup Logging\\n# -----------------------------\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# -----------------------------\\n# Authenticate Vertex AI\\n# -----------------------------\\ncredentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\\nvertexai.init(credentials=credentials, location=REGION)\\n\\n# -----------------------------\\n# Cluster Management\\n# -----------------------------\\ndef get_or_create_basic_ray_cluster():\\n    \\"\\"\\"Create a default Ray cluster on Vertex AI.\\"\\"\\"\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    if cluster:\\n        logger.info(f\\"Cluster {CLUSTER_NAME} already exists.\\")\\n        return cluster.cluster_resource_name\\n\\n    logger.info(f\\"Creating cluster {CLUSTER_NAME}...\\")\\n    head = Resources(machine_type=\\"n1-standard-16\\", node_count=1)\\n    workers = [Resources(\\n        machine_type=\\"n1-standard-8\\",\\n        node_count=2,\\n        accelerator_type=\\"NVIDIA_TESLA_T4\\",\\n        accelerator_count=1\\n    )]\\n    cluster = create_ray_cluster(\\n        head_node_type=head,\\n        worker_node_types=workers,\\n        cluster_name=CLUSTER_NAME,\\n        ray_version=RAY_VERSION,\\n        python_version=PYTHON_VERSION\\n    )\\n    return cluster\\n\\ndef create_autoscaling_ray_cluster():\\n    \\"\\"\\"Create a Ray cluster with autoscaling on Vertex AI.\\"\\"\\"\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    if cluster:\\n        logger.info(f\\"Cluster {CLUSTER_NAME} already exists.\\")\\n        return cluster.cluster_resource_name\\n\\n    logger.info(f\\"Creating cluster {CLUSTER_NAME}...\\")\\n    autoscaling = AutoscalingSpec(min_replica_count=1, max_replica_count=3)\\n    head = Resources(machine_type=\\"n1-standard-16\\", node_count=1)\\n    workers = [Resources(\\n        machine_type=\\"n1-standard-16\\",\\n        accelerator_type=\\"NVIDIA_TESLA_T4\\",\\n        accelerator_count=1,\\n        autoscaling_spec=autoscaling\\n    )]\\n    cluster = create_ray_cluster(\\n        head_node_type=head,\\n        worker_node_types=workers,\\n        cluster_name=CLUSTER_NAME,\\n        ray_version=RAY_VERSION,\\n        python_version=PYTHON_VERSION\\n    )\\n    return cluster\\n\\ndef scale_ray_cluster(cluster_resource_name: str, new_replica_count: int):\\n    \\"\\"\\"Update worker replica count of an existing Ray cluster.\\"\\"\\"\\n    cluster = get_ray_cluster(cluster_resource_name)\\n    for worker in cluster.worker_node_types:\\n        worker.node_count = new_replica_count\\n    updated = update_ray_cluster(\\n        cluster_resource_name=cluster.cluster_resource_name,\\n        worker_node_types=cluster.worker_node_types\\n    )\\n    return updated\\n\\ndef delete_cluster(cluster_resource_name: str):\\n    \\"\\"\\"Delete the specified Ray cluster.\\"\\"\\"\\n    logger.info(f\\"Deleting Ray cluster: {cluster_resource_name}\\")\\n    delete_ray_cluster(cluster_resource_name)\\n\\ndef list_clusters():\\n    \\"\\"\\"List all Ray clusters on Vertex AI.\\"\\"\\"\\n    clusters = list_ray_clusters()\\n    for cluster in clusters:\\n        logger.info(cluster)\\n```\\n\\nAfter deploying a Ray cluster, you can start running your Ray applications! As shown in the next figure, you can either use the Ray Jobs API or run the job interactively.\\n\\n![image.png](figure7.png)\\n***Figure 6: Way to run Ray jobs***\\n\\n### 1. **Ray Jobs API (Recommended)**\\n\\nUse the CLI, Python SDK, or REST API to:\\n\\n- Submit jobs with an entrypoint like `python script.py`\\n- Define a runtime environment (e.g., dependencies)\\n- Run jobs remotely and independently of the client connection\\n- View job status, logs, and manage runs\\n\\nJobs are tied to the cluster\u2019s lifetime \u2014 if the cluster stops, so do all jobs.\\n\\n### 2. **Interactive Mode**\\n\\n- SSH into the cluster node and run scripts directly (via `ray attach`)\\n- Use Ray Client (for advanced users) to connect live from your machine\\n\\nNote: Jobs started this way aren\'t tracked by the Ray Jobs API.\\n\\n\ud83d\udc49 [Full Guide](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html)\\n\\n```python\\n# -----------------------------\\n# Ray Job Submission\\n# -----------------------------\\ndef submit_ray_job(script_path: str):\\n    \\"\\"\\"Submit a Ray job to a given cluster.\\"\\"\\"\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    ray_cluster = get_ray_cluster(cluster_resource_name)\\n    client = JobSubmissionClient(f\\"vertex_ray://{ray_cluster.dashboard_address}\\")\\n\\n    job_id = client.submit_job(\\n        entrypoint=f\\"python3 {script_path}\\",\\n        runtime_env={\\n            \\"working_dir\\": \\".\\",\\n            \\"pip\\": [\\n            f\\"ray=={RAY_VERSION}\\"\\n        ],\\n        },\\n    )\\n    while True:\\n        status = client.get_job_status(job_id)\\n        if status == JobStatus.SUCCEEDED:\\n            logger.info(\\"Job succeeded.\\")\\n            break\\n        elif status == JobStatus.FAILED:\\n            logger.error(\\"Job failed.\\")\\n            break\\n        else:\\n            logger.info(\\"Job is running...\\")\\n            time.sleep(30)\\n\\n# -----------------------------\\n# Direct Cluster Usage\\n# -----------------------------\\ndef run_remote_ray_job():\\n    \\"\\"\\"Example Ray job executed on the cluster.\\"\\"\\"\\n    @ray.remote(num_cpus=1)\\n    def heavy_task(x):\\n        return sum(i * i for i in range(x))\\n\\n    cluster_resource_name = f\\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\\"\\n    ray.init(address=f\\"vertex_ray://{cluster_resource_name}\\", ignore_reinit_error=True)\\n\\t\\t\\n\\t\\t# fetch the computation results\\n    results = [heavy_task.remote(1000000) for _ in range(1000)]\\n    outputs = ray.get(results)\\n    logger.info(f\\"Total result: {sum(outputs)}\\")\\n\\n    ray.shutdown()\\n\\n```\\n\\n```python\\n# -----------------------------\\n# Main Entry Point\\n# -----------------------------\\ndef main():\\n    cluster_resource_name = get_or_create_basic_ray_cluster()\\n\\n    if not cluster_resource_name:\\n        raise RuntimeError(\\"Ray cluster creation failed\\")\\n\\n    logger.info(\\"Listing clusters...\\")\\n    clusters = list_ray_clusters()\\n    if not clusters:\\n        raise RuntimeError(\\"No Ray clusters found.\\")\\n\\n    latest_cluster = clusters[-1].cluster_resource_name\\n    logger.info(f\\"Submitting job to cluster: {latest_cluster}\\")\\n    submit_ray_job(\\"ray_job.py\\")\\n    run_remote_ray_job()\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n\\n```\\n\\n***ray_job.py***\\n```python\\nimport time\\nimport ray\\n\\n#Initialize Ray\\nray.init()\\n\\n# Define a computationally intensive task\\n@ray.remote(num_cpus=1)\\ndef heavy_task(x):\\n    \\"\\"\\"\\n    Simulates a heavy workload by performing a CPU-bound operation.\\n    This example calculates the sum of squares for a range of numbers.\\n    \\"\\"\\"\\n    total = 0\\n    for i in range(x):\\n        total += i * i\\n    time.sleep(1)  # Simulate some work duration\\n    return total\\n\\n# Generate a large number of tasks\\nnum_tasks = 1000\\nresults = []\\n# Initialize connection to the Ray cluster on Vertex AI.\\nray.init(ignore_reinit_error=True) # local testing\\nfor i in range(num_tasks):\\n    results.append(heavy_task.remote(1000000))\\n\\n# Retrieve results (this will trigger autoscaling if needed)\\noutputs = ray.get(results)\\n# Print the sum of the results (optional)\\nprint(f\\"Sum of results: {sum(outputs)}\\")\\n# Terminate the process\\nray.shutdown()\\n```\\n\\n### Conclusion\\n\\nIn this post, we explored how **Ray** provides a powerful framework for scaling AI, data science, and Python applications across distributed infrastructure. We walked through the process of creating and managing Ray clusters using both the **Google Cloud Console** and the **Vertex AI Python SDK**, including how to configure **autoscaling** for dynamic resource management. The accompanying code examples showcased essential capabilities such as **cluster provisioning**, **job submission**, and efficiently executing **distributed workloads** across multiple nodes.\\n\\nIn upcoming posts, we\u2019ll dive deeper into:\\n\\n- **Optimizing machine learning pipelines** with Ray\\n- Implementing **distributed training** for deep learning models\\n- Leveraging Ray\u2019s **advanced libraries** (such as Ray Train and Ray Tune) for scalable, production-ready AI\\n\\nStay tuned\u2014and I hope this post has been a helpful introduction to getting started with Ray!\\n\\n### References\\n\\n- [Ray Documentation](https://docs.ray.io/en/latest/)\\n- [Ray Datasets API](https://docs.ray.io/en/latest/data/dataset.html)\\n- [Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview)\\n- [Ray on Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html)\\n- [Ray Datasets for large-scale machine learning ingest and scoring](https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring)\\n- [Getting Started with Ray Core](https://maxpumperla.com/learning_ray)\\n- [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html)"},{"id":"improve-rag-systems-reliability-with-citations","metadata":{"permalink":"/blog/improve-rag-systems-reliability-with-citations","source":"@site/blog/2025-03-25-improve-rag-systems-reliability-with-citations/index.md","title":"Building Trustworthy RAG Systems with In Text Citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"rag","permalink":"/blog/tags/rag"},{"inline":true,"label":"retrieval-augmented-generation","permalink":"/blog/tags/retrieval-augmented-generation"},{"inline":true,"label":"rag-pipelines","permalink":"/blog/tags/rag-pipelines"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"},{"inline":true,"label":"generative-ai","permalink":"/blog/tags/generative-ai"},{"inline":true,"label":"explainable-ai","permalink":"/blog/tags/explainable-ai"},{"inline":true,"label":"ai-for-research","permalink":"/blog/tags/ai-for-research"},{"inline":true,"label":"citation-generation","permalink":"/blog/tags/citation-generation"},{"inline":true,"label":"langchain","permalink":"/blog/tags/langchain"},{"inline":true,"label":"llamaindex","permalink":"/blog/tags/llamaindex"},{"inline":true,"label":"gemini-api","permalink":"/blog/tags/gemini-api"},{"inline":true,"label":"google-genai","permalink":"/blog/tags/google-genai"},{"inline":true,"label":"trustworthy-ai","permalink":"/blog/tags/trustworthy-ai"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"semantic-search","permalink":"/blog/tags/semantic-search"},{"inline":true,"label":"python","permalink":"/blog/tags/python"}],"readingTime":17.44,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Building Trustworthy RAG Systems with In Text Citations","slug":"improve-rag-systems-reliability-with-citations","description":"Retrieval-Augmented Generation (RAG) has revolutionized how we build question-answering and content creation systems. By combining the power of large language models (LLMs) with external knowledge retrieval, RAG systems can generate more accurate, informative, and up-to-date responses. However, a critical aspect often overlooked is trustworthiness. This is where citations come in. Without citations, a RAG system is a \\"black box,\\".This post will explain the importance of citations in RAG systems and provide some implementations using Google\'s Generative AI SDK, LangChain, and LlamaIndex, with detailed code walkthroughs.","authors":["haruiz"],"tags":["rag","retrieval-augmented-generation","rag-pipelines","llms","generative-ai","explainable-ai","ai-for-research","citation-generation","langchain","llamaindex","gemini-api","google-genai","trustworthy-ai","machine-learning","semantic-search","python"]},"unlisted":false,"prevItem":{"title":"Getting Started with Ray on Google Cloud Platform","permalink":"/blog/getting-started-with-ray-on-google-cloud-platform"},"nextItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nRetrieval-Augmented Generation (RAG) has emerged as a promising method to address the \u201challucination\u201d problem in large language models (LLMs) by grounding responses in external, traceable knowledge sources <a href=\\"#arvix:1\\">[1]</a>\\n. By integrating retrieval mechanisms with generative capabilities, RAG systems produce more accurate, informative, and up-to-date outputs, making them especially powerful for question-answering and content creation tasks. However, a critical but often overlooked aspect is the ability to attribute claims to their original sources. Without proper citations, RAG becomes a \\"black box,\\" undermining the trustworthiness and verifiability of its responses. While much of the existing work has focused on enhancing response quality, less attention has been paid to source attribution<a href=\\"#arvix:2\\">[2]</a>.\\n\\n:::tip\\n## In This Post You Will Learn\\n\\n- Why **citations are critical** in RAG systems for improving trust, traceability, and reducing hallucinations.\\n- **Some approaches for implementing citations in content generation and RAG pipelines:** source-aware generation, inline citations, post-hoc attribution, and more.\\n- **How to build and implement in-line-text citation-aware RAG pipelines using:**\\n    - **LangChain** + FAISS for LLM-backed scholarly QA.\\n    - **LlamaIndex** for structured workflows and granular citations.\\n    - **Google\u2019s Gemini API** with Google Search grounding for real-time web references.\\n:::\\n\\n## Why are Citations Essential?\\n\\nRetrieving external context is a proven strategy for reducing hallucinations and enhancing the reliability of generative AI outputs. However, surfacing relevant documents is only part of the equation\u2014explicitly citing sources is critical for building user trust and enabling verification. Much of the existing research has focused on citation correctness\u2014whether the cited document supports the claim\u2014but correctness alone isn\u2019t sufficient. To establish true credibility, we also need citation faithfulness, which ensures the cited content accurately reflects the intended meaning of the generated response.\\n\\nTogether, these two dimensions\u2014correctness and faithfulness\u2014are foundational to the trustworthiness, transparency, and usability of RAG systems. They support a range of key benefits:\\n\\n- **Verifiability:** Allow users to trace claims back to original documents.\\n- **Trust:** Build user confidence by grounding outputs in identifiable sources.\\n- **Transparency and Explainability:** Help users understand where information comes from.\\n- **Debugging and Improvement:** Make it easier to audit and correct flawed generations.\\n- **Reduced Hallucinations:** Anchor responses in concrete evidence.\\n- **Respecting Intellectual Property:** Ensure proper attribution to original authors and sources.\\n\\nWhile citations don\u2019t guarantee the elimination of hallucinations, they play a crucial role in mitigating them by offering a transparent path to verify information and understand its original context. In the paper \u201cCorrectness is not Faithfulness in RAG Attributions\u201d <a href=\\"#arvix:2\\">[2]</a>, the authors emphasize the importance of distinguishing between citation correctness (does the source support the claim?) and citation faithfulness (does the citation reflect the actual meaning?). Their work calls for more nuanced evaluation metrics that go beyond simple correctness to better assess the quality and reliability of AI-generated citations.\\n\\nThis distinction is illustrated in the following figure:\\n\\n![Correctness vs. Faithfulness in RAG Attributions](figure.png)\\n*(a) Faithful citation: The model generates the correct answer (\\"Berlin is Germany\'s capital\\") based on a relevant retrieved document (\\"Berlin : German capital\\") and correctly cites that document. (b) Citing related context: The model generates the correct answer, likely based on the relevant document or its memory, but incorrectly cites a related but non-supporting retrieved document (\\"Bonn: no longer capital\\").(c) Correct but unfaithful: The model generates the correct answer using its internal memory, not the retrieved documents, but still cites a retrieved document (\\"Berlin : German capital\\") that happens to support the answer. The citation is unfaithful because the cited source wasn\'t the basis for the generation. (d) Incorrect citation: The model generates the correct answer, likely from memory, but incorrectly cites a retrieved document containing false information (\\"Bonn : German capital\\").*\\n\\n\\n## Implementing Citations in RAG Systems \\n\\nCommon approaches to implementing citations in Retrieval-Augmented Generation (RAG) systems include source-aware generation, highlight-based attribution, post-hoc attribution, inline citations, and aggregated source lists. <a href=\\"#arvix:1\\">[1]</a><a href=\\"#arvix:1\\">[2]</a>\\n\\n- **In source-aware generation**, the model is specifically designed or trained to associate facts with their respective source documents during answer generation. This may involve fine-tuning the model on examples that include citations or labeling retrieved text in the prompt to allow the model to reference those labels.\\n\\n- **Highlight-based** attribution visually connects parts of the output to supporting sources using cues such as color-coding or tooltips. While this method enhances clarity, it requires precise alignment. Instead of merely inserting reference numbers, the system highlights sections of the answer to indicate their sources. For instance, certain sentences may be color-coded or underlined, with hover actions revealing excerpts from the original documents.\\n\\n- **Post-hoc attribution** works by generating the answer first and adding citations afterward. In this strategy, the RAG system produces a response without citations and subsequently searches the retrieved documents for evidence to support each statement, integrating citations into the final output.\\n\\n- **Inline citation** involves embedding references, such as \u201c[1],\u201d directly within the text, thereby improving traceability while necessitating well-structured prompts or additional training. This method is typically implemented by directing the model to insert citations during response generation. For example, a prompt might instruct the model to \u201cinclude source numbers in brackets for the information you use,\u201d leading to sentences that end with references like \u201c[1]\u201d or \u201c[2]\u201d corresponding to the source documents. This live citing method (sometimes termed \u201cpre-hoc\u201d) treats citations as an integral part of the answer.\\n\\n- **Aggregated source** lists provide a summary of all sources utilized without integrating citations directly into the text. In this approach, the RAG system presents the answer followed by a bullet list or section titled \\"Sources,\\" detailing all relevant documents that informed the answer, though it does not specify which fact corresponds to which source within the text.\\n\\nIn the following sections, we will explore three distinct approaches for implementing RAG applications with In-line citations using 1) Google\u2019s Generative AI SDK, Gemini and Google Search grounding, 2) LangChain, and 3) LlamaIndex. Each method offers unique features and trade-offs tailored to different use cases and preferences.\\n\\n## Hands-on Implementations (In-text) Citations\\n\\n### LangChain example \\n\\nIn the following example, we implement a pipeline that searches, downloads, semantically processes, and queries scientific papers from arXiv, leveraging vector search and a Large Language Model (LLM) to generate contextual responses with citations:\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport itertools\\nimport typing\\nfrom pathlib import Path\\nimport arxiv\\nimport os\\nfrom dotenv import load_dotenv\\n\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\\nfrom langchain_experimental.text_splitter import SemanticChunker\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom rich import print\\n\\n# Load environment variables from .env file\\nload_dotenv()\\n\\n# Constants\\nFAISS_INDEX_PATH = Path(\\"faiss_index\\")\\nDOWNLOAD_FOLDER = Path(\\"downloads\\")\\n\\n\\ndef fetch_arxiv_papers(query: str, max_results: int = 5) -> list[arxiv.Result]:\\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\\n    return list(arxiv.Client().results(search))\\n\\n\\ndef process_papers(results: list[arxiv.Result], download_folder: Path) -> list:\\n    openai_embeddings = OpenAIEmbeddings()\\n    text_splitter = SemanticChunker(embeddings=openai_embeddings)\\n    download_folder.mkdir(exist_ok=True, parents=True)\\n    documents = []\\n\\n    for index, result in enumerate(results):\\n        pdf_path = result.download_pdf(dirpath=str(download_folder))\\n        pdf_docs = PyPDFLoader(pdf_path).load_and_split(text_splitter)\\n\\n        for doc in pdf_docs:\\n            doc.metadata.update({\\n                \\"title\\": result.title,\\n                \\"authors\\": [author.name for author in result.authors],\\n                \\"entry_id\\": result.entry_id.split(\'/\')[-1],\\n                \\"year\\": result.published.year,\\n                \\"url\\": result.entry_id,\\n                \\"ref\\": f\\"[{index + 1}]\\"\\n            })\\n        documents.extend(pdf_docs)\\n\\n    return documents\\n\\n\\ndef load_or_create_faiss_index(query: str, index_path: Path, download_folder: Path) -> FAISS:\\n    openai_embeddings = OpenAIEmbeddings()\\n\\n    if index_path.exists():\\n        print(\\"Loading existing FAISS index...\\")\\n        return FAISS.load_local(str(index_path), openai_embeddings, allow_dangerous_deserialization=True)\\n\\n    print(\\"Creating a new FAISS index...\\")\\n    documents = process_papers(fetch_arxiv_papers(query), download_folder)\\n    faiss_index = FAISS.from_documents(documents, openai_embeddings)\\n    faiss_index.save_local(str(index_path))\\n    return faiss_index\\n\\n\\ndef search_similar_documents(faiss_index: FAISS, query: str, top_k: int = 20) -> list:\\n    return faiss_index.similarity_search(query, k=top_k)\\n\\n\\ndef generate_response(context_docs: list, question: str) -> str:\\n    sorted_docs = sorted(context_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\"))\\n    formatted_context = [\\n        f\\"{doc.metadata[\'ref\']} {doc.metadata[\'title\']}: {doc.page_content}\\" for doc in sorted_docs\\n    ]\\n\\n    prompt_template = PromptTemplate(\\n        template=\\"\\"\\"\\n        Write a blog post based on the user query.\\n        When referencing information from the context, cite the appropriate source(s) using their corresponding numbers.\\n        Each source has been provided with a number and a title.\\n        Every answer should include at least one source citation.\\n        If none of the sources are helpful, indicate that.\\n\\n        ------\\n        {context}\\n        ------\\n        Query: {query}\\n        Answer:\\n        \\"\\"\\",\\n        input_variables=[\\"query\\", \\"context\\"]\\n    )\\n\\n    model = ChatOpenAI(model=\\"gpt-4o-mini\\")\\n    parser = StrOutputParser()\\n    chain = prompt_template | model | parser\\n\\n    return chain.invoke({\\"context\\": formatted_context, \\"query\\": question})\\n\\n\\ndef main():\\n    query = \\"hallucination in LLMs\\"\\n    question = \\"How to mitigate hallucination ?\\"\\n\\n    faiss_index = load_or_create_faiss_index(query, FAISS_INDEX_PATH, DOWNLOAD_FOLDER)\\n    relevant_docs = search_similar_documents(faiss_index, question)\\n    response = generate_response(relevant_docs, question)\\n\\n    print(\\"\\\\nGenerated Response:\\\\n\\", response)\\n\\n    bibliography = \\"\\\\n\\\\n### References\\\\n\\"\\n    sorted_docs = sorted(relevant_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\"))\\n\\n    for doc_key, documents in itertools.groupby(sorted_docs, key=lambda doc: doc.metadata.get(\\"ref\\", \\"Unknown\\")):\\n        doc = next(documents)\\n        bibliography += (\\n            f\\"{doc.metadata.get(\'ref\', \'Unknown\')} {doc.metadata.get(\'title\', \'Unknown\')}, \\"\\n            f\\"{\', \'.join(doc.metadata.get(\'authors\', \'Unknown\'))}, arXiv {doc.metadata.get(\'entry_id\', \'Unknown\')}, \\"\\n            f\\"{doc.metadata.get(\'year\', \'Unknown\')}. {doc.metadata.get(\'url\', \'Unknown\')}\\\\n\\"\\n        )\\n    print(bibliography)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\n\\nThis workflow uses LangChain, OpenAI\'s models, FAISS for vector storage, and arXiv\'s Python client. Let\u2019s break it down:\\n\\n### Key Steps:\\n\\n#### 1. **Environment Setup and Imports**\\nThe code imports standard libraries (`os`, `itertools`, `pathlib`, etc.), third-party tools (`arxiv`, `dotenv`), and key LangChain modules for PDF loading, text splitting, embeddings, LLM interaction, and vector storage. It also loads API keys securely via environment variables.\\n\\n#### 2. **Query arXiv for Relevant Papers**\\n```python\\nfetch_arxiv_papers()\\n```\\nUses the `arxiv` Python package to search for academic papers matching a query string. Returns a list of arXiv `Result` objects.\\n\\n#### 3. **Download and Process Papers**\\n```python\\nprocess_papers()\\n```\\nDownloads the PDF files, splits them into semantically meaningful text chunks using OpenAI embeddings, and attaches rich metadata such as title, authors, publication year, and a reference number.\\n\\n#### 4. **Create or Load FAISS Index**\\n```python\\nload_or_create_faiss_index()\\n```\\nChecks whether a local FAISS index already exists. If not, it creates one by embedding the paper chunks and saving the index locally.\\n\\n#### 5. **Semantic Search over the Indexed Chunks**\\n```python\\nsearch_similar_documents()\\n```\\nTakes a user query and searches for the most semantically similar document chunks using vector similarity (k-nearest neighbors search).\\n\\n#### 6. **Generate an LLM-Based Answer with Citations**\\n```python\\ngenerate_response()\\n```\\nUses a prompt template and an OpenAI Chat model (`gpt-4o-mini`) to generate a response grounded in the top retrieved documents, including source references inline using numbered citations.\\n\\n#### 7. **Main Program Logic**\\n```python\\nmain()\\n```\\nPuts all the above steps together to:\\n- Search and download papers about *hallucinations in LLMs*\\n- Answer the question: *How to mitigate hallucination?*\\n- Print the generated response\\n- Print a formatted bibliography of the cited papers\\n\\n### LlamaIndex example <a href=\\"#llamaindex:1\\">[3]</a>\\n\\nFollowing with hands-on implementations, let\'s explore how to build a citation-aware query engine using LlamaIndex. This implementation starts by retrieving relevant text chunks from a set of documents, splits them into citable segments, and uses a Large Language Model (LLM) to synthesize a well-cited answer to a given query.\\n\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport asyncio\\nimport logging\\nfrom typing import List, Union\\n\\nfrom dotenv import load_dotenv\\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\\nfrom llama_index.core.workflow import Context, Workflow, StartEvent, StopEvent, step, Event\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.embeddings.openai import OpenAIEmbedding\\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, TextNode\\nfrom llama_index.core.response_synthesizers import ResponseMode, get_response_synthesizer\\nfrom llama_index.core.node_parser import SentenceSplitter\\nfrom llama_index.core.prompts import PromptTemplate\\n\\n# Load environment variables\\nload_dotenv(verbose=True)\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Prompt templates for citation-based QA\\nCITATION_QA_TEMPLATE = PromptTemplate(\\n    \\"Please provide an answer based solely on the provided sources. \\"\\n    \\"When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. \\"\\n    \\"Every answer should include at least one source citation. \\"\\n    \\"Only cite a source when you are explicitly referencing it. \\"\\n    \\"If none of the sources are helpful, you should indicate that. \\\\n\\"\\n    \\"Example:\\\\n\\"\\n    \\"Source 1:\\\\nThe sky is red in the evening and blue in the morning.\\\\n\\"\\n    \\"Source 2:\\\\nWater is wet when the sky is red.\\\\n\\"\\n    \\"Query: When is water wet?\\\\n\\"\\n    \\"Answer: Water will be wet when the sky is red [2], which occurs in the evening [1].\\\\n\\"\\n    \\"Now it\'s your turn. Below are several numbered sources:\\\\n\\"\\n    \\"{context_str}\\\\nQuery: {query_str}\\\\nAnswer: \\"\\n)\\n\\nCITATION_REFINE_TEMPLATE = PromptTemplate(\\n    \\"Please refine the existing answer based solely on the provided sources. \\"\\n    \\"Cite sources where necessary, following this format:\\\\n\\"\\n    \\"Example:\\\\n\\"\\n    \\"Existing answer: {existing_answer}\\\\n\\"\\n    \\"{context_msg}\\\\n\\"\\n    \\"Query: {query_str}\\\\nRefined Answer: \\"\\n)\\n\\nDEFAULT_CITATION_CHUNK_SIZE = 512\\nDEFAULT_CITATION_CHUNK_OVERLAP = 20\\n\\n\\nclass RetrieverEvent(Event):\\n    \\"\\"\\"Event triggered after document retrieval.\\"\\"\\"\\n\\n    nodes: List[NodeWithScore]\\n\\n\\nclass CreateCitationsEvent(Event):\\n    \\"\\"\\"Event triggered after creating citations.\\"\\"\\"\\n\\n    nodes: List[NodeWithScore]\\n\\n\\nclass CitationQueryEngineWorkflow(Workflow):\\n    \\"\\"\\"Workflow for processing queries with retrieval-augmented generation (RAG).\\"\\"\\"\\n\\n    @step\\n    async def retrieve(self, ctx: Context, ev: StartEvent) -> Union[RetrieverEvent, None]:\\n        \\"\\"\\"Retrieve relevant nodes based on the query.\\"\\"\\"\\n        query = ev.get(\\"query\\")\\n        if not query:\\n            logger.warning(\\"No query provided.\\")\\n            return None\\n\\n        logger.info(f\\"Querying database: {query}\\")\\n\\n        await ctx.set(\\"query\\", query)\\n\\n        if ev.index is None:\\n            logger.error(\\"Index is empty. Load documents before querying!\\")\\n            return None\\n\\n        retriever = ev.index.as_retriever(similarity_top_k=2)\\n        nodes = retriever.retrieve(query)\\n\\n        logger.info(f\\"Retrieved {len(nodes)} nodes.\\")\\n        return RetrieverEvent(nodes=nodes)\\n\\n    @step\\n    async def create_citation_nodes(self, ev: RetrieverEvent) -> CreateCitationsEvent:\\n        \\"\\"\\"Create granular citation nodes from retrieved text chunks.\\"\\"\\"\\n        nodes = ev.nodes\\n        new_nodes: List[NodeWithScore] = []\\n\\n        text_splitter = SentenceSplitter(\\n            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\\n            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\\n        )\\n\\n        for node in nodes:\\n            text_chunks = text_splitter.split_text(\\n                node.node.get_content(metadata_mode=MetadataMode.NONE)\\n            )\\n\\n            for idx, text_chunk in enumerate(text_chunks, start=len(new_nodes) + 1):\\n                text = f\\"Source {idx}:\\\\n{text_chunk}\\\\n\\"\\n\\n                new_node = NodeWithScore(\\n                    node=TextNode.model_validate(node.node), score=node.score\\n                )\\n                new_node.node.text = text\\n                new_nodes.append(new_node)\\n\\n        logger.info(f\\"Created {len(new_nodes)} citation nodes.\\")\\n        return CreateCitationsEvent(nodes=new_nodes)\\n\\n    @step\\n    async def synthesize(self, ctx: Context, ev: CreateCitationsEvent) -> StopEvent:\\n        \\"\\"\\"Generate an AI response based on retrieved citations.\\"\\"\\"\\n        llm = OpenAI(model=\\"gpt-4o-mini\\")\\n        query = await ctx.get(\\"query\\", default=None)\\n\\n        synthesizer = get_response_synthesizer(\\n            llm=llm,\\n            text_qa_template=CITATION_QA_TEMPLATE,\\n            refine_template=CITATION_REFINE_TEMPLATE,\\n            response_mode=ResponseMode.COMPACT,\\n            use_async=True,\\n        )\\n\\n        response = await synthesizer.asynthesize(query, nodes=ev.nodes)\\n        return StopEvent(result=response)\\n\\n\\nasync def run_workflow():\\n    \\"\\"\\"Initialize the index and run the query workflow.\\"\\"\\"\\n    logger.info(\\"Loading documents...\\")\\n    documents = SimpleDirectoryReader(\\"downloads\\").load_data()\\n\\n    index = VectorStoreIndex.from_documents(\\n        documents=documents,\\n        embed_model=OpenAIEmbedding(model_name=\\"text-embedding-3-small\\"),\\n    )\\n\\n    logger.info(\\"Running citation query workflow...\\")\\n    workflow = CitationQueryEngineWorkflow()\\n    result = await workflow.run(query=\\"Write a blog post about agents?\\", index=index)\\n\\n    bibliography = \\"\\\\n\\\\n### References\\\\n\\"\\n    for node in result.source_nodes:\\n        bibliography += f\\"{node.get_text()}\\\\n\\"\\n    print(bibliography)\\n\\n    return result\\n\\n\\nif __name__ == \\"__main__\\":\\n    result = asyncio.run(run_workflow())\\n    print(result)\\n```\\n\\n### Key Steps:\\n\\n#### 1. **Environment Setup and Logging Configuration**\\nThe code loads environment variables from `.env` using `dotenv`, and configures logging to help track events during the workflow execution.\\n\\n```python\\nload_dotenv(verbose=True)\\nlogging.basicConfig(level=logging.INFO)\\n```\\n\\n#### 2. **Define Prompt Templates**\\nTwo prompt templates are defined for instructing the LLM:\\n- `CITATION_QA_TEMPLATE`: Generates answers with numbered citations based on the provided context.\\n- `CITATION_REFINE_TEMPLATE`: Refines an existing answer with additional citation context.\\n\\nThese templates ensure the model cites only relevant sources and produces trustworthy output.\\n\\n#### 3. **Declare Custom Events**\\nCustom `Event` classes (`RetrieverEvent`, `CreateCitationsEvent`) are defined to structure the flow of data across the steps of the workflow.\\n\\n\\n#### 4. **Build the Citation Query Workflow**\\n`CitationQueryEngineWorkflow` is a subclass of `Workflow` with three main `@step`s:\\n\\n- **`retrieve()`**:  \\n  Retrieves top-k relevant document nodes from a vector index based on the user\'s query using similarity search.\\n\\n- **`create_citation_nodes()`**:  \\n  Splits the retrieved chunks into smaller, clearly numbered sources (e.g., `Source 1`, `Source 2`), ensuring each text chunk can be referenced independently.\\n\\n- **`synthesize()`**:  \\n  Generates a final, citation-rich response using the GPT-4o-mini model via the LlamaIndex synthesizer tools.\\n\\n#### 5. **Execute the Workflow**\\nThe `run_workflow()` function loads all documents from the `downloads/` directory and builds a `VectorStoreIndex` using OpenAI embeddings. It then runs the query engine on the question:\\n**\\"Write a blog post about agents?\\"**\\n\\nFinally, it prints both the result and a nicely formatted bibliography of all cited text chunks.\\n\\n\\n### Using Grounding with Google Search in the Gemini API example\\n\\nFinally, in this last example, you will learn how to leverage Google Search capabilities within the Gemini API to generate content with inline citations. This approach combines the power of Gemini\u2019s 2.0 with real-time web search to produce informative, grounded responses with proper attributions.\\n\\n\\n### \u2705 Here is all the code:\\n\\n```python\\nimport itertools\\nfrom typing import Optional\\n\\nfrom dotenv import load_dotenv\\nfrom pydantic import BaseModel\\nfrom rich import print\\n\\nfrom google import genai\\nfrom google.genai.types import (\\n    GenerateContentConfig,\\n    Tool,\\n    GoogleSearch,\\n    GroundingChunk\\n)\\n\\nload_dotenv()\\n\\nclass Citation(BaseModel):\\n    \\"\\"\\"Represents a citation extracted from the Gemini grounding metadata.\\"\\"\\"\\n    title: str\\n    score: float\\n    link: str\\n    chunk_index: int\\n    chunk_text: Optional[str] = None\\n    start_index: Optional[int] = None\\n    end_index: Optional[int] = None\\n\\n\\ndef generate_content(prompt: str, model: str) -> genai.types.GenerateContentResponse:\\n    client = genai.Client()\\n    return client.models.generate_content(\\n        model=model,\\n        contents=prompt,\\n        config=GenerateContentConfig(\\n            response_modalities=[\\"TEXT\\"],\\n            tools=[Tool(google_search=GoogleSearch())],\\n        ),\\n    )\\n\\n\\ndef extract_citations(response: genai.types.GenerateContentResponse) -> list[Citation]:\\n    citations = []\\n    grounding_metadata = response.candidates[0].grounding_metadata\\n    for support in grounding_metadata.grounding_supports:\\n        for idx, score in zip(support.grounding_chunk_indices, support.confidence_scores):\\n            chunk: GroundingChunk = grounding_metadata.grounding_chunks[idx]\\n            citations.append(\\n                Citation(\\n                    title=chunk.web.title,\\n                    link=chunk.web.uri,\\n                    score=score,\\n                    chunk_index=idx,\\n                    chunk_text=support.segment.text,\\n                    start_index=support.segment.start_index,\\n                    end_index=support.segment.end_index,\\n                )\\n            )\\n    return citations\\n\\n\\ndef inject_citations_into_text(text: str, citations: list[Citation]) -> str:\\n    citations.sort(key=lambda x: (x.start_index, x.end_index))\\n    offset = 0\\n    for (start, end), group in itertools.groupby(citations, key=lambda x: (x.start_index, x.end_index)):\\n        group_list = list(group)\\n        indices = \\",\\".join(str(c.chunk_index + 1) for c in group_list)\\n        citation_str = f\\"[{indices}]\\"\\n        text = text[:end + offset] + citation_str + text[end + offset:]\\n        offset += len(citation_str)\\n    return text\\n\\n\\ndef format_citation_section(citations: list[Citation]) -> str:\\n    result = \\"\\\\n\\\\n**Citations**\\\\n\\\\n\\"\\n    sorted_citations = sorted(citations, key=lambda x: x.chunk_index)\\n    for chunk_index, group in itertools.groupby(sorted_citations, key=lambda x: x.chunk_index):\\n        citation = list(group)[0]\\n        result += f\\"[{chunk_index + 1}] {citation.title} - {citation.link}\\\\n\\"\\n    return result\\n\\n\\ndef main():\\n    MODEL_NAME = \\"gemini-2.0-flash\\"\\n    response = generate_content(\\"Write a blog post about Agents\\", MODEL_NAME)\\n    citations = extract_citations(response)\\n\\n    generated_text = response.text\\n    final_text = inject_citations_into_text(generated_text, citations)\\n    final_text += format_citation_section(citations)\\n    print(final_text)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n```\\n\\n### Key Steps:\\n\\n#### 1. **Set Up and Define the Citation Schema**\\n```python\\nclass Citation(BaseModel)\\n```\\nDefines a schema for handling citation information, including title, score, link, and text span indices for precise inline placement.\\n\\n#### 2. **Call Gemini\u2019s Multimodal API with Google Search Tooling**\\n```python\\ngenerate_content()\\n```\\nThis function generates content using Gemini (e.g., `gemini-2.0-flash`) and includes a `GoogleSearch` tool, enabling grounded web references.\\n\\n#### 3. **Extract Grounded Citations**\\n```python\\nextract_citations()\\n```\\nPulls out metadata like source titles, URLs, and confidence scores from the model\'s response using Gemini\u2019s `grounding_supports`.\\n\\n#### 4. **Inject Inline Citations**\\n```python\\ninject_citations_into_text()\\n```\\nAdds numbered citation markers like `[1]`, `[2]`, etc., directly into the generated text using the start and end positions returned by Gemini.\\n\\n\\n#### 5. **Format the Citation Section**\\n```python\\nformat_citation_section()\\n```\\nGenerates a clean, markdown-style bibliography list at the end of the post, matching each inline marker with its source.\\n\\n#### 6. **Main Execution**\\n```python\\nmain()\\n```\\nCombines everything: generates content about \\"Agents\\", extracts and injects citations, and prints the final, publication-ready blog post with proper attributions.\\n\\n## Key Differences and Summary\\n\\n- **Grounding with Google Search in the Gemini API** enhances the accuracy and freshness of model responses by leveraging Google\u2019s real-time search and grounding capabilities. It\u2019s straightforward to implement but is tightly integrated with Google\u2019s Gemini models family.\\n\\n- **LangChain:** Provides a modular framework for building RAG pipelines. Offers flexibility in choosing components (document loaders, text splitters, vector stores, LLMs). Requires more manual setup, but allows for greater customization. Focuses on creating a reference string in the metadata and using that in the prompt.\\n- **LlamaIndex:** Offers higher-level abstractions for RAG, including workflows and specialized components for citation handling. Emphasizes creating granular citation nodes for precise referencing. Uses very explicit prompt templates to guide the LLM.\\n\\nAll three approaches achieve the same goal \u2013 generating responses with citations \u2013 but they use different mechanisms and levels of abstraction. The choice of which to use depends on your specific needs and preferences. The most important common thread is the careful management of metadata to track the source of information.\\n\\n## Conclusion\\n\\nIn the era of AI-generated content, trust is everything. Retrieval-Augmented Generation has unlocked new levels of intelligence and context-awareness, but without clear, faithful citations, even the most accurate answers remain suspect. Citations are not just a safeguard against hallucinations\u2014they are the bridge between AI and human understanding, offering transparency, accountability, and traceability. This post has walked through practical, hands-on implementations using LangChain, LlamaIndex, and Google\u2019s Gemini API to demonstrate that citation-aware RAG isn\'t just a research ideal\u2014it\u2019s an achievable standard. As builders and researchers, the responsibility is ours to push beyond plausible-sounding responses and deliver outputs that are grounded, explainable, and verifiably true. The future of reliable AI starts with showing your sources.\\n\\nAll the code snippets and examples in this post are available on GitHub following this link:\\n[llm-app-patterns](https://github.com/haruiz/llm-app-patterns/tree/main)\\n\\n\\n**References**\\n<ul>\\n <li><a id=\\"arvix:1\\" href=\\"https://arxiv.org/abs/2410.11217\\" target=\\"_blank\\">[1] Qian, H., Fan, Y., Zhang, R., & Guo, J. (2024, October 15). On the Capacity of Citation Generation by Large Language Models</a></li>\\n <li><a id=\\"arvix:2\\" href=\\"https://arxiv.org/abs/2412.18004\\" target=\\"_blank\\">[2] Wallat, J., Heuss, M., Maarten, D. R., & Anand, A. (2024, December 23). Correctness is not Faithfulness in RAG Attributions</a></li>\\n<li>\\n<a id=\\"llamaindex:1\\" href=\\"https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/\\" target=\\"_blank\\">[3] LlamaIndex Documentation: Build RAG with in-line citations</a>\\n</li>\\n</ul>"},{"id":"python-for-data-science-exploring-the-syntax","metadata":{"permalink":"/blog/python-for-data-science-exploring-the-syntax","source":"@site/blog/2022-10-26-python-for-data-science-exploring-the-syntax/index.mdx","title":"Python for Data Science Series - Exploring the syntax","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","date":"2022-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":19.62,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Exploring the syntax","slug":"python-for-data-science-exploring-the-syntax","image":"https://haruiz.github.io/img/2022-10-26-python-for-data-science-exploring-the-syntax-og-image.jpg","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Building Trustworthy RAG Systems with In Text Citations","permalink":"/blog/improve-rag-systems-reliability-with-citations"},"nextItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"\x3c!--truncate--\x3e\\n\\n### Introduction\\n\\nIn the [last post](python-for-data-science-part-getting-started) we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python by creating a simple program that uses Google Cloud Vision API to detect faces in an image.\\n\\nYou will learn today:\\n- What is a computer program?\\n- How to write a program in Python?\\n  - Python syntax\\n  - How to organize your code in python using functions\\n- What is a REST API, and how to use it?\\n- How to use Google Cloud Vision API to detect faces in an image?\\n\\nSo lets started!!!\\n\\n# What is a computer program?\\n\\nA computer program is a sequence of instructions we write using a programming language to tell the computer what to do for us. This sequence of instructions contains but is not limited to:\\nShow information to the user, ask the user for input, save and recover data in memory/disk, and perform calculations. So, programming languages provide a set of built-in functions and instructions that can be used to accomplish these tasks.\\n\\n:::tip\\nIf we think about programming languages, we can compare them to different idioms we use to communicate with others. We choose the appropriate language based on the application or where our program will run.\\n:::\\n\\n# How to write a program in Python?\\n\\nWhen we write programs independent of the programming language we decide to use, writing down our algorithm in simple words is always helpful. In a way, we can have a mental model of what our program will be doing and how it will be executed. To do so, we can use Pseudocode, a simplified version of computer programs written in natural or human-readable language that can be easily interpreted. You can check this [**cheat sheet**](https://cheatography.com/lcheong/cheat-sheets/pseudocode/) that will help you to write your program in Pseudocode.\\n\\n:::info Algorithm\\nFinite set of rules to be followed in calculations or other problem-solving operations, especially by a computer.\\n:::\\n\\nSo, let\'s define our pseudocode for our face detection program:\\n\\n```pseudocode showLineNumbers\\nVAR image_path as STRING = INPUT(\\"Please provide the path of the image: \\")\\nIF image_path is empty or image_path dont exist THEN\\n    PRINT(\\"image path could not be empty or the image does not exist\\")\\n    EXIT\\nENDIF\\n\\nFUNCTION read_image(image_path) as STRING\\n    VAR image_bytes as BYTES = read(image_path)\\n    RETURN image_bytes\\nENDFUNCTION\\n\\nFUNCTION detect_faces_on_image(image_bytes as BYTES) as LIST:\\n    api_response as LIST = call_face_detection_gcp_api(image_bytes)\\n    IF api_response is empty THEN\\n        PRINT(\\"No faces found\\")\\n        RETURN\\n    faces as LIST = []\\n    FOR json_entity in api_response THEN\\n        face as DICT = {\\n            \\"confidence\\": json_entity.confidence,\\n            \\"bounding_box\\": json_entity.bounding_box,\\n            \\"is_happy\\" : json_entity.joy_likelihood == \\"VERY_LIKELY\\"\\n        }\\n        faces.append(face)\\n    ENDFOR\\n    RETURN faces\\nENDFUNCTION\\n\\nimage_bytes = read_image(image_path)\\nfaces_list = detect_faces(image_bytes)\\ndisplay_detect_faces(faces_list)\\n```\\n\\nAs you can see in Pseudocode, we can skip the implementation details of our program. We write down our algorithm using a high-level language, so in this way, we have a big picture of the tasks we need to perform that we can use later to translate our algorithm into a programming language. In line 13, for instance,  we need to call the Google Cloud Vision API to detect the faces in the image, but we have yet to determine how it will be implemented.\\n\\n## Python syntax\\n\\nTo learn about python syntax, we will navigate through the Pseudocode and convert it into a python script.\\n\\n### Variables\\n\\nProgramming is all about data and manipulating it to solve problems. So, we need to have a way to store data on our computer that we can access later during execution. To do so, we use variables. Variables are a way to store data in memory where we can save almost any data. In Python, it is straightforward to define a variable; we need to use the assignment operator `=` followed by the value we want to store, and the Python interpreter will take care of the rest. Under the hood, it will allocate memory for the variable and store its value. We can save strings, integers, floats, booleans, lists, dictionaries, and other data types. Let\'s see an example:\\n\\n```python\\n# String\\nmy_string = \\"Hello World\\"\\n# Integer\\nmy_integer = 1\\n# Float\\nmy_float = 1.0\\n# Boolean\\nmy_boolean = True\\n# List\\nmy_list = [1, 2, 3]\\n# Dictionary\\nmy_dict = {\\"key\\": \\"value\\"}\\n```\\n\\nWe can obtain the memory address and type of a variable using the `id()` and `type()` functions respectively.\\n\\n```python\\nmy_string = \\"Hello World\\"\\nmy_string_2 = \\"Hello World\\"\\nprint(id(my_string))\\nprint(id(my_string_2))\\nprint(type(my_string))\\nprint(type(my_string_2))\\n```\\n\\nIn our program, in line 1, we define a variable `image_path` and assign it the value of the user input. In Python, the `input()` function allows us to grab information from the user so we can save the value into a variable. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimage_path = input(\\"Please provide the path of the image:\\")\\n```\\n\\nThe syntax is very similar to the Pseudocode. However, you can notice that in Python, we don\'t specify the variable type. That is because Python is a dynamically typed language, meaning that the variable type is inferred during the execution. In terms of productivity, this is very convenient because we don\'t need to worry about specifying the type of the variables when we define them. However, it can sometimes be a source of errors if we are not carefully doing operations.\\n\\nPython will raise an error if we try to perform an operation that is not supported by the type of the variable. Let\'s see an example:\\n\\n```python\\na = 2   \\nb = \\"2\\"\\n# error-line-next\\nprint(a + b)\\n# TypeError: unsupported operand type(s) for +: \'int\' and \'str\'\\n```\\n\\n\\n:::warning Rules for creating variables across languages\\n- A variable name must start with a letter or an underscore character.\\n- A variable name cannot start with a number.\\n- A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ ).\\n- Variable names are case-sensitive (name, Name and NAME are three different variables).\\n- The reserved words(keywords) cannot be used naming the variable.\\n:::\\n\\n### Conditional blocks\\n\\nA common task in programming is to execute a block of code only if a condition is met. In Python, we can use the `if` statement to do so. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\n```\\n\\nIn the example above, we check if the variable `a` is equal to 2. If that is the case, we print the message \\"a is equal to 2\\". We can also use the `else` statement to execute a block of code if the condition is not met. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\nelse:\\n    print(\\"a is not equal to 2\\")\\n```\\n\\nIn our program, we need to check if the user input is empty or if the image path does not exist. We can use the `if` statement to do so. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimport os\\n\\nimage_path = input(\\"Please provide the path of the image:\\")\\nif image_path == \\"\\" or not os.path.exists(image_path):\\n    print(\\"image path could not be empty or the image does not exist\\")\\n    exit()\\n```\\n\\nAgain, we can observe that the syntax is very similar to the Pseudocode, just with the addition of the `os.path.exists()` function in the condition to check whether a image exists or not. The os module it is included in the Python standard library, and it provides a way to interact with the operating system. We also use the `exit()` function to exit the program in case the condition is met. We are going to discuss about modules later in this article.\\n\\n:::info Python Standard Library\\nThe Python Standard Library is a set of modules that comes with the Python installation. It provides a wide range of built-in functions and classes that we can use in our programs for different purposes. You can find more information about the Python Standard Library [here](https://docs.python.org/3/library/index.html).\\n\\nSome of the most used modules are:\\n- **os:** provides a way to interact with the operating system.\\n- **sys:** provides a way to interact with the Python interpreter.\\n- **json:** provides a way to work with JSON data.\\n- **re:** provides a way to work with regular expressions.\\n- **math:** provides a way to work with mathematical operations.\\n- **random:** provides a way to work with random numbers.\\n- **datetime:** provides a way to work with dates and times.\\n- **urllib:** provides a way to work with URLs.It is a very useful module to work with APIs. We are going to use it in the next section to call the Google Cloud Vision API. \\n:::\\n\\n### Functions\\n\\nFunctions are a way to encapsulate a block of code that we can reuse in our program. In Python, we can define a function using the `def` keyword followed by the function name and the parameters. Let\'s see an example:\\n\\n```python\\ndef add(a, b):\\n    \\"\\"\\"\\n    This function adds two numbers\\n    :param a: first number\\n    :param b: second number\\n    :return: sum of the two numbers\\n    \\"\\"\\"\\n    return a + b\\n\\nif __name__ == \\"__main__\\":\\n    print(add(1, 2))\\n```\\n\\nThe paremeters are the variables that we need to pass to the function to perform the task. In the example above, we define a function called `add` that takes two parameters `a` and `b`, and the function returns the sum of the values of the two parameters. We can call the function by using the function name followed by the parameters. In the example above, we call the function `add` with the parameters `1` and `2`. The function returns the value `3` and we print it in the console.\\n\\nIn our program, we have two main functions that we need to implement, `read_image` and `call_face_detection_gcp_api.` The first takes the image path as a parameter and returns the image data. The second takes the image data as a parameter, requests the Google Cloud Vision API to detect faces in the image, and returns the face annotations in JSON format. Let\'s see how we can translate the `read_image`  function from Pseudocode into Python:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n```\\n\\nThere is a new syntax here in the `read_image` function to be discussed. \\n\\n- **Function annotations:** Although not mandatory, we can specify the parameters\' type and the functions\' return value in Python. These are called function annotations. Although the Python interpreter does not enforce them, and we still have to check the type of the parameters programmatically,  annotations are extremely useful for other project contributors to navigate through the code and understand how the function must be called. In the example above, we specify that the function takes a string as a parameter and returns a bytes object.\\n\\nAnother good thing about function annotations is that It makes the function more readable and will also helps to avoid errors when function is called in other parts of the program.\\n\\n- **Context Managers:** It can also be noticed that we use the `with` statement to open the file in the `read_image` function. These blocks of code are called context managers in Python, and in this case, it ensures that the file is closed after the block of code is executed. We will discuss context managers later in other articles since this is an advanced topic. For more information about context managers, you can check the [Python documentation](https://docs.python.org/3/reference/compound_stmts.html#with).\\n\\n- **Encoding:** We can also see that we use the `rb` mode to open the file. This mode allows us to read the file as bytes so we can encode it in base64 to send it to the Google Cloud Vision API. That is required because the API only accepts images encoded in this format. For more information about the `rb` mode, you can check the [Python documentation](https://docs.python.org/3/library/functions.html#open), and face detection API documentation [here](https://cloud.google.com/vision/docs/detecting-faces).\\n\\n:::info Encoding data\\nEncodings are a way to represent a sequence of bytes in a different format. The most common encodings are ASCII, UTF-8, and base64. ASCII is a 7-bit encoding that represents the first 128 characters of Unicode. UTF-8 is a variable-length encoding that represents the first 1,112,064 characters of Unicode. Base64 is a way to represent binary data in ASCII characters and it is used to send binary data in text-based protocols such as HTTP. For more information about encodings, you can check the [Python documentation](https://docs.python.org/3/library/codecs.html#standard-encodings).\\n:::\\n\\n- **Error handling:** In order to catch the errors in our python functions we can use the `try` and `except` block. The `try` statement allows us to execute a block of code and catch the errors that can happen in the `except` statement. A `finally` block can also be used to execute a block of code after the `try` and `except` blocks.\\nLet\'s see how to do this:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    try:\\n        # read and load the image into memory\\n        with open(image_path, \\"rb\\") as f:\\n            return f.read() # read the image\'s bytes\\n    except Exception as e:\\n        raise Exception(\\"Error reading the image: \\", e)\\n    finally:\\n        print(\\"finally block\\")\\n```\\n\\n### Modules\\n\\nModules are a way to group a set of functions and classes in our programs. In Python, we can import a module using the `import` keyword followed by the module name.\\n\\n```python\\nimport os # import the os module\\nif __name__ == \\"__main__\\":\\n    print(os.path.exists(\\"image.jpg\\"))\\n```\\n\\nIn the example above, we import the `os` module and use the `os.path.exists()` function to check if the file `image.jpg` exists. We can also import a specific function from a module using the `from` keyword. Let\'s see an example:\\n\\n```python\\nfrom os import path\\nif __name__ == \\"__main__\\":\\n    print(path.exists(\\"image.jpg\\"))\\n```\\n\\nFollowing with our example, to implement the `call_face_detection_gcp_api` function, we need to import the `urllib` module. This module provides a set of function we can use to call the Google Cloud Vision API. Let\'s see how to do this:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string so it can be sent to the API\\n    :param image_bytes:\\n    :return: base64 string\\n    \\"\\"\\"\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call GCP Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: response the face annotations as JSON\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    image_base64 = image_to_base64(image_bytes)\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # Create request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n```\\n\\nFor more information about using the `urlib` module, you can check the [Python documentation](https://docs.python.org/3/library/urllib.request.html).\\n\\n:::tip What is an REST API?\\nREST stands for Representational State Transfer. It is an architectural style for designing networked applications, that allows to expose data and functionality to external clients in public(wan) or private(lan) networks. Clients could be web applications, mobile applications, or even other services. For more information about REST APIs, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Representational_state_transfer).REST APIs are implemented using HTTP methods. The most common methods are `GET`, `POST`, `PUT`, `PATCH`, and `DELETE`, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods). It also provides standard data formats to send and receive data, for instance `JSON` and `XML`. More information [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages).\\n:::\\n\\nThe video below give you a quick overview of how REST APIs work:\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/7YcW25PHnAA\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Loops\\n\\nLoops are a way to execute a block of code multiple times. In Python, we can use the `for` loop to iterate over a list of elements. Let\'s see an example:\\n\\n```python\\nfor i in range(10):\\n    print(i)\\n```\\n\\nThe `range` function returns a list of numbers from 0 to 10. The `for` loop iterates over the list and prints each element. The `range` function can also receive a start and end value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10):\\n    print(i)\\n```\\n\\nThe `range` function can also receive a step value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10, 2):\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a list of elements. Let\'s see an example:\\n\\n```python   \\nfor i in [1, 2, 3, 4, 5]:\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a dictionary. Let\'s see an example:\\n\\n```python\\nfor key, value in {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3}.items():\\n    print(f\\"key: {key}, value: {value}\\")\\n```\\n\\nThe `for` loop can also be used to iterate over a string. Let\'s see an example:\\n\\n```python\\nfor char in \\"Hello World\\":\\n    print(char)\\n```\\n\\nThe `while` loop is used to execute a block of code while a condition is true. Let\'s see an example:\\n\\n```python\\ni = 0\\nwhile i < 10:\\n    print(i)\\n    i += 1\\n```\\n\\nIn our code in the last function of our script, we need to iterate over the list of faces returned by the API. Let\'s see how we do this in line `18`\\n\\n```python showLineNumbers {18-23}\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: list of faces found\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n```\\n\\nI skipped the `call_face_detection_gcp_api` function explanation since it was supposed to be an introductory tutorial. However, I have tried my best to comment on the code so you can develop an intuition on what the function does. To get more information about how to call the `GCP face detection API,` you can check the official documentation [here](https://cloud.google.com/vision/docs/detecting-faces). You must create a Google Cloud Platform account to use the API. To see how to create the project and get the API key, you can check the [official documentation](https://cloud.google.com/vision/docs/libraries#client-libraries-install-python).\\n\\nIn the next section, we will see how to do more advance things with Python using third party packages and libraries. For now I will leave you with the full code of the script:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string\\n    :param image_bytes:\\n    :return:\\n    \\"\\"\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call Google Cloud Platform Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    image_base64 = image_to_base64(image_bytes)\\n    # Create request body\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # make request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n\\n\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n\\n\\ndef main():\\n    try:\\n        image_path = input(\\"Please provide the path of the image:\\")\\n        assert image_path != \\"\\" and os.path.exists(image_path), \\"image path could not be empty or the image does not exist\\"\\n        # read and return image as bytes\\n        image = read_image(image_path)\\n        # pass the image to the face detection function to detect faces\\n        faces = detect_faces_on_image(image, API_KEY=\\"<GCP API KEY>\\")\\n        # print the number of faces found\\n        print(\\"number of faces found:\\", len(faces))\\n        # iterate over the faces and do something\\n        for face in faces:\\n            print(face[\\"is_happy\\"])\\n    except Exception as e:\\n        print(f\\"Error running the script: {e}\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\nThis is all for this tutorial. I hope you enjoyed it. If you have any questions, please leave a comment below or contact me on LinkedIn. If you want to see more tutorials like this, please subscribe to my newsletter (See top menu). To access the code for this tutorial, you can check the [GitHub repository](https://github.com/haruiz/blog-code/blob/main/python-for-data-science-exploring-the-syntax/main.py)\\n\\n\\n## Useful Links\\n- [GCP Face Detection API](https://cloud.google.com/vision/docs/detecting-faces)\\n- [GCP Machine Learning APIs](https://cloud.google.com/products/ai)\\n- [Python Cheat Sheet](https://www.pythoncheatsheet.org/)\\n- [Python Documentation](https://docs.python.org/3/)\\n- [Python Tutorial](https://docs.python.org/3/tutorial/index.html)\\n- [Python Standard Library](https://docs.python.org/3/library/index.html)\\n- [Python Package Index](https://pypi.org/)\\n- [Python for Data Science](https://www.python.org/about/apps/)\\n- [What is a REST API?](https://www.youtube.com/watch?v=lsMQRaeKNDk&ab_channel=IBMTechnology)\\n- [What is JSON?](https://www.youtube.com/watch?v=iiADhChRriM&ab_channel=IBMTechnology)"},{"id":"python-for-data-science-part-getting-started","metadata":{"permalink":"/blog/python-for-data-science-part-getting-started","source":"@site/blog/2022-08-27-python-for-data-science-getting-started/index.mdx","title":"Python for Data Science Series - Getting started","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","date":"2022-08-27T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":6.76,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Getting started","slug":"python-for-data-science-part-getting-started","image":"https://haruiz.github.io/img/2022-08-27-python-for-data-science-getting-started-og-image.jpg","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"},"nextItem":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","permalink":"/blog/python-environments-with-pyenv-and-poetry"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\nimport VideoPlayer from \\"@site/src/components/VideoPlayer\\";\\n\\n## Introduction\\n\\nProgramming is an essential skill for data scientists. If you are considering starting a data science career, the sooner you learn how to code, the better it will be. Most data sciences jobs rely on programming to automate cleaning and organizing data sets, design databases, fine-tune machine learning algorithms, etc. Therefore, having some experience in programming Languages such as Python, R, and SQL makes your life easier and will allow you to automate your analysis pipelines.\\n\\nIn this week\'s post, we will focus on Python. A general-purpose programming language that allows us to work with data and explore different algorithms and techniques that would be extremely useful to add to our analysis toolbox.\\n\\n### Why should I learn how to program?\\n\\n\\nTo help organizations make better decisions,  a data scientist is a technical expert who uses mathematical and statistical techniques to manipulate, analyze and extract patterns from raw/noisy data to produce information. Those tools include but are not limited to statistical inference, pattern recognition, machine learning, deep learning, etc. \\n\\nData Scientist\'s responsibilities involve:\\n\\n- Work closely with business stakeholders to understand their goals and determine how data can be used to achieve them.  \\n- Fetching information from various sources and analyzing it to get a clear understanding of how an organization performs\\n- Undertaking data collection, preprocessing, and analysis\\n- Building models to address business problems\\n-  Presenting information in a way that your audience can understand using different data visualization techniques\\n\\nAlthough programming is not required to be a data scientist, taking advantage of the power of computers, most of these tasks can be automated. So, programming skills provide data scientists with the superpowers to manipulate, process, and analyze big datasets, automate and develop computational algorithms to produce results (faster and more effectively), and create neat visualizations to present the data more intuitively.\\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/dU1xS07N-FA\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n## Programming languages for data science\\n\\nThere are hundreds of programming languages out there, built for diverse purposes. Some are better suited for web or mobile development, others for data analysis, etc. Choosing the correct language to use will depend on your level of experience, role, and/or project goals. In the last few years, Python has been ranked as one of the top programming languages data scientists use to manipulate, process, and analyze big datasets.\\n\\nBut why is Python so popular? Well, I will list some reasons why data scientists love Python and what makes this language suitable for high productivity and performance in processing large amounts of data.\\n\\n### Why Python?\\n\\n- Python is **open source**, so is freely available to everyone.You can even use it to develop commercial applications.\\n- Python is **Multi-Platform**. It can be run on any platform, including Windows, Mac, Linux, and Raspberry Pi.\\n- Python is a **Multi-paradigm** language, which means it can be used for both object-oriented and functional programming. It comes from you writing code in a way that is easy to read and understand.\\n- Python is **Multi-purpose**, so you can use it to develop almost any kind of application. You can use it to develop web applications, game development, data analysis, machine learning, and much more.\\n- Python syntax is **easy to read** and **easy to write**. So the learning curve is low in comparison to other languages.\\n- Data Science **packages ecosystem**: Python also has [PyPI package index,a python package repository](https://pypi.org/), where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages.Some of the most used libraries for data science and machine learning are:\\n\\n  - [Tensorflow](https://www.tensorflow.org/), a high-performance numerical programming library for deep learning.\\n  - [Pandas](https://pandas.pydata.org/), a Python library for data analysis and manipulation.\\n  - [NumPy](https://www.numpy.org/), a Python library for scientific computing ( that offers an extensive collection of advanced mathematical functions, including linear algebra, Fourier transforms, random number generation, etc.)\\n  - [Matplotlib](https://matplotlib.org/), a Python library for plotting graphs and charts.\\n  - [Scikit-learn](https://scikit-learn.org/stable/index.html), a Python library for machine learning.\\n  - [Seaborn](https://seaborn.pydata.org/), a Python library for statistical data visualization.\\n\\n- **High performance:** Although some people complain about performance in Python (see [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)), mainly caused by some features such as dynamic typing, it is also simple to extend developing modules in other compiled languages like C++ or C which could [speed up your code by 100x.](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)\\n  \\nThe following section will introduce you to the Python programming language, and we will start learning its syntax.\\n\\n## Hands-on Tutorial\\n\\n:::tip\\nTo set up our python environment, we will use `pyenv` and `poetry.` You can learn more about these tools in the previous post.\\n[Python environments with pyenv and poetry](/blog/python-environments-with-pyenv-and-poetry)\\n:::\\n\\nWe will start with a simple program that prints \\"Hello World\\" on the screen, and from there, we will begin navigating into the python syntax, learning some of its keywords and essential building blocks. Start creating a folder called \\"python_demo\\" and a file called \\"hello_world.py.\\" To do so, run the following commands in the terminal:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'cd workspace\' , comment: \\"moving into the workspace directory. It could be any folder in your machine where you want to have your python_demo folder\\"},\\n{ type: \'input\', value: \'mkdir python_demo\' , comment: \\"creating the python_demo folder inside the workspace folder\\" },\\n{ type: \'input\', value: \'cd python_demo\' , comment: \\"going into the python_demo folder\\" },\\n{ type: \'input\', value: \'touch hello_world.py\' , comment: \\"creating the hello_world.py file inside the python_demo folder, in windows use the command type\\" },\\n{ type: \'input\', value: \\"pyenv version\\", comment: \\"checking the python version being used by pyenv to create the Python environment\\"},\\n{ type: \\"output\\", value: \\"python 3.10.0\\"},\\n{ type: \\"input\\", value: \\"poetry init\\", comment: \\"initialize poetry project into the python_demo directory\\"},\\n{ type: \\"input\\", value: \\"poetry install\\", comment: \\"create python environment within the folder\\"}\\n]} />\\n\\nIf all the command runs successfully, you should see the following folder structure:\\n```bash\\n\u251c\u2500\u2500 python_demo\\n\u2502   \u251c\u2500\u2500 hello_world.py\\n\u2502   \u251c\u2500\u2500 poetry.lock\\n\u2502   \u2514\u2500\u2500 pyproject.toml\\n```\\nAnd the `pyproject.toml` file should look like this:\\n```toml\\n[tool.poetry]\\nname = \\"python_demo\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [`Henry Ruiz  <henryruiz22@gmail.com>`]\\n\\n[tool.poetry.dependencies]\\npython = \\"^3.10\\"\\n\\n[tool.poetry.dev-dependencies]\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\nYou can see that the Python version was set to 3.10.0. that will depend on the version of Python you are using with pyenv.\\n\\n:::tip\\nTo check the python version run the command `pyenv version` in the terminal.\\n:::\\n\\nTo open our python_demo folder in pycharm check the animation below.\\n\\n<VideoPlayer videoUrl={require(\\"./open-folder-pycharm.mp4\\").default} />\\n\\nAt this point, you should know how to create and run python files. So, in the coming tutorials, we will be working on the hello_world.py file, exploring the python syntax, and learning cool things about Python and data science.\\n\\nThanks for reading!, and I hope this tutorial helped you to get started with Python.\\n\\n\\n**Some useful resources**\\n\\n- [Python Tutorial](https://docs.python.org/3/tutorial/)\\n- [Python Language Reference](https://docs.python.org/3/reference/)\\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\\n- [Why Coding is important in Data Science](https://www.dqindia.com/coding-important-data-science/)\\n- [Python for Data Science](https://www.geeksforgeeks.org/python-for-data-science/)\\n- [Top programming languages for data scientists in 2022](https://www.datacamp.com/blog/top-programming-languages-for-data-scientists-in-2022)\\n- [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)\\n- [Write Your Own C-extension to Speed Up Python by 100x](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)"},{"id":"python-environments-with-pyenv-and-poetry","metadata":{"permalink":"/blog/python-environments-with-pyenv-and-poetry","source":"@site/blog/2022-08-07-Python-environments-with-pyenv-and-poetry/index.mdx","title":"Python for Data Science Series - Python environments with pyenv and poetry","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","date":"2022-08-07T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":19.34,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","slug":"python-environments-with-pyenv-and-poetry","hide_table_of_contents":false,"image":"https://haruiz.github.io/img/2022-08-07-Python-environments-with-pyenv-and-poetry-og_image.png","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"import AudioPlayer from \\"@site/src/components/AudioPlayer\\";\\n\\n<AudioPlayer audioSrc={require(\\"./audio.wav\\").default} />\\n\\n\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\n\\nimport TOCInline from \'@theme/TOCInline\';\\n\\nimport Image from \'@theme/IdealImage\';\\n\\nimport Tabs from \'@theme/Tabs\';\\n\\nimport TabItem from \'@theme/TabItem\';\\n\\n[//]: # ()\\n[//]: # (:::tip In this post you will learn)\\n\\n[//]: # ()\\n[//]: # (<TOCInline toc={toc} />)\\n\\n[//]: # ()\\n[//]: # (:::)\\n\\n## Introduction\\n\\nPython, a versatile programming language widely embraced in fields such as web development, data science, machine learning, and scientific computing. However, navigating through different Python installations and dependencies can often become overwhelming. On this post we will explore how tools like pyenv and poetry can simplify this process by effectively managing project dependencies. Let\'s embark on this journey of optimizing code environments together!\\n\\n### Why Python?\\n\\nAccording to the [**2022 stack overflow developer survey**](https://survey.stackoverflow.co/2022/#technology-most-loved-dreaded-and-wanted), Python is one of the most widely used programming languages today. Of 71,467 responses, 68% of developers expressed that they love the language and are planning to continue working with Python, and approximately 12.000 of those who haven\'t got the chance to use it have expressed their interest in starting developing with it. Its popularity is mainly due to its simplicity in syntax, expressiveness, and versatility. We can use Python to create any kind of software, from web applications to scientific computing.\\n\\n\\nPython also has [**PyPI package index**](https://pypi.org/),a python package repository, where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages. \\n\\n:::info\\nThe Python Package Index, abbreviated as PyPI (/\u02ccpa\u026api\u02c8a\u026a/) and also known as the Cheese Shop (a reference to the Monty Python\'s Flying Circus sketch \\"Cheese Shop\\"), is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl and to the CRAN repository for R.<a href=\\"#wikipedia:1\\">[1]</a>\\n:::\\n### Python Dependency hell\\n\\n\\nWell, it sounds like Python is amazing! However, if you have been using Python for a while, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! An issue commonly known as dependency hell, which is a term associated with the frustration arising from problems managing our project\'s dependencies. \\n\\nDependency hell in Python often happens because pip does not have a dependency resolver and because all dependencies are shared across projects. So, other projects could be affected when a given dependency may need to be updated or uninstalled. \\n\\nOn top of it, since Python doesn\'t distinguish between different versions of the same library in the `/site-packages` directory, this leads to many conflicts when you have two projects requiring different versions of the same library or the global installation doesn\'t match.\\n\\nThus, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./dependency-hell.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\n### Virtual environments to the rescue!\\n\\nA Python virtual environment is a separate folder where only your project\'s dependencies(packages) are located. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and its own independent set of installed Python packages in its site directories. That is a very convenient way to prevent `Dependency Hell.`\\n\\n:::tip\\nPython virtual environment allows multiple versions of Python to coexist in the same machine, so you can test your application using different Python versions. It also keeps your project\'s dependencies isolated, so they don\'t interfere with the dependencies of others projects.\\n:::\\n\\nThere are different tools out there that can be used to create Python virtual environments. In this post, I will show you how to use pyenv and poetry. However, you can also try other tools, such as [virtualenv](https://virtualenv.pypa.io/en/latest/) or anaconda, and based on your experience, you can choose that one you feel most comfortable with.\\nthe video below will provide you with more information about these kinds of tools.\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/3J02sec99RM\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Pyenv\\npyenv is a command line tool which allows you to install and run multiple versions of Python in the same machine. For those who come from a javascript background, pyenv is a very similar tool to nvm.\\n\\n**Setup & get started with pyenv**\\n\\nYou can follow the steps below for installing `pyenv` on macOS or check the [documentation](https://github.com/pyenv/pyenv) for alternative installation methods. \\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl https://pyenv.run | bash\', comment: \\"Install pyenv\\"},\\n{type: \'output\', value: \'Installing pyenv...\'},\\n{type: \'output\', value: \'Installation complete!\'}\\n]} />\\n\\n\\nAfter having installed pyenv, you can then install any python version running the command `pyenv install <version>`.\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv install 3.9.0\', comment: \\"Install python 3.9.0 in my machine\\"},\\n{type: \'output\', value: \'Downloading Python-3.9.0.tar.xz...\'},\\n{type: \'output\', value: \'-> https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xz\', delay: 1000},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installed Python-3.9.0 to /Users/haruiz/.pyenv/versions/3.9.0\'}\\n]} />\\n\\n:::tip\\nif you are not sure about which versions are available to be installed in your machine, you can run the command `pyenv install --list`.\\n:::\\n\\nYou can run the command `pyenv versions` to check which Python versions have been installed.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv versions\' , comment: \\"Check which versions of Python are installed\\"},\\n{type: \'output\', value: \'system\'},\\n{type: \'output\', value: \'* 3.10.0 (set by /Users/haruiz/.pyenv/version)\'},\\n{type: \'output\', value: \'3.9.0\'}\\n]} />\\n\\nTo set the default version of Python to be used, you can run the command `pyenv global <version>`. This version will be used when you run `python` or `python3` in your terminal.\\n\\n<TermynalReact lines ={[\\n{type: \'input\', value: \'pyenv global 3.10.0\', comment: \\"Set python 3.10.0 as the default version\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nAlternatively to the `pyenv global` command, Sometimes you want to set a specific version of Python to be used within a specific folder. You can create a `.python-version` file in the folder and set the version you want to use,  or by running the command `pyenv local <version>`. pyenv will then use this version when you run `python` or `python3` in the folder.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'mkdir myproject\', comment: \\"Create a folder called myproject\\"},\\n{type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n{type: \'input\', value: \'pwd\',  comment: \\"Check the current directory after the cd command\\"},\\n{type: \'output\', value: \'/Users/haruiz/myproject\'},\\n{type: \'input\', value: \'pyenv local 3.9.0\', comment: \\"Set python 3.9.0 as the default version in myproject\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nTo make sure what python version is being used by pyenv, you can run the command `pyenv version`.\\n\\n### Poetry\\n\\nPoetry is a tool that allows you to manage your project\'s dependencies and facilitates the process of packaging for distribution. It resolves your project dependencies and makes sure that there are no conflicts between them.\\n\\nPoetry integrates with the [PyPI](https://pypi.org/) package index to find and install your environment dependencies, and pyenv to set your project python runtime.\\n\\nTo install poetry we follow the steps below:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\', comment: \\"Install poetry\\"},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installation complete!\'},\\n{type: \\"input\\", value: `export PATH=\\"\\\\$HOME/.poetry/bin:\\\\$PATH\\"`, comment: \\"Add poetry to the PATH\\"},\\n{type: \'input\', value: \'poetry --version\', comment: \\"Check the version of poetry after installing it\\"},\\n{type: \'output\', value: \'Poetry version 1.1.13\'},\\n{type: \'input\', value: \'poetry help completions\', comment: \\"Check the completions of poetry\\"},\\n{type: \'output\', value: \'poetry completions bash\'},\\n{type: \'input\', value: \'poetry config virtualenvs.in-project true\', comment: \\"Configure poetry to create virtual environments inside the project\'s root directory\\"}\\n]} />\\n\\nIf you were able to run the previous commands, we can then move forward with the rest of the tutorial.\\n\\nTo ask poetry to create a new project, we use the command `poetry new <project name>`. \\nThis will create a new folder with the name `<project name>` and a `pyproject.toml` folder inside it.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry new myproject\', comment: \\"Create a new project called myproject\\"},\\n    {type: \'output\', value: \'Created package myproject in myproject\'}\\n]} />\\n\\nIf you already have a project, and you want to use poetry to manage the dependencies, you can use the command `poetry init`. So, poetry will add the `pyproject.toml` file to your project.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n    { type: \'input\', value: \'poetry init\', comment: \\"Initialize poetry in myproject\\"},\\n]} />\\n\\n\\nThe main file of your poetry project is the `pyproject.toml` file. This file defines your project\'s dependencies(python packages) and holds the required metadata for packaging. Poetry updates this file every time a new python package is installed. By sharing this file with others, they can recreate your project environment and run your application. To do so, they will need to have poetry installed and run the command `poetry install` within the same folder where the `pyproject.toml` file is located.\\n\\nNow we can start adding dependencies to our project. To do so, we use the command `poetry add <package name>`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry add numpy pandas\', comment: \\"Add numpy and pandas to the project, this command replaces the pip install command\\"},\\n    {type: \'output\', value: \'Installed requests\'}\\n]} />\\n\\nNow our `pyproject.toml` file looks like:\\n\\n```toml\\n    [tool.poetry]\\n    name = \\"myproject\\"\\n    version = \\"0.1.0\\"\\n    description = \\"\\"\\n    authors = [`Henry Ruiz  <henry.ruiz.tamu@gmail.com>`]\\n    \\n    [tool.poetry.dependencies]\\n    python = \\"^3.10\\"\\n    numpy = \\"^1.23.1\\"\\n    pandas = \\"^1.4.3\\"\\n    \\n    [tool.poetry.dev-dependencies]\\n    pytest = \\"^5.2\\"\\n    \\n    [build-system]\\n    requires = [\\"poetry-core>=1.0.0\\"]\\n    build-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nLest review that file sections:\\n\\n- **\\\\[tool.poetry\\\\]:** This section contains informational metadata about our package, such as the package name, description, author details, etc. Most of the config values here are optional unless you\'re planning on publishing this project as an official PyPi package. \\n- **\\\\[tool.poetry.dependencies\\\\]:** This section defines the dependencies of your project. Here is where you define the python packages that your project requires to run. We can update this file manually if it is needed.\\n- **\\\\[tool.poetry.dev-dependencies\\\\]:** This section defines the dev dependencies of your project. These dependencies are not required for your project to run, but they are useful for development.\\n- **\\\\[build-system\\\\]:** This is rarely a section you\'ll need to touch unless you upgrade your version of Poetry.\\n\\nTo see in a nicer format the dependencies of your project, you can use the command `poetry show --tree`. This command draws a graph of all of our dependencies as well as the dependencies of our dependencies.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --tree\', comment: \\"Show the dependencies of our project\\"}]} />\\n\\nIf we are not sure at some point that we have the latest version of a dependency, we can tell poetry to check on our package repository if there is a new version by using \u201c\u2014 latest\u201d option\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --latest\', comment: \\"Show the latest version of our dependencies\\"}]} />\\n\\nIf we list our folder content, we will see that not only the `pyproject.toml` file is created, but also some other folders and files. So, let\'s take a look at the contents of the `myproject` folder.\\n\\n```bash\\n\u251c\u2500\u2500 .venv\\n\u2502\xa0\xa0 \u251c\u2500\u2500 .gitignore\\n\u2502\xa0\xa0 \u251c\u2500\u2500 bin\\n\u2502\xa0\xa0 \u251c\u2500\u2500 lib\\n\u2502\xa0\xa0 \u2514\u2500\u2500 pyvenv.cfg\\n\u251c\u2500\u2500 README.rst\\n\u251c\u2500\u2500 myproject\\n\u2502\xa0\xa0 \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 poetry.lock\\n\u251c\u2500\u2500 pyproject.toml\\n\u2514\u2500\u2500 tests\\n    \u251c\u2500\u2500 __init__.py\\n    \u2514\u2500\u2500 test_myproject.py\\n\\n5 directories, 11 files\\n```\\n\\n- **\\\\`.venv\\\\`**: This folder is created by poetry when it creates a virtual environment.It isolates the project from the system environment and provides a clean environment for your project. It contains the Python interpreter and your projects dependencies. \\n- **poetry.lock**: When Poetry finished installing the dependencies, it writes all of the packages and the exact versions of them to the poetry.lock file, locking the project to those specific versions. \\n\\n:::note\\n Notice that this folder structure is created only if the `poetry new myproject` was executed. When poetry is initialized within a folder that already exists ( using the `poetry init` command), only the `pryproject.toml` and the .env folder are created.\\n:::\\n\\n:::tip\\nYou should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies. For more info, check this link : [Poetry basic usage](https://python-poetry.org/docs/basic-usage/)\\n:::\\n\\nBuilding our project and publishing it is just running the ```poetry build``` and ```poetry publish``` commands, so it is pretty intuitive. The publish command will submit our application to pip, so other developers can easily install it.\\n\\n### Hands-on tutorial \\n\\n**Creating a python package using poetry**\\n\\nIn this section, you will learn how to create a simple python package named `style_image` with poetry. This simple python package takes two images, the style image, and the content image, and performs style transfer. \\"Style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together, so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.\\"<a href=\\"#tensorflow-docs:1\\">[2]</a>\\n\\nFor our `style_image` package we will use the `magenta/arbitrary-image-stylization-v1-256` model available in TensorflowHub under-the-hood.\\n\\nSo, let\'s do it!!\\n\\nWe will start by creating a new project called `style_image` using the command `poetry new style_image`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'pyenv version\', comment: \\"Check the version of python that is being used by pyenv, it would be the python version that will be used by poetry\\"},\\n    { type: \'input\', value: \'poetry new style_image\', comment: \\"Create a new project called style_image\\"},\\n]} />\\n\\n**Installing package dependencies**\\n\\nNext we are going to install the dependencies of our project, so we run the commands:\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd style_image\', comment: \\"Move into the style_image folder where the `pyproject.toml` file is located\\"},\\n    { type: \'input\', value: \'poetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\', comment: \\"Add the dependencies to the project\\", lineDelay: 10.0},\\n    {type: \\"output\\", value: \\"Updating dependencies\\"}, \\n    {type: \\"output\\", value: \\"Resolving dependencies...\\"}, \\n    {type: \\"progress\\", progressPercent: 50},\\n    { type: \'output\', value: `Solver Problem : \\\\n \\nThe current project\\\\\'s Python requirement (>=3.10,<4.0) \\nis not compatible with \\nsome of the required packages Python requirement..... \\\\n\\nFor tensorflow, a possible solution would be to set the \'python\' property to \\">=3.10,<3.11\\"\\n                    `, color: \\"red\\"},\\n]} />\\n\\nWe will see that there is an error trying to install tensorflow:\\n```bash showLineNumbers {14-15}\\nCreating virtualenv style-image in /Users/haruiz/temp/style_image/.venv\\nUsing version ^0.12.0 for tensorflow-hub\\nUsing version ^2.9.1 for tensorflow\\nUsing version ^1.23.1 for numpy\\nUsing version ^9.2.0 for Pillow\\nUsing version ^0.20.0 for validators\\nUsing version ^0.6.1 for typer\\n\\nUpdating dependencies\\nResolving dependencies... (4.2s)\\n\\nSolverProblemError\\n\\nThe current project\'s Python requirement `(>=3.10,<4.0)` is not compatible with some of the required packages Python requirement:\\n- tensorflow-io-gcs-filesystem requires Python `>=3.7, <3.11`, so it will not be satisfied for Python `>=3.11,<4.0`\\n```\\n\\nThe great thing is that poetry generally provides information on how to fix them. For the error above, poetry suggests restricting the python property to `>=3.10,<3.11` in the pyproject.toml file.\\nFor tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to `>=3.10,<3.11`\\n\\n:::tip\\nMake sure you always check the output in the terminal.\\n:::\\n\\nSo the `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nWe can then try to install the dependencies again:\\n\\n```bash\\npoetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\\n```\\n\\nAfter installing the dependencies, our `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9-15}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\ntensorflow-hub = \\"^0.12.0\\"\\nnumpy = \\"^1.23.1\\"\\nPillow = \\"^9.2.0\\"\\ntensorflow = \\"^2.9.1\\"\\nvalidators = \\"^0.20.0\\"\\ntyper = {extras = [\\"all\\"], version = \\"^0.6.1\\"}\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\n**Coding our `style_image` package**\\n\\nAt this point, we are ready to start coding, let\'s create the folder structure below and replace the code in each .py file with the code on this repository [https://github.com/haruiz/style_image](https://github.com/haruiz/style_image):\\n```bash\\n    \u251c\u2500\u2500 README.md\\n    \u251c\u2500\u2500 README.rst\\n    \u251c\u2500\u2500 data\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 content_image.jpg\\n    \u251c\u2500\u2500 main.py\\n    \u251c\u2500\u2500 poetry.lock\\n    \u251c\u2500\u2500 pyproject.toml\\n    \u251c\u2500\u2500 style_image\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 core\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 style_image.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 main.py\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 util\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __pycache__\\n    \u2502\xa0\xa0     \u2514\u2500\u2500 image_utils.py\\n    \u251c\u2500\u2500 stylized_image.png\\n    \u2514\u2500\u2500 tests\\n        \u251c\u2500\u2500 __init__.py\\n        \u2514\u2500\u2500 test_style_image.py\\n```\\nCode :\\n\\n<Tabs>\\n  <TabItem value=\\"main.py\\" label=\\"main.py\\" default>\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\n\\nif __name__ == \\"__main__\\":\\n\\n    content_image_path = \\"data/content_image.jpg\\"\\n    style_image_path = \\"data/style_image.jpg\\"\\n\\n    stylized_image = (\\n        StyleImage(style_image_path)\\n        .transfer(content_image_path, output_image_size=800)\\n        .save(\\"stylized_image.jpg\\")\\n    )\\n```\\n\\n</TabItem>\\n  <TabItem value=\\"core/style_image.py\\" label=\\"core/style_image.py\\">\\n\\n```python showLineNumbers \\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\n\\nfrom style_image.util import ImageUtils\\nfrom PIL import Image as PILImage\\n\\n\\nclass StyleImage:\\n    def __init__(self, style_image_path):\\n        self._style_image_path = style_image_path\\n        hub_handle = (\\n            \\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\\"\\n        )\\n        self._hub_module = hub.load(hub_handle)\\n\\n    def transfer(\\n        self, content_image_path, output_image_size=384, style_img_size=(256, 256)\\n    ):\\n        \\"\\"\\"\\n        transfer the style of the style image to the content image\\n        :param content_image_path: image path of the content image :\\n        :param output_image_size: The content image size can be arbitrary.\\n        :param style_img_size: The style prediction model was trained with image size 256 and it\'s the\\n        recommended image size for the style image (though, other sizes work as\\n        well but will lead to different results).\\n        Recommended to keep it at 256.\\n        :return:\\n        \\"\\"\\"\\n        content_img_size = (output_image_size, output_image_size)\\n        # Load the content and style images.\\n        content_image = ImageUtils.load_image(content_image_path, content_img_size)\\n        style_image = ImageUtils.load_image(self._style_image_path, style_img_size)\\n        # Stylize image.\\n        stylized_image_tensor = self._hub_module(\\n            tf.constant(content_image), tf.constant(style_image)\\n        )[0]\\n        stylized_image_arr = tf.image.convert_image_dtype(\\n            stylized_image_tensor, tf.uint8\\n        ).numpy()\\n        stylized_image_arr = stylized_image_arr[0]  # Remove batch dimension.\\n        stylized_image = PILImage.fromarray(stylized_image_arr)\\n        return stylized_image\\n```\\n\\n  </TabItem>\\n  <TabItem value=\\"util/image_utils.py\\" label=\\"util/image_utils.py\\">\\n\\n```python showLineNumbers \\nimport functools\\nimport tensorflow as tf\\nimport os\\nimport validators\\n\\n\\nclass ImageUtils:\\n    @staticmethod\\n    def crop_center(image):\\n        \\"\\"\\"Returns a cropped square image.\\"\\"\\"\\n        shape = image.shape\\n        new_shape = min(shape[1], shape[2])\\n        offset_y = max(shape[1] - shape[2], 0) // 2\\n        offset_x = max(shape[2] - shape[1], 0) // 2\\n        image = tf.image.crop_to_bounding_box(\\n            image, offset_y, offset_x, new_shape, new_shape\\n        )\\n        return image\\n\\n    @classmethod\\n    @functools.lru_cache(maxsize=None)\\n    def load_image(cls, image_path, image_size=(256, 256), \\n                   preserve_aspect_ratio=True):\\n        \\"\\"\\"Loads and preprocesses images.\\"\\"\\"\\n        # Cache image file locally.\\n        if validators.url(image_path):\\n            image_path = tf.keras.utils.get_file(\\n                os.path.basename(image_path)[-128:], image_path\\n            )\\n        # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\\n        img = tf.io.decode_image(\\n            tf.io.read_file(image_path), channels=3, dtype=tf.float32\\n        )[tf.newaxis, ...]\\n        img = cls.crop_center(img)\\n        img = tf.image.resize(\\n            img, image_size, preserve_aspect_ratio=preserve_aspect_ratio\\n        )\\n        return img\\n\\n``` \\n\\n  </TabItem>\\n<TabItem value=\\"style_image/main.py\\" label=\\"style_image/main.py\\">\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\nimport typer\\n\\napp = typer.Typer()\\n\\n\\ndef style_image_callback(value: str):\\n    style_urls = dict(\\n        kanagawa_great_wave=\\"https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\\",\\n        kandinsky_composition_7=\\"https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\\",\\n        hubble_pillars_of_creation=\\"https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\\",\\n        van_gogh_starry_night=\\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\\",\\n        turner_nantes=\\"https://upload.wikimedia.org/wikipedia/commons/b/b7/JMW_Turner_-_Nantes_from_the_Ile_Feydeau.jpg\\",\\n        munch_scream=\\"https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg\\",\\n        picasso_demoiselles_avignon=\\"https://upload.wikimedia.org/wikipedia/en/4/4c/Les_Demoiselles_d%27Avignon.jpg\\",\\n        picasso_violin=\\"https://upload.wikimedia.org/wikipedia/en/3/3c/Pablo_Picasso%2C_1911-12%2C_Violon_%28Violin%29%2C_oil_on_canvas%2C_Kr%C3%B6ller-M%C3%BCller_Museum%2C_Otterlo%2C_Netherlands.jpg\\",\\n        picasso_bottle_of_rum=\\"https://upload.wikimedia.org/wikipedia/en/7/7f/Pablo_Picasso%2C_1911%2C_Still_Life_with_a_Bottle_of_Rum%2C_oil_on_canvas%2C_61.3_x_50.5_cm%2C_Metropolitan_Museum_of_Art%2C_New_York.jpg\\",\\n        fire=\\"https://upload.wikimedia.org/wikipedia/commons/3/36/Large_bonfire.jpg\\",\\n        derkovits_woman_head=\\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Derkovits_Gyula_Woman_head_1922.jpg\\",\\n        amadeo_style_life=\\"https://upload.wikimedia.org/wikipedia/commons/8/8e/Untitled_%28Still_life%29_%281913%29_-_Amadeo_Souza-Cardoso_%281887-1918%29_%2817385824283%29.jpg\\",\\n        derkovtis_talig=\\"https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg\\",\\n        amadeo_cardoso=\\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Amadeo_de_Souza-Cardoso%2C_1915_-_Landscape_with_black_figure.jpg\\",\\n    )\\n    if value in style_urls:\\n        return style_urls[value]\\n    return value\\n\\n\\n@app.command()\\ndef main(\\n    style_image: str = typer.Option(\\n        ..., \\"--style_image\\", \\"-s\\", callback=style_image_callback\\n    ),\\n    content_image: str = typer.Option(..., \\"--content_image\\", \\"-c\\"),\\n    output_image_size: int = typer.Option(384, \\"--output_image_size\\", \\"-sz\\"),\\n    output_image_path: str = typer.Option(\\"stylized_image.png\\", \\"--output_image_path\\", \\"-o\\"),\\n):\\n    style_image = StyleImage(style_image)\\n    stylized_image = style_image.transfer(content_image, output_image_size=output_image_size)\\n    stylized_image.save(output_image_path)\\n\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n:::note running your scripts using the virtual environment\\nNotice that if you want to execute the `main.py` file or any other file/script using the python environment you just created, you need to run the command `poetry run python main.py.` So, poetry knows that you are running the `main.py` file with the python environment created for the `style_image` package.\\nIf you feel more comfortable running `python main.py,` instead of running `poetry run ...` you can permanently activate the environment running the command `poetry shell.`. So it will be activated for all the commands you run.\\n:::\\n\\nPoetry and pyenv are integrated with `visual studio` code and `Pycharm`. In fact, they will automatically recognize the python environment created by poetry.\\n\\n**Publishing our package to PyPi**\\n\\nPublishing our package in Pypi should be straightforward. We just run the `poetry publish` command. Since this is just a demo, we are going to publish our package to the pypi test repository `https://test.pypi.org/.` However, the steps should be the same in production `https://pypi.org/.`\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry build\', comment: \\"build package\\" },\\n    { type: \'input\', value: \'poetry config repositories.testpypi https://test.pypi.org/legacy/\', comment: \\"add repository\\" },\\n    { type: \'input\', value: \'poetry config repositories\', comment: \\"list repositories\\" },\\n    { type: \'output\', value: `{\'testpypi\': {\'url\': \'https://test.pypi.org/\'}}`, color: \\"gray\\" },\\n    { type: \'input\', value: \'poetry publish -r testpypi\', comment: \\"publish package to testpypi repository\\" },\\n    { type: \'prompt\', value: \'username : haruiz\'},\\n    { type: \'prompt\', value: \'password\'},\\n    { type: \\"output\\", value: \\"Publishing style_image (0.1.0) to testpypi..\\", color:\\"green\\"}\\n  ]} />\\n\\nIf the `publish` command is successful, you will be able to find the package in the testpypi repository.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./pypi-test.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\nThat is all!! We are done!!. You can check the links below for more information about poetry. \\n\\nThanks for your support and don\'t forget to share,\\n\\n\\n**Some useful resources**\\n- [Poetry Documentation](https://python-poetry.org/)\\n- [Pyenv Documentation](https://github.com/pyenv/pyenv)\\n- [Great talk about poetry](https://www.youtube.com/watch?v=QX_Nhu1zhlg&ab_channel=PyGotham2019)\\n- [Package Python Projects the Proper Way with Poetry](https://hackersandslackers.com/python-poetry-package-manager/)\\n- [Poetry: Finally an all-in-one tool to manage Python packages](https://medium.com/analytics-vidhya/poetry-finally-an-all-in-one-tool-to-manage-python-packages-3c4d2538e828)\\n- [Making Python Packages Part 2: How to Publish & Test Your Package on PyPI with Poetry](https://towardsdatascience.com/packages-part-2-how-to-publish-test-your-package-on-pypi-with-poetry-9fc7295df1a5)\\n- [Publishing to a private Python repository with Poetry](https://medium.com/packagr/publishing-to-a-private-python-repository-with-poetry-23b660484471)\\n- [Python Virtual Environments tutorial using Virtualenv and Poetry](https://serpapi.com/blog/python-virtual-environments-using-virtualenv-and-poetry/)\\n- [The Nine Circles of Python Dependency Hell](https://medium.com/knerd/the-nine-circles-of-python-dependency-hell-481d53e3e025)\\n- [Get started with pyenv & poetry. Saviours in the python chaos!](https://blog.jayway.com/2019/12/28/pyenv-poetry-saviours-in-the-python-chaos/)\\n\\n**References**\\n<ul>\\n <li><a id=\\"wikipedia:1\\" href=\\"https://en.wikipedia.org/wiki/Python_Package_Index\\" target=\\"_blank\\">[1] Python Package Index</a></li>\\n <li><a id=\\"tensorflow-docs:1\\" href=\\"https://www.tensorflow.org/tutorials/generative/style_transfer\\" target=\\"_blank\\">[2] Neural style transfer</a></li>\\n</ul>"}]}}')}}]);