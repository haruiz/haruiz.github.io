"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3923],{1310:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/medias/audio-b843a85179165964cec35c8c02fea66d.wav"},1534:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure6-0316e20b495a18b9dfacc3765c04b171.png"},2378:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure2-4bda561da7c4d105aa7b4a31ab063ae0.png"},5209:(e,n,t)=>{t.d(n,{A:()=>j});var s=t(6540);const r="audioPlayer_bD77",a="controlButton_18Mu",i="playPauseButton_BOck",o="timeDisplay__nfh",l="progressBar_GLd5",c="volumeControl_jpxX",d="volumeSlider_ecko",u="speedControl_V7oK",h="speedSelect_tSpO",g="loadingContainer_ejfH",m="loadingText_ca0W",p="spinner_NOhs";var y=t(5604),x=t(4848);const f=e=>{if(isNaN(e)||e===1/0)return"00:00";const n=Math.floor(e/60),t=Math.floor(e%60);return`${String(n).padStart(2,"0")}:${String(t).padStart(2,"0")}`},j=e=>{let{audioSrc:n,title:t="Audio Version"}=e;const[j,b]=(0,s.useState)(!1),[w,_]=(0,s.useState)(0),[k,R]=(0,s.useState)(0),[v,A]=(0,s.useState)(1),[C,I]=(0,s.useState)(!1),[S,P]=(0,s.useState)(1),[T,E]=(0,s.useState)(!1),N=(0,s.useRef)(null),D=(0,s.useRef)(null),U=(0,s.useRef)(null);(0,s.useEffect)((()=>{const e=N.current;if(!e)return;const n=()=>{_(e.duration),R(e.currentTime),E(!0)},t=()=>R(e.currentTime),s=()=>{b(!1),R(e.duration),cancelAnimationFrame(U.current)};return b(!1),R(0),_(0),E(!1),U.current&&cancelAnimationFrame(U.current),e.addEventListener("loadedmetadata",n),e.addEventListener("timeupdate",t),e.addEventListener("ended",s),()=>{e.removeEventListener("loadedmetadata",n),e.removeEventListener("timeupdate",t),e.removeEventListener("ended",s),U.current&&cancelAnimationFrame(U.current)}}),[n]);const G=(0,s.useCallback)((()=>{if(N.current&&D.current){const e=N.current,n=D.current,t=e.currentTime;R(t),n.value=t,n.style.setProperty("--seek-before-width",t/w*100+"%"),U.current=requestAnimationFrame(G)}}),[w]);(0,s.useEffect)((()=>(j?(N.current.play(),U.current=requestAnimationFrame(G)):(N.current.pause(),cancelAnimationFrame(U.current)),()=>cancelAnimationFrame(U.current))),[j,G]);return(0,x.jsxs)("div",{className:r,children:[(0,x.jsx)("audio",{ref:N,src:n,preload:"metadata"}),T?(0,x.jsxs)(x.Fragment,{children:[(0,x.jsx)("button",{onClick:()=>{T&&b((e=>!e))},className:`${a} ${i}`,"aria-label":j?"Pause":"Play",children:j?(0,x.jsx)(y.kwt,{}):(0,x.jsx)(y.gSK,{})}),(0,x.jsx)("span",{className:o,children:f(k)}),(0,x.jsx)("input",{ref:D,type:"range",className:l,value:k,max:w||0,onChange:e=>{if(!T)return;const n=Number(e.target.value);N.current.currentTime=n,R(n),D.current.style.setProperty("--seek-before-width",n/w*100+"%")},"aria-label":"Seek progress",style:{"--seek-before-width":k/w*100+"%"}}),(0,x.jsx)("span",{className:o,children:f(w)}),(0,x.jsxs)("div",{className:c,children:[(0,x.jsx)("button",{onClick:()=>{const e=N.current;if(C){const n=v>0?v:.5;A(n),e.volume=n,I(!1)}else e.volume=0,I(!0),A(0)},className:a,"aria-label":C?"Unmute":"Mute",children:C?(0,x.jsx)(y.FZ2,{}):(0,x.jsx)(y.pPd,{})}),(0,x.jsx)("input",{type:"range",min:"0",max:"1",step:"0.01",value:C?0:v,onChange:e=>{const n=Number(e.target.value);A(n),N.current.volume=n,I(0===n)},className:d,style:{"--volume-before-width":(C?0:100*v)+"%"}})]}),(0,x.jsx)("div",{className:u,children:(0,x.jsxs)("select",{value:S,onChange:e=>{const n=Number(e.target.value);P(n),N.current.playbackRate=n},className:h,"aria-label":"Playback speed",children:[(0,x.jsx)("option",{value:"0.5",children:"0.5x"}),(0,x.jsx)("option",{value:"0.75",children:"0.75x"}),(0,x.jsx)("option",{value:"1",children:"1x"}),(0,x.jsx)("option",{value:"1.25",children:"1.25x"}),(0,x.jsx)("option",{value:"1.5",children:"1.5x"}),(0,x.jsx)("option",{value:"2",children:"2x"})]})})]}):(0,x.jsxs)("div",{className:g,children:[(0,x.jsx)("span",{className:m,children:"Loading audio..."}),(0,x.jsx)("div",{className:p})]})]})}},8733:e=>{e.exports=JSON.parse('{"permalink":"/blog/getting-started-with-ray-on-google-cloud-platform","source":"@site/blog/2025-04-01-getting-started-with-ray-on-google-cloud-platform/index.md","title":"Getting Started with Ray on Google Cloud Platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"ray","permalink":"/blog/tags/ray"},{"inline":true,"label":"google-cloud","permalink":"/blog/tags/google-cloud"},{"inline":true,"label":"vertex-ai","permalink":"/blog/tags/vertex-ai"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"scalable-ai","permalink":"/blog/tags/scalable-ai"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"cloud-computing","permalink":"/blog/tags/cloud-computing"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"data-science","permalink":"/blog/tags/data-science"},{"inline":true,"label":"mlops","permalink":"/blog/tags/mlops"},{"inline":true,"label":"ray-datasets","permalink":"/blog/tags/ray-datasets"},{"inline":true,"label":"parallel-computing","permalink":"/blog/tags/parallel-computing"}],"readingTime":19.275,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Getting Started with Ray on Google Cloud Platform","slug":"getting-started-with-ray-on-google-cloud-platform","description":"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.","authors":["haruiz"],"tags":["ray","google-cloud","vertex-ai","distributed-computing","machine-learning","scalable-ai","python","cloud-computing","kubernetes","data-science","mlops","ray-datasets","parallel-computing"]},"unlisted":false,"nextItem":{"title":"Building Trustworthy RAG Systems with In Text Citations","permalink":"/blog/improve-rag-systems-reliability-with-citations"}}')},8885:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure5-9921c72f0c55c995026f2f2a38c8a359.png"},8996:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var s=t(8733),r=t(4848),a=t(8453),i=t(5209);const o={title:"Getting Started with Ray on Google Cloud Platform",slug:"getting-started-with-ray-on-google-cloud-platform",description:"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax. This post introduces how to get started with Ray on Google Cloud Platform, covering the fundamentals of Ray\u2019s distributed architecture, core components, and scaling strategies. You\u2019ll learn how to deploy and manage Ray clusters on Vertex AI, configure autoscaling, and run distributed Python and machine learning workloads with practical code examples.",authors:["haruiz"],tags:["ray","google-cloud","vertex-ai","distributed-computing","machine-learning","scalable-ai","python","cloud-computing","kubernetes","data-science","mlops","ray-datasets","parallel-computing"]},l=void 0,c={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:3},{value:"In This Post You Will Learn",id:"in-this-post-you-will-learn",level:2},{value:"What is Ray?",id:"what-is-ray",level:3},{value:"Ray&#39;s Core Components",id:"rays-core-components",level:3},{value:"Head Node",id:"head-node",level:4},{value:"Worker Nodes",id:"worker-nodes",level:4},{value:"From Cores to Clusters: Understanding Worker Distribution and Computation Scaling",id:"from-cores-to-clusters-understanding-worker-distribution-and-computation-scaling",level:3},{value:"Ray Scaling mode",id:"ray-scaling-mode",level:3},{value:"How Data Is Shared Across Worker Nodes",id:"how-data-is-shared-across-worker-nodes",level:3},{value:"Blocks",id:"blocks",level:3},{value:"<strong>Data format compatibility</strong>",id:"data-format-compatibility",level:3},{value:"Seamless Data Framework Compatibility",id:"seamless-data-framework-compatibility",level:3},{value:"Setting up Ray on VertexAI",id:"setting-up-ray-on-vertexai",level:3},{value:"1. <strong>Ray Jobs API (Recommended)</strong>",id:"1-ray-jobs-api-recommended",level:3},{value:"2. <strong>Interactive Mode</strong>",id:"2-interactive-mode",level:3},{value:"References",id:"references",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.A,{audioSrc:t(1310).A}),"\n",(0,r.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"As AI and machine learning workloads continue to grow in scale and complexity, the need for flexible and efficient distributed computing frameworks becomes increasingly important. Ray is an open-source framework built to simplify the development and execution of distributed applications using familiar Python syntax."}),"\n",(0,r.jsx)(n.p,{children:"When combined with the power and scalability of Google Cloud Platform (GCP), particularly Vertex AI\u2014Ray enables seamless orchestration of large-scale data science and machine learning workflows, from local prototyping to full production deployments."}),"\n",(0,r.jsx)(n.p,{children:"This post introduces the fundamentals of Ray and walks you through how to deploy and manage Ray clusters on GCP using Vertex AI, empowering you to run scalable and efficient distributed workloads with minimal operational overhead."}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.mdxAdmonitionTitle,{}),(0,r.jsx)(n.h2,{id:"in-this-post-you-will-learn",children:"In This Post You Will Learn"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"What Ray is and how it supports scalable Python and AI applications"}),"\n",(0,r.jsx)(n.li,{children:"Core components of a Ray cluster: head nodes, worker nodes, autoscaler, and scheduler"}),"\n",(0,r.jsx)(n.li,{children:"How to configure and manage Ray clusters on Google Cloud Platform with Vertex AI"}),"\n",(0,r.jsx)(n.li,{children:"How Ray handles distributed task execution and resource allocation across nodes and GPUs"}),"\n",(0,r.jsx)(n.li,{children:"How to submit jobs and interact with Ray clusters using the Ray Job API and Python SDK"}),"\n",(0,r.jsx)(n.li,{children:"Best practices for using Ray Datasets to efficiently ingest, transform, and serve distributed data"}),"\n",(0,r.jsx)(n.li,{children:"Practical examples for configuring autoscaling, deploying workloads, and optimizing cluster usage"}),"\n"]})]}),"\n",(0,r.jsx)(n.h3,{id:"what-is-ray",children:"What is Ray?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ray"})," is an open-source, unified framework engineered to enable scalable and distributed computing for AI and Python-based applications. It provides an efficient and extensible compute layer for executing parallel and distributed workloads, abstracting the complexities typically associated with distributed systems such as resource management, orchestration, and fault tolerance."]}),"\n",(0,r.jsx)(n.p,{children:"By decoupling infrastructure concerns from application logic, Ray allows developers and researchers to seamlessly scale individual components or end-to-end machine learning (ML) pipelines\u2014from prototyping on a local machine to executing large-scale workloads across multi-node, multi-GPU clusters.\nRay\u2019s modular and composable architecture supports diverse roles across the machine learning and data science lifecycle, delivering specific advantages for each:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For Data Scientists:"})," Ray enables transparent parallelization of data preprocessing, feature extraction, model training, and evaluation workflows across heterogeneous compute resources, without requiring in-depth knowledge of distributed systems. Its native integration with popular Python libraries (e.g., NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch) ensures smooth adoption into existing ecosystems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For Machine Learning Engineers:"})," Ray facilitates the development of production-grade ML platforms through unified APIs that allow the same codebase to scale seamlessly from a developer's laptop to large-scale clusters. This consistency accelerates the deployment lifecycle, reduces operational overhead, and ensures reproducibility in model training and experimentation workflows."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For Systems Engineers and DevOps Teams:"})," Ray handles lower-level system functions including task scheduling, resource allocation, fault tolerance, and elastic scaling. It also supports Kubernetes-native deployments, making it easier to integrate with modern cloud-native infrastructure and CI/CD pipelines."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"rays-core-components",children:"Ray's Core Components"}),"\n",(0,r.jsxs)(n.p,{children:["Ray enables seamless scaling of high-performance analytical workloads\u2014from a single laptop to a large, distributed cluster. You can get started locally with a simple ",(0,r.jsx)(n.code,{children:"ray.init()"})," call during development. However, to scale applications across multiple nodes in production, you'll need to ",(0,r.jsx)(n.em,{children:"deploy a Ray cluster"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"Ray cluster"})," is a collection of nodes working together to execute tasks and manage data in a distributed environment. Each cluster consists of a ",(0,r.jsx)(n.strong,{children:"head node"})," and one or more ",(0,r.jsx)(n.strong,{children:"worker nodes"}),", each with distinct responsibilities that enable efficient and scalable execution."]}),"\n",(0,r.jsx)(n.h4,{id:"head-node",children:"Head Node"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"head node"})," acts as the central coordinator of the cluster. It runs the core components responsible for orchestration and cluster management, including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Global Control Store (GCS):"})," A distributed metadata store that tracks task specifications, function definitions, object locations, and scheduling events. GCS ensures scalability and fault tolerance across the cluster."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autoscaler:"})," A daemon process running on the head node\u2014or as a sidecar container within the head pod in Kubernetes environments\u2014that continuously monitors resource utilization and dynamically adjusts the number of worker nodes in response to real-time workload demands, enabling elastic cluster scaling both upward and downward as needed."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Driver Processes:"})," Entry points for Ray applications. Each driver launches and manages a job, which may include thousands of distributed tasks and actors."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"While the head node is capable of executing tasks and actors, in larger clusters it's typically configured to focus solely on coordination duties to optimize performance and avoid resource contention."}),"\n",(0,r.jsx)(n.h4,{id:"worker-nodes",children:"Worker Nodes"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Worker nodes"})," are dedicated to executing user-defined tasks and actors. They do not run any cluster management components, allowing them to fully commit their resources to computation. Worker nodes participate in distributed scheduling and contribute to Ray's object store, which enables efficient sharing of intermediate results across tasks."]}),"\n",(0,r.jsx)(n.p,{children:"The follwing figure illustrates the core components of a Ray cluster:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"Ray Components",src:t(2378).A+"",width:"1818",height:"862"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 1: Ray Cluster Components"})})]}),"\n",(0,r.jsx)(n.p,{children:"By integrating these core components, Ray delivers a flexible and powerful framework for distributed computing\u2014capable of supporting diverse workloads such as large-scale data processing, machine learning pipelines, and real-time inference."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Ray nodes are implemented as pods when running on Kubernetes."})}),"\n",(0,r.jsx)(n.h3,{id:"from-cores-to-clusters-understanding-worker-distribution-and-computation-scaling",children:"From Cores to Clusters: Understanding Worker Distribution and Computation Scaling"}),"\n",(0,r.jsxs)(n.p,{children:["As mentioned, in a Ray cluster, worker nodes are responsible for executing tasks and actors, utilizing the available computational resources such as CPU cores and GPUs. The distribution of workloads across these resources is managed through Ray's resource allocation mechanisms. The resource allocation is managed by the ",(0,r.jsx)(n.strong,{children:"Ray Scheduler"}),", which assigns tasks and actors to nodes based on specified resource requirements."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Resource Specification:"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Developers can specify worker resource requirements using the @ray.remote decorator and ray_actor_options to allocate CPUs, GPUs, and custom resources. The scheduler then ensures tasks are executed on nodes with sufficient resources to optimize cluster performance."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import ray\n\n# defining task resources using the @ray.remote decorator\n@ray.remote(num_cpus=2, num_gpus=1)\ndef my_function():\n    # Function implementation\n    pass\n\n@ray.remote(num_cpus=1)\nclass MyActor:\n    # Actor implementation\n    pass\n"})}),"\n",(0,r.jsxs)(n.p,{children:["In this example, ",(0,r.jsx)(n.code,{children:"my_function"})," requires 2 CPU cores and 1 GPU, while ",(0,r.jsx)(n.code,{children:"MyActor"})," requires 1 CPU core. By default, if resource requirements are not specified, Ray assigns 1 CPU core to each task or actor."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Fractional Resource Allocation:"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Additionally, ray supports fractional resource specifications, allowing tasks or actors to utilize portions of a resource. This is particularly useful for lightweight tasks or when sharing resources among multiple processes. For example, to allocate half a CPU core to a task"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-Python",children:"# defining a task with fractional CPU allocation\n@ray.remote(num_cpus=0.5)\ndef lightweight_task():\n    # Task implementation\n    pass\n"})}),"\n",(0,r.jsx)(n.p,{children:"Similarly, if two actors can share a single GPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-Python",children:"# defining actors with shared GPU allocation\n@ray.remote(num_gpus=0.5)\nclass SharedGPUActor:\n    # Actor implementation\n    pass\n"})}),"\n",(0,r.jsx)(n.p,{children:"This configuration allows for efficient utilization of resources by enabling multiple processes to share the same hardware components."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Node Resource Configuration:"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["When initializing a Ray cluster, you can define the resources available on each node. By default, Ray detects the number of CPU cores and GPUs on a machine and assigns them as available resources. However, you can override these defaults during initialization:",(0,r.jsx)(n.a,{href:"https://maxpumperla.com/learning_ray/ch_02_ray_core/",children:"Max Pumperla"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import ray\n\nray.init(num_cpus=4, num_gpus=2, resources={"custom_resource": 1})\n'})}),"\n",(0,r.jsx)(n.p,{children:'This command starts a Ray node with 4 CPU cores, 2 GPUs, and an additional custom resource labeled "custom_resource" with a quantity of 1.'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Resource Management in Distributed Environments:"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["In distributed setups, such as those using Kubernetes with KubeRay, resource requests and limits can be specified in the pod templates to ensure appropriate allocation:",(0,r.jsx)(n.a,{href:"https://medium.com/google-cloud/simplifying-ray-and-distributed-computing-2c4b5ca72ad8",children:"Medium+1Ray+1"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'resources:\n  limits:\n    nvidia.com/gpu: 1\n  requests:\n    memory: "4Gi"\n    cpu: "2"\n'})}),"\n",(0,r.jsx)(n.p,{children:"This configuration requests 2 CPU cores, 4 GiB of memory, and 1 GPU for the pod, ensuring that the Ray worker has the necessary resources allocated by the Kubernetes scheduler."}),"\n",(0,r.jsx)(n.p,{children:"By explicitly defining resource requirements and configurations, Ray effectively manages the distribution of tasks and actors across CPU cores, processes, machines, and GPUs, optimizing the utilization of computational resources within the cluster."}),"\n",(0,r.jsx)(n.h3,{id:"ray-scaling-mode",children:"Ray Scaling mode"}),"\n",(0,r.jsxs)(n.p,{children:["Ray is designed to scale Python applications in two ways: across multiple machines (",(0,r.jsx)(n.strong,{children:"horizontal scaling"}),") and within a single machine (",(0,r.jsx)(n.strong,{children:"vertical scaling"}),")."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Horizontal Scaling:"})," Ray seamlessly expands applications from a single node to a cluster of machines. By dynamically distributing tasks across nodes, it enables efficient parallel processing\u2014particularly valuable for large-scale machine learning tasks like distributed training and hyperparameter tuning."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vertical Scaling:"})," On a single machine, Ray maximizes resource utilization by parallelizing tasks across multiple CPU cores and GPUs. This optimization enhances performance for operations like data preprocessing and model inference."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Through these complementary scaling strategies, Ray offers the flexibility to handle varying computational demands, making it ideal for diverse AI and machine learning applications."}),"\n",(0,r.jsx)(n.h3,{id:"how-data-is-shared-across-worker-nodes",children:"How Data Is Shared Across Worker Nodes"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Ray"}),", efficient ",(0,r.jsx)(n.strong,{children:"data sharing and distributed computation"})," are foundational to its performance. At the heart of this is the ",(0,r.jsx)(n.strong,{children:"Ray Object Store"}),", a distributed shared-memory system designed to facilitate fast and scalable data sharing across the cluster.  Each node in a Ray cluster maintains its ",(0,r.jsx)(n.strong,{children:"own local object store"}),", which stores immutable objects\u2014such as datasets or model parameters\u2014used by Ray tasks and actors. This design allows for ",(0,r.jsx)(n.strong,{children:"zero-copy access"})," within a node, significantly reducing serialization overhead and memory duplication. When multiple workers on the same node need to access the same object, they can do so directly from shared memory, leading to highly efficient ",(0,r.jsx)(n.strong,{children:"intra-node data sharing"}),". (",(0,r.jsx)(n.a,{href:"https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring",children:"https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring"}),")"]}),"\n",(0,r.jsxs)(n.p,{children:["However, when data needs to be accessed across nodes (",(0,r.jsx)(n.strong,{children:"inter-node sharing"}),"), Ray\u2019s ",(0,r.jsx)(n.strong,{children:"distributed scheduler"})," comes into play. It orchestrates the transfer by serializing the object on the source node, transmitting it over the network, and deserializing it into the object store on the destination node. To avoid unnecessary data movement and associated costs, Ray incorporates ",(0,r.jsx)(n.strong,{children:"locality-aware scheduling"}),"\u2014a strategy where tasks are preferentially scheduled on nodes where the needed data is already present. This greatly improves system performance and reduces latency."]}),"\n",(0,r.jsxs)(n.p,{children:["Ray also provides a high-level API for data handling through its ",(0,r.jsx)(n.a,{href:"https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring",children:(0,r.jsx)(n.strong,{children:"Datasets"})})," module, which serves as the primary user-facing Python interface for working with distributed data."]}),"\n",(0,r.jsxs)(n.p,{children:["At its core, ",(0,r.jsx)(n.strong,{children:"Ray Datasets"})," represent a ",(0,r.jsx)(n.strong,{children:"distributed dataset abstraction"}),", where the underlying data is partitioned into blocks that are distributed across the Ray cluster and stored in distributed memory. This architecture enables parallelism by design."]}),"\n",(0,r.jsxs)(n.p,{children:["Each block of data can be loaded in parallel by worker tasks, with each task pulling a block (e.g., from cloud storage like S3) and storing it in the local object store of its node. The client-side ",(0,r.jsx)(n.code,{children:"Dataset"})," object maintains lightweight references to these distributed blocks, enabling efficient tracking and manipulation without needing to move data unnecessarily. When operations are applied to the ",(0,r.jsx)(n.code,{children:"Dataset"}),", they are executed in parallel across the distributed blocks\u2014allowing scalable data transformations and preprocessing workflows."]}),"\n",(0,r.jsx)(n.p,{children:"A typical usage pattern for Ray Datasets involves:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Creating"})," a ",(0,r.jsx)(n.code,{children:"Dataset"})," from external storage or in-memory data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Applying"})," transformations using parallel operations (e.g., ",(0,r.jsx)(n.code,{children:"map"}),", ",(0,r.jsx)(n.code,{children:"filter"}),", ",(0,r.jsx)(n.code,{children:"split"}),", ",(0,r.jsx)(n.code,{children:"groupby"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Consuming"})," the processed dataset by either writing it to external storage or feeding it into training and scoring pipelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{type:"note",children:[(0,r.jsxs)(n.p,{children:["As it is shown in the figure 3, the ",(0,r.jsx)(n.strong,{children:"Ray Datasets API"})," is not designed to replace general-purpose data processing frameworks like Spark. Instead, it serves as the ",(0,r.jsx)(n.strong,{children:"last-mile bridge"})," between upstream ETL pipelines and ",(0,r.jsx)(n.strong,{children:"distributed applications running on Ray"}),"."]}),(0,r.jsxs)(n.p,{children:["This bridging role becomes especially powerful when combined with ",(0,r.jsx)(n.strong,{children:"Ray-native DataFrame libraries"})," during the data processing stage. By keeping data in memory across stages, you can seamlessly run an ",(0,r.jsx)(n.strong,{children:"end-to-end data-to-ML pipeline entirely within Ray"}),", without the overhead of writing intermediate results to external storage."]}),(0,r.jsxs)(n.p,{children:["In this architecture, ",(0,r.jsx)(n.strong,{children:"Ray acts as the universal compute substrate"}),", and Datasets function as the ",(0,r.jsx)(n.strong,{children:"distributed data backbone"}),", connecting each stage of the pipeline\u2014from data ingestion and transformation to training and inference\u2014with high efficiency and flexibility."]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"Ray Datasets",src:t(9955).A+"",width:"1469",height:"459"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 2: Ray Data Ingestion Pipeline"})})]}),"\n",(0,r.jsx)(n.h3,{id:"blocks",children:"Blocks"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"block"})," is the fundamental unit of data storage and transfer in ",(0,r.jsx)(n.strong,{children:"Ray Data"}),". Each block holds a disjoint subset of rows and is stored in Ray\u2019s ",(0,r.jsx)(n.strong,{children:"shared-memory object store"}),", enabling efficient parallel loading, transformation, and distribution across the cluster."]}),"\n",(0,r.jsxs)(n.p,{children:["Blocks can contain data of any modality\u2014such as text, binary data (e.g., images), or numerical arrays. However, the full capabilities of Ray Datasets are best realized with ",(0,r.jsx)(n.strong,{children:"tabular data"}),". In this case, each block represents a ",(0,r.jsx)(n.strong,{children:"partition of a distributed table"}),", internally stored as an ",(0,r.jsx)(n.strong,{children:"Apache Arrow Table"}),", forming a highly efficient, columnar, distributed Arrow dataset."]}),"\n",(0,r.jsxs)(n.p,{children:["The figure below illustrates a dataset composed of three blocks, each containing 1,000 rows. The ",(0,r.jsx)(n.code,{children:"Dataset"})," object itself resides in the process that initiates execution (typically the ",(0,r.jsx)(n.strong,{children:"driver process"}),"), while the blocks are stored as immutable objects in the Ray object store, distributed across the cluster."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"image.png",src:t(9788).A+"",width:"1293",height:"515"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 3: Ray Dataset Blocks"})})]}),"\n",(0,r.jsx)(n.h3,{id:"data-format-compatibility",children:(0,r.jsx)(n.strong,{children:"Data format compatibility"})}),"\n",(0,r.jsxs)(n.p,{children:["Ray Datasets supports a wide range of ",(0,r.jsx)(n.strong,{children:"popular tabular file formats"}),"\u2014including ",(0,r.jsx)(n.strong,{children:"CSV"}),", ",(0,r.jsx)(n.strong,{children:"JSON"}),", and ",(0,r.jsx)(n.strong,{children:"Parquet"}),"\u2014as well as various ",(0,r.jsx)(n.strong,{children:"storage backends"})," like local disk, ",(0,r.jsx)(n.strong,{children:"Amazon S3"}),", ",(0,r.jsx)(n.strong,{children:"Google Cloud Storage (GCS)"}),", ",(0,r.jsx)(n.strong,{children:"Azure Blob Storage"}),", and ",(0,r.jsx)(n.strong,{children:"HDFS"}),". This broad compatibility is made possible by ",(0,r.jsx)(n.strong,{children:"Arrow\u2019s I/O layer"}),", which provides a unified and efficient interface for reading and writing data."]}),"\n",(0,r.jsxs)(n.p,{children:["In addition to tabular data, Ray Datasets also supports ",(0,r.jsx)(n.strong,{children:"parallel reads and writes"})," of ",(0,r.jsx)(n.strong,{children:"NumPy arrays"}),", ",(0,r.jsx)(n.strong,{children:"text"}),", and ",(0,r.jsx)(n.strong,{children:"binary files"}),", enabling seamless ingestion of multi-modal datasets."]}),"\n",(0,r.jsxs)(n.p,{children:["Together, this flexible I/O support and Ray\u2019s scalable execution engine make Datasets a powerful tool for ",(0,r.jsx)(n.strong,{children:"efficiently loading large-scale data"})," into your cluster\u2014ready for transformation, training, or serving."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Read structured data from disk, cloud storage, etc.\nray.data.read_parquet("s3://path/to/parquet")\nray.data.read_json("...")\nray.data.read_csv("...")\nray.data.read_text("...")\n\n# Read tensor / image / file data.\nray.data.read_numpy("...")\nray.data.read_binary_files("...")\n\n# Create from in-memory objects.\nray.data.from_objects([list, of, python, objects])\nray.data.from_pandas([list, of, pandas, dfs])\nray.data.from_numpy([list, of, numpy, arrays])\nray.data.from_arrow([list, of, arrow, tables])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"seamless-data-framework-compatibility",children:"Seamless Data Framework Compatibility"}),"\n",(0,r.jsxs)(n.p,{children:["Beyond just storage I/O, ",(0,r.jsx)(n.strong,{children:"Ray Datasets"})," supports ",(0,r.jsx)(n.strong,{children:"bidirectional in-memory data exchange"})," with a wide range of popular data frameworks\u2014making it easy to integrate into your existing workflows."]}),"\n",(0,r.jsxs)(n.p,{children:["When frameworks like ",(0,r.jsx)(n.strong,{children:"Spark"}),", ",(0,r.jsx)(n.strong,{children:"Dask"}),", ",(0,r.jsx)(n.strong,{children:"Modin"}),", or ",(0,r.jsx)(n.strong,{children:"Mars"})," are run on Ray, Datasets can interact directly with them in memory, enabling efficient data sharing without unnecessary serialization or disk I/O. For smaller-scale, local data operations, Datasets also works smoothly with ",(0,r.jsx)(n.strong,{children:"Pandas"})," and ",(0,r.jsx)(n.strong,{children:"NumPy"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["To simplify data loading into machine learning models, Ray Datasets includes built-in adapters for both ",(0,r.jsx)(n.strong,{children:"PyTorch"})," and ",(0,r.jsx)(n.strong,{children:"TensorFlow"}),". These adapters allow you to convert a Ray Dataset into a framework-native structure\u2014such as ",(0,r.jsx)(n.code,{children:"torch.utils.data.IterableDataset"})," or ",(0,r.jsx)(n.code,{children:"tf.data.Dataset"}),"\u2014so you can plug them directly into your training loops with minimal effort."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Convert from existing DataFrames.\nray.data.from_spark(spark_df)\nray.data.from_dask(dask_df)\nray.data.from_modin(modin_df)\n\n# Convert to DataFrames and ML datasets.\ndataset.to_spark()\ndataset.to_dask()\ndataset.to_modin()\ndataset.to_torch()\ndataset.to_tf()\n\n# Convert to objects in the shared memory object store.\ndataset.to_numpy_refs()\ndataset.to_arrow_refs()\ndataset.to_pandas_refs()\n"})}),"\n",(0,r.jsx)(n.h1,{id:"deploying-ray-clusters-at-scale",children:"Deploying Ray Clusters at Scale"}),"\n",(0,r.jsx)(n.p,{children:"Ray offers native cluster deployment support on these technology stacks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["On ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index",children:"AWS and GCP"}),". Community-supported integrations exist for Azure, Aliyun, and vSphere, and natively on GCP using vertexai."]}),"\n",(0,r.jsxs)(n.li,{children:["On ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index",children:"Kubernetes"}),", through the officially supported KubeRay project."]}),"\n",(0,r.jsxs)(n.li,{children:["On ",(0,r.jsx)(n.a,{href:"https://www.anyscale.com/ray-on-anyscale?utm_source=ray_docs&utm_medium=docs&utm_campaign=ray-doc-upsell&utm_content=ray-cluster-deployment&__hstc=152921254.1a5e5e4ba1437e11b22cfd8a9df17049.1742234597786.1743123888642.1743186807718.5&__hssc=152921254.2.1743186807718&__hsfp=4074942027",children:"Anyscale"}),", a fully managed Ray platform by Ray's creators. You can use existing AWS, GCP, Azure and Kubernetes clusters, or utilize Anyscale's hosted compute layer."]}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"deploying-ray-clusters-on-vertex-ai",children:"Deploying Ray Clusters on Vertex AI"}),"\n",(0,r.jsx)(n.p,{children:"Vertex AI offers a flexible and scalable environment for running Ray, allowing you to harness the power of Ray\u2019s distributed computing within Google Cloud\u2019s managed ML platform. Whether you're running training, tuning, or serving workloads, deploying Ray on Vertex AI gives you full control over cluster lifecycle and resource usage."}),"\n",(0,r.jsxs)(n.p,{children:["Ray clusters on Vertex AI are designed to ",(0,r.jsx)(n.strong,{children:"stay active"})," to ensure consistent capacity for critical machine learning workloads or seasonal demand spikes. ",(0,r.jsx)(n.strong,{children:"Unlike custom jobs"}),", which automatically release resources after completion, ",(0,r.jsx)(n.strong,{children:"Ray clusters persist"})," until explicitly deleted."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use long-running Ray clusters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["You repeatedly submit the ",(0,r.jsx)(n.strong,{children:"same Ray job"})," and want to benefit from ",(0,r.jsx)(n.strong,{children:"data or image caching"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["You run ",(0,r.jsx)(n.strong,{children:"many short-lived jobs"})," where startup time outweighs job runtime\u2014making persistent clusters more efficient."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-ray-on-vertexai",children:"Setting up Ray on VertexAI"}),"\n",(0,r.jsx)(n.p,{children:"To run Ray clusters on Vertex AI, follow these quick setup steps:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["First, ",(0,r.jsx)(n.strong,{children:"enable the Vertex AI API in your Google Cloud project"}),":"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"gcloud services enable aiplatform.googleapis.com\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Install the Vertex AI SDK"})," for Python"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The SDK includes features such as the ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html",children:"Ray Client"}),", BigQuery integration, cluster management, and prediction APIs."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Via Google Cloud Console:"})}),"\n",(0,r.jsxs)(n.p,{children:["After ",(0,r.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster",children:"creating a Ray cluster"}),", the Vertex AI console provides access to a ",(0,r.jsx)(n.strong,{children:"Colab Enterprise notebook"})," that guides you through the SDK installation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Via Workbench or Custom Environment:"})}),"\n",(0,r.jsxs)(n.p,{children:["If you're using ",(0,r.jsx)(n.strong,{children:"Vertex AI Workbench"})," or any other Python environment, install the SDK manually:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install google-cloud-aiplatform[ray]\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"\ud83d\udd10 Networking & Security Tips for Ray on Vertex AI",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd10 ",(0,r.jsx)(n.strong,{children:"(Optional)"})," Enable ",(0,r.jsx)(n.strong,{children:"VPC Service Controls"})," to reduce the risk of data exfiltration. Be aware that this restricts access to resources outside the perimeter (e.g., public Cloud Storage buckets)."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udf10 ",(0,r.jsx)(n.strong,{children:"Use an auto mode VPC network"})," (one per project is recommended). Avoid custom mode or multiple VPC networks in the same project, as these may cause cluster creation to fail."]}),"\n"]})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create a Ray Cluster"}),". To perform this task, you can use either the Google Cloud Console or the Vertex AI SDK for Python. Ray clusters on Vertex AI support up to ",(0,r.jsx)(n.strong,{children:"2,000 total nodes"}),", with a maximum of ",(0,r.jsx)(n.strong,{children:"1,000 nodes per worker pool"}),". Although there is no limit to the number of worker pools, creating too many (e.g., 1,000 pools with a single node each) can lead to reduced performance. It's recommended to balance the number of worker pools and nodes per pool for optimal scalability and efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Via GCP Console"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["To create a Ray cluster on Vertex AI from the GCP console,\xa0",(0,r.jsx)(n.strong,{children:'navigate to the Ray on Vertex AI page in the Google Cloud console, click "Create Cluster," configure the cluster settings (name, region, machine types, etc.), and then click "Create"'})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"image.png",src:t(8885).A+"",width:"1076",height:"825"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 4: Ray Cluster on Vertex AI Menu"})})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"image.png",src:t(1534).A+"",width:"1076",height:"825"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 5: Ray Cluster on Vertex AI Creation Form"})})]}),"\n",(0,r.jsxs)(n.p,{children:["For detailed instructions and guidance on choosing the best configuration for your needs, follow the link: ",(0,r.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#console",children:"https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#console"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Via Python"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Alternatively, you can create, and manage your Ray clusters using Python with the following code snippets:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport logging\nimport ray\nfrom ray.job_submission import JobSubmissionClient, JobStatus\nfrom google.cloud import aiplatform as vertexai\nfrom google.oauth2 import service_account\n\nfrom vertex_ray import create_ray_cluster, update_ray_cluster, delete_ray_cluster, list_ray_clusters, get_ray_cluster\nfrom vertex_ray import Resources, AutoscalingSpec\n\n# -----------------------------\n# Configuration\n# -----------------------------\nPROJECT_NAME = "<your-project-name>"\nPROJECT_NUMBER = "<your-project-number>"\nREGION = "us-central1"\nCLUSTER_NAME = "ray-cluster"\nSERVICE_ACCOUNT_FILE = "<path-to-your-service-account-file>.json"\nRAY_VERSION = "2.33"\nPYTHON_VERSION = "3.10"\n\n# -----------------------------\n# Setup Logging\n# -----------------------------\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# -----------------------------\n# Authenticate Vertex AI\n# -----------------------------\ncredentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\nvertexai.init(credentials=credentials, location=REGION)\n\n# -----------------------------\n# Cluster Management\n# -----------------------------\ndef get_or_create_basic_ray_cluster():\n    """Create a default Ray cluster on Vertex AI."""\n\n    cluster_resource_name = f"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}"\n    cluster = get_ray_cluster(cluster_resource_name)\n    if cluster:\n        logger.info(f"Cluster {CLUSTER_NAME} already exists.")\n        return cluster.cluster_resource_name\n\n    logger.info(f"Creating cluster {CLUSTER_NAME}...")\n    head = Resources(machine_type="n1-standard-16", node_count=1)\n    workers = [Resources(\n        machine_type="n1-standard-8",\n        node_count=2,\n        accelerator_type="NVIDIA_TESLA_T4",\n        accelerator_count=1\n    )]\n    cluster = create_ray_cluster(\n        head_node_type=head,\n        worker_node_types=workers,\n        cluster_name=CLUSTER_NAME,\n        ray_version=RAY_VERSION,\n        python_version=PYTHON_VERSION\n    )\n    return cluster\n\ndef create_autoscaling_ray_cluster():\n    """Create a Ray cluster with autoscaling on Vertex AI."""\n\n    cluster_resource_name = f"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}"\n    cluster = get_ray_cluster(cluster_resource_name)\n    if cluster:\n        logger.info(f"Cluster {CLUSTER_NAME} already exists.")\n        return cluster.cluster_resource_name\n\n    logger.info(f"Creating cluster {CLUSTER_NAME}...")\n    autoscaling = AutoscalingSpec(min_replica_count=1, max_replica_count=3)\n    head = Resources(machine_type="n1-standard-16", node_count=1)\n    workers = [Resources(\n        machine_type="n1-standard-16",\n        accelerator_type="NVIDIA_TESLA_T4",\n        accelerator_count=1,\n        autoscaling_spec=autoscaling\n    )]\n    cluster = create_ray_cluster(\n        head_node_type=head,\n        worker_node_types=workers,\n        cluster_name=CLUSTER_NAME,\n        ray_version=RAY_VERSION,\n        python_version=PYTHON_VERSION\n    )\n    return cluster\n\ndef scale_ray_cluster(cluster_resource_name: str, new_replica_count: int):\n    """Update worker replica count of an existing Ray cluster."""\n    cluster = get_ray_cluster(cluster_resource_name)\n    for worker in cluster.worker_node_types:\n        worker.node_count = new_replica_count\n    updated = update_ray_cluster(\n        cluster_resource_name=cluster.cluster_resource_name,\n        worker_node_types=cluster.worker_node_types\n    )\n    return updated\n\ndef delete_cluster(cluster_resource_name: str):\n    """Delete the specified Ray cluster."""\n    logger.info(f"Deleting Ray cluster: {cluster_resource_name}")\n    delete_ray_cluster(cluster_resource_name)\n\ndef list_clusters():\n    """List all Ray clusters on Vertex AI."""\n    clusters = list_ray_clusters()\n    for cluster in clusters:\n        logger.info(cluster)\n'})}),"\n",(0,r.jsx)(n.p,{children:"After deploying a Ray cluster, you can start running your Ray applications! As shown in the next figure, you can either use the Ray Jobs API or run the job interactively."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"image.png",src:t(9847).A+"",width:"1080",height:"354"}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Figure 6: Way to run Ray jobs"})})]}),"\n",(0,r.jsxs)(n.h3,{id:"1-ray-jobs-api-recommended",children:["1. ",(0,r.jsx)(n.strong,{children:"Ray Jobs API (Recommended)"})]}),"\n",(0,r.jsx)(n.p,{children:"Use the CLI, Python SDK, or REST API to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Submit jobs with an entrypoint like ",(0,r.jsx)(n.code,{children:"python script.py"})]}),"\n",(0,r.jsx)(n.li,{children:"Define a runtime environment (e.g., dependencies)"}),"\n",(0,r.jsx)(n.li,{children:"Run jobs remotely and independently of the client connection"}),"\n",(0,r.jsx)(n.li,{children:"View job status, logs, and manage runs"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Jobs are tied to the cluster\u2019s lifetime \u2014 if the cluster stops, so do all jobs."}),"\n",(0,r.jsxs)(n.h3,{id:"2-interactive-mode",children:["2. ",(0,r.jsx)(n.strong,{children:"Interactive Mode"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["SSH into the cluster node and run scripts directly (via ",(0,r.jsx)(n.code,{children:"ray attach"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"Use Ray Client (for advanced users) to connect live from your machine"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Note: Jobs started this way aren't tracked by the Ray Jobs API."}),"\n",(0,r.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html",children:"Full Guide"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# -----------------------------\n# Ray Job Submission\n# -----------------------------\ndef submit_ray_job(script_path: str):\n    """Submit a Ray job to a given cluster."""\n    cluster_resource_name = f"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}"\n    ray_cluster = get_ray_cluster(cluster_resource_name)\n    client = JobSubmissionClient(f"vertex_ray://{ray_cluster.dashboard_address}")\n\n    job_id = client.submit_job(\n        entrypoint=f"python3 {script_path}",\n        runtime_env={\n            "working_dir": ".",\n            "pip": [\n            f"ray=={RAY_VERSION}"\n        ],\n        },\n    )\n    while True:\n        status = client.get_job_status(job_id)\n        if status == JobStatus.SUCCEEDED:\n            logger.info("Job succeeded.")\n            break\n        elif status == JobStatus.FAILED:\n            logger.error("Job failed.")\n            break\n        else:\n            logger.info("Job is running...")\n            time.sleep(30)\n\n# -----------------------------\n# Direct Cluster Usage\n# -----------------------------\ndef run_remote_ray_job():\n    """Example Ray job executed on the cluster."""\n    @ray.remote(num_cpus=1)\n    def heavy_task(x):\n        return sum(i * i for i in range(x))\n\n    cluster_resource_name = f"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}"\n    ray.init(address=f"vertex_ray://{cluster_resource_name}", ignore_reinit_error=True)\n\t\t\n\t\t# fetch the computation results\n    results = [heavy_task.remote(1000000) for _ in range(1000)]\n    outputs = ray.get(results)\n    logger.info(f"Total result: {sum(outputs)}")\n\n    ray.shutdown()\n\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# -----------------------------\n# Main Entry Point\n# -----------------------------\ndef main():\n    cluster_resource_name = get_or_create_basic_ray_cluster()\n\n    if not cluster_resource_name:\n        raise RuntimeError("Ray cluster creation failed")\n\n    logger.info("Listing clusters...")\n    clusters = list_ray_clusters()\n    if not clusters:\n        raise RuntimeError("No Ray clusters found.")\n\n    latest_cluster = clusters[-1].cluster_resource_name\n    logger.info(f"Submitting job to cluster: {latest_cluster}")\n    submit_ray_job("ray_job.py")\n    run_remote_ray_job()\n\nif __name__ == "__main__":\n    main()\n\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"ray_job.py"})})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport ray\n\n#Initialize Ray\nray.init()\n\n# Define a computationally intensive task\n@ray.remote(num_cpus=1)\ndef heavy_task(x):\n    """\n    Simulates a heavy workload by performing a CPU-bound operation.\n    This example calculates the sum of squares for a range of numbers.\n    """\n    total = 0\n    for i in range(x):\n        total += i * i\n    time.sleep(1)  # Simulate some work duration\n    return total\n\n# Generate a large number of tasks\nnum_tasks = 1000\nresults = []\n# Initialize connection to the Ray cluster on Vertex AI.\nray.init(ignore_reinit_error=True) # local testing\nfor i in range(num_tasks):\n    results.append(heavy_task.remote(1000000))\n\n# Retrieve results (this will trigger autoscaling if needed)\noutputs = ray.get(results)\n# Print the sum of the results (optional)\nprint(f"Sum of results: {sum(outputs)}")\n# Terminate the process\nray.shutdown()\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In this post, we explored how ",(0,r.jsx)(n.strong,{children:"Ray"})," provides a powerful framework for scaling AI, data science, and Python applications across distributed infrastructure. We walked through the process of creating and managing Ray clusters using both the ",(0,r.jsx)(n.strong,{children:"Google Cloud Console"})," and the ",(0,r.jsx)(n.strong,{children:"Vertex AI Python SDK"}),", including how to configure ",(0,r.jsx)(n.strong,{children:"autoscaling"})," for dynamic resource management. The accompanying code examples showcased essential capabilities such as ",(0,r.jsx)(n.strong,{children:"cluster provisioning"}),", ",(0,r.jsx)(n.strong,{children:"job submission"}),", and efficiently executing ",(0,r.jsx)(n.strong,{children:"distributed workloads"})," across multiple nodes."]}),"\n",(0,r.jsx)(n.p,{children:"In upcoming posts, we\u2019ll dive deeper into:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimizing machine learning pipelines"})," with Ray"]}),"\n",(0,r.jsxs)(n.li,{children:["Implementing ",(0,r.jsx)(n.strong,{children:"distributed training"})," for deep learning models"]}),"\n",(0,r.jsxs)(n.li,{children:["Leveraging Ray\u2019s ",(0,r.jsx)(n.strong,{children:"advanced libraries"})," (such as Ray Train and Ray Tune) for scalable, production-ready AI"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Stay tuned\u2014and I hope this post has been a helpful introduction to getting started with Ray!"}),"\n",(0,r.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/",children:"Ray Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/data/dataset.html",children:"Ray Datasets API"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",children:"Ray on Vertex AI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"Ray on Kubernetes"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.anyscale.com/blog/ray-datasets-for-machine-learning-training-and-scoring",children:"Ray Datasets for large-scale machine learning ingest and scoring"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://maxpumperla.com/learning_ray",children:"Getting Started with Ray Core"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html",children:"Ray Client"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},9788:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure4-a1d000c6ebd60f6cdf58d01906c2bb4f.png"},9847:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure7-ee92b5425882017adfd46a0837da2392.png"},9955:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/figure3-6c9422a9cff478b7e0b04be495211dd1.png"}}]);