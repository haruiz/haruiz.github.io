"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[866],{4612:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"intro-to-system-design","metadata":{"permalink":"/blog/intro-to-system-design","source":"@site/blog/2024-03-18-intro-to-system-design/index.mdx","title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","description":"System design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.","date":"2024-03-18T00:00:00.000Z","formattedDate":"March 18, 2024","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":34.665,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz"}],"frontMatter":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","slug":"intro-to-system-design","description":"System design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"nextItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"}},"content":"\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nSystem design is the process of laying out a system\'s structure, components, modules, interfaces, and data to meet specified requirements. It is a multidisciplinary field that requires a wide range of skills and knowledge, including software engineering, computer science, network engineering, and project management. For machine learning engineers and data scientists, comprehending a system\'s life cycle provides a blueprint for building, deploying, and maintaining ML/AI solutions in production. This post will introduce and discuss some of the more critical stages of the system design process (including requirements analysis, architecture, development, deployment, and scaling). It will also introduce some technologies and tools that can be used to design, develop, and deploy systems, such as Docker, Docker Compose, Docker Swarm, and Kubernetes.\\n\\n\\n## System design process overview\\n\\n### 1. Defining the requirements of the system\\n\\nDefining the requirements of the system is the first step in the system design process, and it involves gathering, analyzing, and documenting the requirements for the system. The requirements analysis phase is crucial, as it provides the foundation for the rest of the system design process. It helps to ensure that the system will meet the needs of its users and stakeholders and that it will be developed within the constraints of time, budget, and resources. The requirements analysis phase typically involves the following activities:\\n\\n1. **Gathering Requirements**: This involves collecting information about the needs, goals, and constraints of the system from its users and stakeholders. This information can be gathered through interviews, surveys, questionnaires, and workshops.\\n\\n2. **Analyzing Requirements**: This involves analyzing the gathered information to identify the key features, functions, and constraints of the system. It also involves identifying any conflicts or inconsistencies in the requirements.\\n\\n3. **Documenting Requirements**: This involves documenting the requirements in a clear, concise, and unambiguous manner. The requirements should be documented in a way that is understandable to all stakeholders, including developers, testers, and project managers.\\n\\n4. **Validating Requirements**: This involves validating the requirements with the users and stakeholders to ensure that they accurately reflect their needs and goals. It also involves ensuring that the requirements are complete, consistent, and feasible.\\n\\n5. **Managing Requirements**: This involves managing changes to the requirements throughout the system design process. It involves tracking changes, resolving conflicts, and ensuring that the requirements are kept up-to-date.\\n\\n#### Types of Requirements\\n\\nThere are several types of requirements that need to be considered when designing a system. These include:\\n\\n1. **Functional Requirements**: These are the requirements that describe the functions, features, and capabilities of the system. They specify what the system should do, and they are typically expressed as use cases, user stories, or functional specifications.\\n\\n2. **Non-Functional Requirements**: These are the requirements that describe the quality attributes of the system, such as performance, reliability, availability, security, and usability. They specify how well the system should perform, and they are typically expressed as performance requirements, security requirements, and usability requirements.\\n\\n3. **Business Requirements**: These are the requirements that describe the business goals, objectives, and constraints of the system. They specify why the system is being developed, and they are typically expressed as business cases, business rules, and business process models.\\n\\n4. **User Requirements**: These are the requirements that describe the needs, goals, and constraints of the users of the system. They specify who will use the system, and they are typically expressed as user profiles, user scenarios, and user interface designs.\\n\\n5. **System Requirements**: These are the requirements that describe the technical constraints and dependencies of the system. They specify how the system will be developed, deployed, and maintained, and they are typically expressed as system architecture, system interfaces, and system dependencies.\\n\\n6. **Regulatory Requirements**: These are the requirements that describe the legal, ethical, and regulatory constraints of the system. They specify how the system should comply with laws, regulations, and standards, and they are typically expressed as compliance requirements, privacy requirements, and security requirements.\\n\\n7. **Data Requirements**: These are the requirements that describe the data needs, constraints, and dependencies of the system. They specify what data the system will use, store, and process, and they are typically expressed as data models, data flows, and data storage.\\n\\n### 2. Selecting the appropriate methodology\\n\\nLike any other software solution, ML systems require a well-structured methodology to maximize the success rate of the implementation. ML algorithms are the less challenging part. The hard part is making algorithms work with other software components to solve real-world problems.\\n\\nThere are several **software development methodologies** that can be used to develop a ML system, such as the waterfall model, the agile model, and the iterative model. Each of these methodologies has its own strengths and weaknesses, and they are suitable for different types of projects and teams. These methodologies also provided a framework for gathering, analyzing, and documenting the requirements the system.\\n\\n1. **Waterfall Model**: The waterfall model is a linear and sequential software development methodology that divides the development process into distinct phases, such as requirements analysis, design, implementation, testing, and maintenance. Each phase must be completed before the next phase can begin, and the process is difficult to change once it has started. The waterfall model is suitable for projects with well-defined requirements and stable technologies, but it is not suitable for projects with changing requirements and emerging technologies.\\n\\n2. **Agile Model**: The agile model is an iterative and incremental software development methodology that focuses on delivering working software in short iterations, typically two to four weeks. It emphasizes collaboration, flexibility, and customer feedback, and it is suitable for projects with changing requirements and emerging technologies. The agile model is based on the principles of the Agile Manifesto, which emphasizes individuals and interactions, working software, customer collaboration, and responding to change.\\n\\n3. **Iterative Model**: The iterative model is a software development methodology that divides the development process into small, incremental, and iterative cycles, each of which produces a working prototype of the system. The iterative model is suitable for projects with evolving requirements and complex technologies, and it is based on the principles of the spiral model, which emphasizes risk management, prototyping, and incremental development.\\n\\n#### Adaptations for ML and Data Science Projects\\n\\n**Considerations**\\n\\nWhile these methodologies offer frameworks for managing work, ML projects may require specific adaptations:\\n\\n1. **Iterative Experimentation:** Embrace the iterative nature of ML, where initial models often serve as baselines for further experimentation and refinement.\\n\\n2. **Flexible Planning:** Allow for adjustments in project scope and direction based on intermediate results and discoveries.\\n\\n3. **Model Versioning and Experiment Tracking:** Implement tools and practices for tracking different model versions, experiment parameters, and results to ensure reproducibility and facilitate decision-making.\\n\\n4. **Collaboration between Data Scientists and Domain Experts:** Foster close collaboration to ensure that models are developed with a deep understanding of the domain and are aligned with business needs.\\n\\n**Adaptations**\\n\\n1. **Scrum:** Scrum is a popular Agile framework that organizes work into small, manageable pieces delivered in short cycles called sprints, typically lasting 2-4 weeks. For ML projects, Scrum can facilitate rapid experimentation and iteration. Teams can define sprints for different phases of ML development, such as data preparation, model training, evaluation, and deployment. Daily stand-ups can help track progress and address blockers quickly.\\n\\n2. **Kanban:** Kanban emphasizes continuous delivery without overburdening the team, using a visual board to track work items through various stages of completion. In ML projects, Kanban can be used to manage the flow of tasks like data annotation, feature engineering, model experimentation, and performance tuning. Its flexibility is particularly useful for projects where priorities shift frequently based on experimental results or business needs.\\n\\n3. **Extreme Programming (XP):** XP focuses on technical excellence and high customer involvement, with practices like pair programming, test-driven development (TDD), and frequent releases. While some XP practices may not directly translate to ML projects (e.g., TDD is challenging due to the probabilistic nature of ML outcomes), the emphasis on quality and collaboration can be beneficial. For instance, pair programming can be adapted for collaborative model development and code reviews, ensuring high-quality code and model architecture.\\n\\n4. **Lean Development:** Lean development aims to reduce waste and focus on delivering value. For ML projects, this can mean focusing on high-value tasks, such as feature engineering, model experimentation, and deployment, while minimizing time spent on less critical activities. Techniques like value stream mapping can help identify bottlenecks and streamline the ML workflow.\\n\\n5. **Feature-Driven Development (FDD):** FDD is an iterative and incremental approach that focuses on building features in short iterations. For ML projects, this can translate to developing specific features or components of the ML pipeline in short cycles, ensuring that each iteration delivers tangible value.\\n\\nOnce we have gathered, analyzed, and documented the requirements for the system, we can move on to the next phase of the system design process, which is architecturing the system.\\n\\n### 3. Architecturing your solution\\n\\nArchitecturing a system is the act of decomposing a system into multiple building blocks so that we can identify how each building block can be developed, deployed, and maintained to achieve a high level of modularity, flexibility, and scalability. The architecture of a system provides a high-level view of how these components are arranged and interact with each other to achieve the desired functionality and performance. Several architectural styles and patterns can be used to design a system, including the client-server architecture, the microservices architecture, and the event-driven architecture. However, selecting the appropriate architectural style depends on the type of application to be developed and the system\'s requirements.\\n\\nClassifying applications involves categorizing them based on criteria such as functionality, deployment methods, technology used, target user base, and platform. Here, we are providing a classification based on the deployment model.\\n\\n- **Web Applications:** Accessed via web browsers, e.g., Google Docs, Salesforce.\\n- **Desktop Applications:** Installed on a personal computer or laptop, e.g., Microsoft Word, Adobe Photoshop.\\n- **Mobile Applications:** Designed for smartphones and tablets, e.g., Instagram, Uber.\\n- **Cloud Applications:** Hosted on cloud services and accessible over the Internet, e.g., Dropbox, Slack.\\n\\nAn ML model can be deployed in any of these application types. For example, a web application can use machine learning to provide personalized recommendations to users; a desktop application can use a machine learning model to automate repetitive tasks; and a mobile application can use a machine learning model to recognize speech or images, etc.\\n\\n#### Client-\\"Server\\" Architecture\\n\\nFor web applications, cloud applications, and sometimes mobile applications, the **client-server architecture** is a common choice for the system architecture. The client-server architecture is a distributed computing architecture that divides the system into two major components: the client and the server. The client is the end-user device or application that requests and consumes the services provided by the server. The server is the remote computer or service that provides the resources, services, or data to the client. The client-server architecture provides a scalable, flexible, and reliable way to distribute and manage resources and services across a network. This architecture consists of the following components:\\n\\n1. **Client**: The client is a device or a program that requests resources and services from the server. The client can be a web browser, a mobile app, or a desktop application.\\n\\n2. **Server**: The server is a device or a program that provides resources and services to the client. The server can be a web server, an application server, or a database server.\\n\\n3. **Network**: The network is the medium through which the client and server communicate with each other. The network can be a local area network (LAN), a wide area network (WAN), or the internet.\\n\\n4. **Protocol**: The protocol is a set of rules and conventions that govern the communication between the client and server. The protocol can be HTTP, HTTPS, TCP, or UDP.\\n\\nIn software development, the component of your app running in the client side is called the **frontend** and the component running in the server side is called the **backend**. The frontend is responsible for the user interface and user experience, while the backend is responsible for the business logic, data storage, and integration with external systems. The frontend and backend communicate with each other using APIs, such as RESTful APIs, GraphQL APIs, or WebSocket APIs. I will discuss more about APIs and Webservices in a future post.\\n\\n![client-server-architecture](client-server.png)\\n\\nClient-Server-based applications can be deployed in different ways, following different architectural patterns. The most common ones are:\\n\\n#### Server-Based architecture patterns\\n\\n#### Monolithic Architecture\\n\\nThe monolithic architecture is a traditional software architecture pattern that consists of a single, self-contained application that contains all the components, modules, and services of the system. The monolithic architecture is based on the principles of tight coupling, where the components of the system are tightly integrated and dependent on each other. The monolithic architecture is suitable for small to medium-sized applications with simple requirements and low complexity. It is also suitable for applications with stable technologies and well-defined requirements. The monolithic architecture consists of the following components:\\n\\n1. **User Interface**: The user interface is the front-end component of the system that interacts with the user. It can be a web interface, a mobile interface, or a desktop interface.\\n2. **Business Logic**: The business logic is the core component of the system that implements the business rules, processes, and workflows. It can be implemented as a set of classes, functions, or procedures.\\n3. **Data Storage**: The data storage is the back-end component of the system that stores and manages the data. It can be a relational database, a NoSQL database, or a file system.\\n4. **Integration**: The integration is the component of the system that integrates with external systems, services, and APIs. It can be implemented as a set of connectors, adapters, or gateways.\\n\\n#### Microservices Architecture\\n\\nThe microservices architecture is a modern software architecture that consists of a collection of small, independent, and loosely-coupled services that are developed, deployed, and maintained independently. The microservices architecture is based on the principles of loose coupling, where the components of the system are decoupled and independent of each other. The microservices architecture is suitable for large-scale applications with complex requirements and high complexity. It is also suitable for applications with changing requirements and emerging technologies. The microservices architecture consists of the following components:\\n\\n1. **Service**: The service is a small, independent, and loosely-coupled component of the system that provides a specific set of functions and capabilities. It can be implemented as a RESTful API, a message queue, or a microservice.\\n2. **Container**: The container is a lightweight, portable, and self-contained environment that hosts the service. It can be implemented as a Docker container, a Kubernetes pod, or a serverless function.\\n3. **Orchestration**: The orchestration is the component of the system that manages the deployment, scaling, and monitoring of the services. It can be implemented as a container orchestrator, a service mesh, or a serverless platform.\\n4. **Gateway**: The gateway is the component of the system that provides a single entry point for the services. It can be implemented as an API gateway, a message broker, or a load balancer\\n5. **Database**: The database add to each service the capability to store and manage the data. It can be a relational database, a NoSQL database, or a distributed database.\\n\\nThese are the most common architectural patterns for server-based applications. Each pattern has its own strengths and weaknesses, and the choice of pattern depends on the requirements and constraints of the system. However, with the rise of cloud computing and serverless computing, the **serverless architecture** has become an alternative to the develop and deploy applications.\\n\\n#### Serverless-Based Architecture patterns\\n\\n The serverless architecture is a cloud computing model that abstracts the infrastructure and runtime environment from the developer, allowing them to focus on writing code and deploying applications without managing servers. The serverless architecture is based on the principles of event-driven computing, where events, such as HTTP requests, database changes, or file uploads, trigger the execution of code. The serverless architecture consists of the following components:\\n\\n1. **Function**: The function is a small, stateless, and event-driven piece of code that performs a specific task or function. It can be implemented as a serverless function, a lambda function, or a cloud function.\\n2. **Event**: The event is a trigger that initiates the execution of the function. It can be an HTTP request, a database change, or a file upload.\\n3. **Cloud**: The cloud is the infrastructure and runtime environment that hosts and manages the functions. It can be a cloud provider, such as AWS, Azure, or GCP.\\n4. **API**: The API is the interface that exposes the functions to the client. It can be a RESTful API, a GraphQL API, or a message queue.\\n\\n#### Server-based vs Serverless-based architectures\\n\\n- **Monolithic architectures** are best suited for small to medium-sized applications where simplicity and ease of management are key. However, they can become cumbersome to update and scale as the application grows.\\n- **Microservices** offer a highly scalable and flexible architecture that is suitable for complex applications that need to rapidly evolve. They require a significant upfront investment in design and infrastructure management but provide long-term benefits in scalability and maintainability.\\n- **Serverless architectures** abstract the management of servers, making it easier for developers to deploy code that scales automatically with demand. This model is cost-effective for applications with fluctuating workloads but introduces new challenges in managing application state and understanding cloud provider limitations.\\n\\n\\n![Architectural-styles](architectural-styles.svg)\\n\\n| Factor | Monolithic | Microservices | Serverless |\\n| :---: | :---: | :---: | :---: |\\n| Definition | A software development <br/> approach where an <br/> application is built as a <br/> single and indivisible unit. | An architecture that <br/> structures an application <br/> as a collection of small, <br/> autonomous services <br/> modeled around a <br/> business domain. | A cloud-computing <br/> execution model where the <br/> cloud provider dynamically <br/> manages the allocation and <br/> provisioning of servers. |\\n| Complexity | Simple to develop and <br/> deploy initially but <br/> becomes more complex <br/> and unwieldy as the <br/> application grows. | Complex to design and <br/> implement due to its <br/> distributed nature, but <br/> easier to manage, <br/> understand, and update <br/> in the long term. | Low operational complexity <br/> for developers as the cloud <br/> provider manages the <br/> infrastructure, but can have <br/> complex architecture <br/> patterns. |\\n| Scalability | Scaling requires <br/> duplicating the entire <br/> application, which can be <br/> inefficient for resources. | Services can be scaled <br/> independently based on <br/> demand, leading to <br/> efficient use of resources. | Automatically scales based <br/> on the workload by running <br/> code in response to events, <br/> without provisioning or <br/> managing servers. |\\n| Development | Development is <br/> straightforward in the <br/> early phases but can slow <br/> down as the application <br/> grows due to tightly <br/> coupled components. | Enables the use of <br/> different technologies <br/> and programming <br/> languages for different <br/> services, potentially <br/> increasing development <br/> speed. | Development focuses on <br/> individual functions, <br/> potentially speeding up <br/> development cycles, but <br/> requires understanding of <br/> serverless patterns and <br/> limits. |\\n| Deployment | Deploying updates <br/> requires redeploying the <br/> entire application, which <br/> can be slow and risky. | Services can be deployed <br/> independently, allowing <br/> for faster and less risky <br/> updates. | Code is deployed to the <br/> cloud provider, which then <br/> takes care of deployment, <br/> scaling, and management, <br/> simplifying deployment <br/> processes. |\\n| Maintenance | Maintenance can be <br/> challenging as fixing a <br/> bug or making an update <br/> requires redeploying the <br/> entire application. | Easier to maintain and <br/> update individual <br/> services without <br/> impacting the entire <br/> application. | Maintenance of the <br/> infrastructure is handled by <br/> the cloud provider, but <br/> developers must manage <br/> their code\'s scalability and <br/> performance within the <br/> serverless environment. |\\n| Cost | Costs can be predictable <br/> but may not efficiently <br/> utilize resources due to <br/> the need to scale the <br/> entire application. | Potentially more cost- <br/> efficient as resources are <br/> used more effectively by <br/> scaling services <br/> independently. | Cost-effective for <br/> applications with variable <br/> traffic but can become <br/> expensive if not managed <br/> properly, due to the pay-per- <br/> use pricing model. |\\n| Use Cases | Suitable for small <br/> applications or projects <br/> where simplicity and ease <br/> of deployment are <br/> prioritized. | Ideal for large, complex <br/> applications requiring <br/> scalability, flexibility, and <br/> rapid iteration. <br/> darr | Best for event-driven <br/> scenarios, sporadic <br/> workloads, and rapid <br/> development cycles, where <br/> managing infrastructure is <br/> not desirable. |\\n\\nIt is important to be aware that these are just some architectural patterns that can be used to design a system. There are many other patterns and styles that can be used to design your system architecture, such as event-driven architecture, service-oriented architecture, and peer-to-peer architecture. As this article attempts to introduce the topic, only some of the most common architectures were mentioned.\\n\\n### 3.Developing and building your application\\n\\nThe process of developing a system involves the selection of the appropriate technologies, tools, and frameworks to implement the system. It also consists of designing and developing the system components, such as the user interface, business logic, and data storage. The development process should be straightforward after clearly defining the systems\' requirements.\\n\\n### 4. Testing your application\\n\\nTesting is an essential part of the development process, as it helps to ensure that the system meets its requirements and performs as expected before going into production. Testing involves verifying and validating the system\'s functionality, performance, reliability, and security. It also involves identifying and fixing defects, bugs, and issues in the system. There are several types of testing that can be used to test a system, including:\\n\\n- **Unit Testing**: This involves testing individual components, modules, or functions of the system to ensure that they work as expected. It is typically performed by developers using testing frameworks, such as JUnit, NUnit, or Mocha.\\n- **Integration Testing**: This involves testing the interactions and dependencies between the components, modules, or services of the system to ensure that they work together as expected. It is typically performed by developers using testing frameworks, such as TestNG, Cucumber, or Postman.\\n- **System Testing**: This involves testing the system as a whole to ensure that it meets its requirements and performs as expected. It is typically performed by testers using testing tools, such as Selenium, JMeter, or SoapUI.\\n- **Performance Testing**: This involves testing the performance and scalability of the system to ensure that it can handle the expected load and stress. It is typically performed by testers using performance testing tools, such as Apache JMeter, LoadRunner, or Gatling.\\n- **Security Testing**: This involves testing the security and reliability of the system to ensure that it is protected from unauthorized access and malicious attacks. It is typically performed by security experts using security testing tools, such as OWASP ZAP, Burp Suite, or Nessus.\\n- **User Acceptance Testing**: This involves testing the system with real users to ensure that it meets their needs and expectations. It is typically performed by users using acceptance testing tools, such as UserTesting, UsabilityHub, or UserZoom.\\n- **Stress Testing**: This involves testing the system under extreme conditions to ensure that it can handle the maximum load and stress. It is typically performed by testers using stress testing tools, such as Apache JMeter, LoadRunner, or Gatling.\\n\\n### 5. Deploying your App to production\\n\\nDeploying a system can be a complex and time-consuming process, and it requires careful planning and coordination to minimize the risk of downtime and data loss. Several deployment strategies and strategies exist. However, in today\'s world, Docker is generally the most suitable alternative to simplify this task. Docker is a platform for developing, shipping, and running applications using containerization. Containers are lightweight, portable, and self-contained environments that can run on any machine with the Docker runtime installed. They provide a consistent and reliable way to package and deploy applications, and they are widely compatible with cloud computing environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Docker provides several benefits for deploying systems, including:\\n\\n- **Portability**: Containers can run on any machine with the Docker runtime installed, making them highly portable and compatible with different environments.\\n- **Consistency**: Containers provide a consistent and reliable way to package and deploy applications, ensuring that they run the same way in development, testing, and production environments.\\n- **Isolation**: Containers provide a high level of isolation between applications, ensuring that they do not interfere with each other and that they are secure and reliable.\\n- **Scalability**: Containers can be easily scaled up or down to meet the demands of the system, making them highly scalable and flexible.\\n- **Efficiency**: Containers are lightweight and efficient, requiring minimal resources and providing fast startup times and high performance.\\n- **Security**: Containers provide a high level of security, ensuring that applications are protected from unauthorized access and malicious attacks.\\n- **Automation**: Containers can be easily automated using tools and platforms, such as Kubernetes, Docker Swarm, and Amazon ECS, making them easy to manage and maintain.\\n- **Cost-Effectiveness**: Containers are cost-effective, requiring minimal resources and providing high performance, making them ideal for cloud computing environments.\\n- **Flexibility**: Containers are flexible, allowing developers to use different technologies, tools, and frameworks to develop and deploy applications.\\n- **Reliability**: Containers are reliable, ensuring that applications run consistently and predictably in different environments.\\n- **Compatibility**: Containers are compatible with different operating systems, such as Linux, Windows, and macOS, making them highly versatile and widely used.\\n\\n![Deployment modes](deployment-modes.png)\\n\\n#### Docker\\nDocker was introduced to the world by Solomon Hykes in 2013, founder and CEO of a company called dotCloud. It provides a platform for building, shipping, and running distributed applications. Docker introduced the concept of \u201ccontainers\u201d to package software into isolated environments that can run on any system with the Docker engine installed. This deployment model makes it easy to run the same application in different environments, such as development, testing, and production, without worrying about dependencies, configurations, or compatibility issues.\\n\\n##### Docker components\\n\\nDocker consists of several components that work together to provide its functionality. These components include the Docker engine, the Docker client, and the Docker registry.\\n\\n- **Docker Engine**: The Docker engine is the core component of Docker that provides the runtime environment for containers. It consists of the Docker daemon, which is responsible for building, running, and distributing containers, and the Docker runtime, which is responsible for executing the processes of containers. The Docker engine can run on any system with the Docker runtime installed, including Linux, Windows, and macOS, and it can be managed and monitored using tools and platforms, such as Docker Swarm, Kubernetes, and Amazon ECS.\\n\\n- **Docker Client**: The Docker client is a command-line interface (CLI) that allows developers to interact with the Docker engine, providing a simple and intuitive way to build, run, and manage containers. The Docker client can also be used with graphical user interfaces (GUIs) and integrated development environments (IDEs), providing a seamless and consistent experience for developers.\\n\\n- **Docker Registry**: The Docker registry is a repository for storing and distributing containers, allowing developers to build, push, and pull containers from Docker registries, such as Docker Hub, Amazon ECR, and Google Container Registry. The Docker registry provides a high level of visibility and control over the distribution of containers, ensuring that they are secure, reliable, and efficient.\\n\\n##### Docker Hub\\n\\nDocker also provides a centralized repository called the Docker Hub, where developers can store and share their containers with others. This makes it easy to find and reuse existing containers and to collaborate with other developers on projects. Docker Hub provides a wide range of official and community-contributed containers, including base images, application images, and service images. It also provides features for managing and monitoring containers, such as versioning, tagging, and scanning. Docker Hub is widely used by developers, organizations, and cloud providers, and it provides a high level of visibility and control over the distribution of containers. We can use Docker Hub to store and share our containers so our team members can easily access and use them.\\n\\n![Docker Hub](docker-ecosystem.png)\\n\\n\\n##### Transforming my application into a container image?\\n\\nEverything start by creating a `Dockerfile` that contains the instructions to build a Docker image. The Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. The Docker image is built using the `docker build` command, which reads the `Dockerfile` and executes the instructions to create the image. The Docker image is then stored in a registry, such as Docker Hub, Amazon ECR, or Google Container Registry, where it can be shared and distributed with others. The Docker image can be run as a container using the `docker run` command, which creates an instance of the image and runs it as a container. The Docker container is a running instance of the image that can be managed and monitored using the Docker engine. The Docker container can be stopped, started, paused, and deleted using the `docker stop`, `docker start`, `docker pause`, and `docker rm` commands, respectively. The Docker container can also be managed and monitored using tools and platforms, such as Docker Compose, Docker Swarm, and Kubernetes.\\n\\n**Dockerfile example**\\n```bash\\n# Use an official Python runtime as a parent image\\nFROM python:3.8-slim\\n\\n# Set the working directory in the container\\nWORKDIR /app\\n\\n# Copy the current directory contents into the container at /app\\nCOPY . /app\\n\\n# Install any needed packages specified in requirements.txt\\nRUN pip install --trusted-host pypi.python.org -r requirements.txt\\n\\n# Make port 80 available to the world outside this container\\nEXPOSE 80\\n\\n# Define environment variable\\nENV NAME World\\n\\n# Run app.py when the container launches\\nCMD [\\"python\\", \\"app.py\\"]\\n```\\n\\n##### How does Docker work?\\n\\nUnder the hood, Docker uses a client-server architecture, where the Docker client communicates with the Docker daemon, which is responsible for building, running, and distributing containers. The Docker client and daemon can run on **the same system** or on **different systems**, and they communicate with each other using a **REST API** over a Unix socket or a network interface. As it was mentioned, the Docker daemon is responsible for managing the containers, images, volumes, networks, and other resources of the system. However, it also provides a high-level API for interacting with the Docker engine, allowing developers to build, run, and manage containers using simple commands and scripts.\\nThe Docker daemon is also responsible for managing the lifecycle of containers, including creating, starting, stopping, pausing, and deleting containers, as well as managing their resources, such as CPU, memory, and storage.\\n\\n![Docker](docker.png)\\n\\n##### The underlying technology\\n\\nDocker is written in the `Go programming language` and takes advantage of several features of the Linux kernel to deliver its functionality. The Linux kernel provides the core features and capabilities of the Docker engine, such as process isolation, resource management, and networking.\\n\\n**Linux kernel features** that Docker relies on include:\\n\\n- **Cgroups (control groups)** provide the ability to limit and prioritize the resources of containers, such as CPU, memory, and storage.\\n- **Namespaces** provide the ability to isolate and control the processes, users, and network of containers, ensuring that they do not interfere with each other.\\n- **Union file systems** provide the ability to create and manage the file systems of containers, allowing them to share and reuse the same files and directories.\\n\\n:::tip\\n\\nThe Linux kernel is the main component of the Linux operating system (OS). It\'s a computer program that acts as the interface between a computer\'s hardware and its processes. The kernel manages resources as efficiently as possible and enables communication between applications and hardware.\\n\\n:::\\n\\nSo the question you probably have, how can Docker run containers on Windows and MacOS if Docker relies on the Linux kernel?\\n\\n**On Windows** you can run Docker containers using the following approaches:\\n\\n**Windows Subsystem for Linux (WSL) 2:** With the introduction of WSL 2, Docker can run Linux containers natively on Windows. WSL 2 provides a full Linux kernel built into Windows, allowing Docker to interface directly with the kernel without the need for a virtual machine (VM). This approach is efficient and integrates well with Windows environments.\\n\\n**Docker Desktop for Windows:** Before WSL 2, Docker Desktop for Windows used a lightweight VM to host a Linux kernel. This VM then runs the Docker Engine and, by extension, Docker containers.\\n\\n**On macOS**, docker also utilizes a lightweight virtual machine to run a Linux kernel. Docker Desktop for Mac leverages macOS\'s native virtualization frameworks (such as Hypervisor.framework for Intel processors and the Virtualization framework for Apple silicon) to run this VM efficiently. This setup allows Docker containers to run in a Linux-like environment on Mac, with Docker Desktop handling the complexities of managing the VM.\\n\\n![Docker  on Linux and Windows](docker-linux-win.png)\\n\\n**Docker was designed to run Linux-based docker containers**, as it was developed on top of some of the Linux kernel features. However, Microsoft offers four container-based Windows images from which users can build. Each base image is a different type of the Windows or Windows Server operating system, has a different on-disk footprint, and has a different set of the Windows API set.All Windows container base images are discoverable through Docker Hub. The Windows container base images themselves are served from mcr.microsoft.com, the Microsoft Container Registry (MCR). This is why the pull commands for the Windows container base images look like the following:\\n\\n```bash\\ndocker pull mcr.microsoft.com/windows/servercore:ltsc20229\\n```\\n\\nFor more information, you can check the [official documentation](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-base-images).\\n\\n\\n**Most common Docker commands**\\n\\n```bash\\n# Pull an image from Docker Hub\\ndocker pull <image-name>\\n\\n# List all images on the system\\ndocker images\\n\\n# Run a container from an image\\ndocker run <image-name>\\n\\n# List all running containers\\ndocker ps\\n\\n# List all containers\\ndocker ps -a\\n\\n# Stop a running container\\ndocker stop <container-id>\\n\\n# Start a stopped container\\ndocker start <container-id>\\n\\n# Remove a container\\ndocker rm <container-id>\\n\\n# Remove an image\\ndocker rmi <image-name>\\n```\\n\\n\\n##### Running Multi-Container Applications\\n\\nOne of the challenges of running multi-container applications is managing the dependencies and interactions between them. Docker Compose, Swarm and  Kubernetes they are all tools that can be used to define, configure, and run multi-container applications. Cloud providers, such as AWS, Azure, and GCP, also provide managed services for running multi-container applications, such as Amazon ECS, Azure Container Instances, and Google Cloud Run. In the next video you can see 3 alternatives to run multi-container applications on GCP.\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/jh0fPT-AWwM\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n\\nNow, lets dive into the introduction of Docker Compose, Docker Swarm and Kubernetes.\\n\\n- **Docker Compose** is a tool for defining and running multi-container applications on a single host. It is easy to use and requires minimal setup, making it a popular choice for developers who want to quickly set up and test their applications locally. Docker Compose provides a simple and convenient way to define, configure, and run multi-container applications, but it is limited to a single host and does not provide the same level of scalability and resource management as Kubernetes.\\n\\n```yaml\\nversion: \'3\'\\nservices:\\n    frontend:\\n        image: frontend:latest\\n        ports:\\n        - \\"80:80\\"\\n        depends_on:\\n        - backend\\n    backend:\\n        image: backend:latest\\n        ports:\\n        - \\"8080:8080\\"\\n        environment:\\n        - DATABASE_URL=postgres://user:password@db:5432/db\\n    db:\\n        image: postgres:latest\\n        environment:\\n        - POSTGRES_USER=user\\n        - POSTGRES_PASSWORD=password\\n        - POSTGRES_DB=db\\nnetworks:\\n    default:\\n      external:\\n        name: my-network\\n```\\n\\n- **Kubernetes**, on the other hand, is a production-ready platform for deploying, scaling, and managing containerized applications. It provides a powerful and flexible architecture for managing multi-container applications at scale, and it includes features for automatic scaling, rolling updates, self-healing, and resource management. Kubernetes is designed for large, complex, and mission-critical applications, and it provides a high degree of availability and resilience. If this technology catch your attention, and you want to learn more about it, I recommend you to check the next video. It provides a great overview about how kubernetes comes to life into the world of multi-container applications.\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/BE77h7dmoQU\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n- **Docker Swarm** is a native clustering and orchestration tool for Docker. It allows you to create and manage a cluster of Docker hosts, and it provides features for scaling, load balancing, and service discovery. Docker Swarm is easy to use and integrates well with Docker, making it a good choice for developers who want to manage multi-container applications without the complexity of Kubernetes.\\n\\n\\n### 6. Scaling your App to meet the demands of its users\\n\\nWhen deploying a system, it is essential to consider how the system will scale to meet the demands of its users. Scalability describes the ability of a system to handle an increasing amount of work without compromising its performance, reliability, and availability. It is also related to system elasticity, which is the ability of a system to adapt to changes in the workload by adding (scaling up) or removing (scaling out) resources. Systems can be scaled in two ways: `vertically` and `horizontally`.\\n\\n#### Horizontal Scaling (Scaling Out/In)\\n\\nHorizontal scaling involves adding more machines or nodes to a pool of resources to manage increased load. It\'s like adding more lanes to a highway to accommodate more traffic. This approach is common in distributed systems, such as cloud computing environments, where you can add more instances or servers to handle more requests.\\n\\n**Advantages:**\\n\\n- **Scalability:** It\'s easier to scale applications indefinitely by simply adding more machines into the existing infrastructure.\\n\\n- **Flexibility:** You can scale the system up or down by adding or removing resources as demand changes, often automatically.\\n\\n- **Fault Tolerance:** Horizontal scaling can improve the reliability and availability of a system. If one node fails, others can take over, reducing the risk of system downtime.\\n\\n**Disadvantages:**\\n\\n- **Complexity:** Managing a distributed system with many nodes can be more complex, requiring sophisticated software and tools for load balancing, distributed data management, and failover mechanisms.\\n\\n- **Data Consistency:** Ensuring data consistency across nodes can be challenging, especially in databases or systems requiring real-time synchronization.\\n\\n#### Vertical Scaling (Scaling Up/Down)\\n\\nVertical scaling involves increasing the capacity of an existing machine or node by adding more resources to it, such as CPU, RAM, or storage. It\'s akin to upgrading the engine in a car to achieve higher performance.\\n\\n**Advantages:**\\n\\n- **Simplicity:** It is often simpler to implement as it may require just upgrading existing hardware. It doesn\'t involve the complexity of managing multiple nodes.\\n\\n- **Immediate Performance Boost:** Upgrading hardware can provide an immediate improvement in performance for applications that can utilize the extra resources.\\n\\n**Disadvantages:**\\n\\n- **Limited Scalability:** There is a physical limit to how much you can upgrade a single machine, and eventually, you might hit the maximum capacity of what a single server can handle.\\n\\n- **Downtime:** Upgrading hardware might require downtime, which can be a significant drawback for systems that require high availability.\\n\\n- **Cost:** Beyond certain points, vertical scaling can become prohibitively expensive as high-end hardware components can cost significantly more.\\n\\nThe choice between horizontal and vertical scaling depends on the specific requirements, architecture, and constraints of the system in question. Horizontal scaling is favored for applications designed for cloud environments and those requiring high availability and scalability. Vertical scaling might be chosen for applications with less demand for scalability or where simplicity and immediate performance improvement are prioritized. Often, a hybrid approach is used, combining both strategies to leverage the advantages of each.\\n\\nScaling your system is something you can control,and plan ahead with the support of your infrastructure team. However, doing this manually is rather time-consuming, especially when the increased load only sustains for a short period of time. In other words, you\u2019re always too late. This is where autoscaling comes in, by automatically scaling either horizontally or vertically when the current incoming load requires it.\\n\\n#### Autoscaling\\n\\nAuto-scaling, or automatic scaling, is a technique that dynamically adjusts the amount of computational resources in a server farm or a cloud environment based on the current demand. It is closely related to both horizontal and vertical scaling, but it primarily leverages horizontal scaling due to its flexibility and the ease with which resources can be added or removed in cloud-based environments. I encorage you to check the next video to understand how Kubernetes relies on autoscaling to manage the resources of your system in cloud or on-premises environments.\\n\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/XpeAITE4uqA\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### 7. Monitoring and Logging\\n\\nMonitoring and logging are essential for understanding the behavior and performance of a system in a production environment. It involves collecting and analyzing data about the system\'s performance, availability, and reliability, while logging involves recording and storing data about the system\'s activities, events, and errors. Monitoring and logging are crucial for identifying and diagnosing issues, optimizing performance, and ensuring the system meets its service level objectives (SLOs) and service level agreements (SLAs). There are several tools and platforms that can be used for monitoring and logging. The selection of the appropriate tools and platforms depends on the requirements and constraints of the system.\\n\\nThis is the end of the introduction to system design for data scientists and ML engineers. I hope you have enjoyed it and learned something new. If you have any questions, feel free to ask in the comments section."},{"id":"python-for-data-science-exploring-the-syntax","metadata":{"permalink":"/blog/python-for-data-science-exploring-the-syntax","source":"@site/blog/2022-10-26-python-for-data-science-exploring-the-syntax/index.mdx","title":"Python for Data Science Series - Exploring the syntax","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","date":"2022-10-26T00:00:00.000Z","formattedDate":"October 26, 2022","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":19.475,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz"}],"frontMatter":{"title":"Python for Data Science Series - Exploring the syntax","slug":"python-for-data-science-exploring-the-syntax","image":"https://haruiz.github.io/img/2022-10-26-python-for-data-science-exploring-the-syntax-og-image.jpg","description":"In the last post, we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python and create a simple program that uses Google Cloud Vision API to detect faces in an image.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"FullStack AI Series - Intro to System Design for Data Scientists and ML Engineers","permalink":"/blog/intro-to-system-design"},"nextItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"\x3c!--truncate--\x3e\\n\\n### Introduction\\n\\nIn the [last post](python-for-data-science-part-getting-started) we discussed the importance of programming in the data science context and why Python is considered one of the top languages used by data scientists. In this week\'s post, we will explore the syntax of Python by creating a simple program that uses Google Cloud Vision API to detect faces in an image.\\n\\nYou will learn today:\\n- What is a computer program?\\n- How to write a program in Python?\\n  - Python syntax\\n  - How to organize your code in python using functions\\n- What is a REST API, and how to use it?\\n- How to use Google Cloud Vision API to detect faces in an image?\\n\\nSo lets started!!!\\n\\n# What is a computer program?\\n\\nA computer program is a sequence of instructions we write using a programming language to tell the computer what to do for us. This sequence of instructions contains but is not limited to:\\nShow information to the user, ask the user for input, save and recover data in memory/disk, and perform calculations. So, programming languages provide a set of built-in functions and instructions that can be used to accomplish these tasks.\\n\\n:::tip\\nIf we think about programming languages, we can compare them to different idioms we use to communicate with others. We choose the appropriate language based on the application or where our program will run.\\n:::\\n\\n# How to write a program in Python?\\n\\nWhen we write programs independent of the programming language we decide to use, writing down our algorithm in simple words is always helpful. In a way, we can have a mental model of what our program will be doing and how it will be executed. To do so, we can use Pseudocode, a simplified version of computer programs written in natural or human-readable language that can be easily interpreted. You can check this [**cheat sheet**](https://cheatography.com/lcheong/cheat-sheets/pseudocode/) that will help you to write your program in Pseudocode.\\n\\n:::info Algorithm\\nFinite set of rules to be followed in calculations or other problem-solving operations, especially by a computer.\\n:::\\n\\nSo, let\'s define our pseudocode for our face detection program:\\n\\n```pseudocode showLineNumbers\\nVAR image_path as STRING = INPUT(\\"Please provide the path of the image: \\")\\nIF image_path is empty or image_path dont exist THEN\\n    PRINT(\\"image path could not be empty or the image does not exist\\")\\n    EXIT\\nENDIF\\n\\nFUNCTION read_image(image_path) as STRING\\n    VAR image_bytes as BYTES = read(image_path)\\n    RETURN image_bytes\\nENDFUNCTION\\n\\nFUNCTION detect_faces_on_image(image_bytes as BYTES) as LIST:\\n    api_response as LIST = call_face_detection_gcp_api(image_bytes)\\n    IF api_response is empty THEN\\n        PRINT(\\"No faces found\\")\\n        RETURN\\n    faces as LIST = []\\n    FOR json_entity in api_response THEN\\n        face as DICT = {\\n            \\"confidence\\": json_entity.confidence,\\n            \\"bounding_box\\": json_entity.bounding_box,\\n            \\"is_happy\\" : json_entity.joy_likelihood == \\"VERY_LIKELY\\"\\n        }\\n        faces.append(face)\\n    ENDFOR\\n    RETURN faces\\nENDFUNCTION\\n\\nimage_bytes = read_image(image_path)\\nfaces_list = detect_faces(image_bytes)\\ndisplay_detect_faces(faces_list)\\n```\\n\\nAs you can see in Pseudocode, we can skip the implementation details of our program. We write down our algorithm using a high-level language, so in this way, we have a big picture of the tasks we need to perform that we can use later to translate our algorithm into a programming language. In line 13, for instance,  we need to call the Google Cloud Vision API to detect the faces in the image, but we have yet to determine how it will be implemented.\\n\\n## Python syntax\\n\\nTo learn about python syntax, we will navigate through the Pseudocode and convert it into a python script.\\n\\n### Variables\\n\\nProgramming is all about data and manipulating it to solve problems. So, we need to have a way to store data on our computer that we can access later during execution. To do so, we use variables. Variables are a way to store data in memory where we can save almost any data. In Python, it is straightforward to define a variable; we need to use the assignment operator `=` followed by the value we want to store, and the Python interpreter will take care of the rest. Under the hood, it will allocate memory for the variable and store its value. We can save strings, integers, floats, booleans, lists, dictionaries, and other data types. Let\'s see an example:\\n\\n```python\\n# String\\nmy_string = \\"Hello World\\"\\n# Integer\\nmy_integer = 1\\n# Float\\nmy_float = 1.0\\n# Boolean\\nmy_boolean = True\\n# List\\nmy_list = [1, 2, 3]\\n# Dictionary\\nmy_dict = {\\"key\\": \\"value\\"}\\n```\\n\\nWe can obtain the memory address and type of a variable using the `id()` and `type()` functions respectively.\\n\\n```python\\nmy_string = \\"Hello World\\"\\nmy_string_2 = \\"Hello World\\"\\nprint(id(my_string))\\nprint(id(my_string_2))\\nprint(type(my_string))\\nprint(type(my_string_2))\\n```\\n\\nIn our program, in line 1, we define a variable `image_path` and assign it the value of the user input. In Python, the `input()` function allows us to grab information from the user so we can save the value into a variable. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimage_path = input(\\"Please provide the path of the image:\\")\\n```\\n\\nThe syntax is very similar to the Pseudocode. However, you can notice that in Python, we don\'t specify the variable type. That is because Python is a dynamically typed language, meaning that the variable type is inferred during the execution. In terms of productivity, this is very convenient because we don\'t need to worry about specifying the type of the variables when we define them. However, it can sometimes be a source of errors if we are not carefully doing operations.\\n\\nPython will raise an error if we try to perform an operation that is not supported by the type of the variable. Let\'s see an example:\\n\\n```python\\na = 2   \\nb = \\"2\\"\\n# error-line-next\\nprint(a + b)\\n# TypeError: unsupported operand type(s) for +: \'int\' and \'str\'\\n```\\n\\n\\n:::warning Rules for creating variables across languages\\n- A variable name must start with a letter or an underscore character.\\n- A variable name cannot start with a number.\\n- A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ ).\\n- Variable names are case-sensitive (name, Name and NAME are three different variables).\\n- The reserved words(keywords) cannot be used naming the variable.\\n:::\\n\\n### Conditional blocks\\n\\nA common task in programming is to execute a block of code only if a condition is met. In Python, we can use the `if` statement to do so. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\n```\\n\\nIn the example above, we check if the variable `a` is equal to 2. If that is the case, we print the message \\"a is equal to 2\\". We can also use the `else` statement to execute a block of code if the condition is not met. Let\'s see an example:\\n\\n```python\\na = 2\\nif a == 2:\\n    print(\\"a is equal to 2\\")\\nelse:\\n    print(\\"a is not equal to 2\\")\\n```\\n\\nIn our program, we need to check if the user input is empty or if the image path does not exist. We can use the `if` statement to do so. Let\'s see how we can translate this Pseudocode line into Python:\\n\\n```python\\nimport os\\n\\nimage_path = input(\\"Please provide the path of the image:\\")\\nif image_path == \\"\\" or not os.path.exists(image_path):\\n    print(\\"image path could not be empty or the image does not exist\\")\\n    exit()\\n```\\n\\nAgain, we can observe that the syntax is very similar to the Pseudocode, just with the addition of the `os.path.exists()` function in the condition to check whether a image exists or not. The os module it is included in the Python standard library, and it provides a way to interact with the operating system. We also use the `exit()` function to exit the program in case the condition is met. We are going to discuss about modules later in this article.\\n\\n:::info Python Standard Library\\nThe Python Standard Library is a set of modules that comes with the Python installation. It provides a wide range of built-in functions and classes that we can use in our programs for different purposes. You can find more information about the Python Standard Library [here](https://docs.python.org/3/library/index.html).\\n\\nSome of the most used modules are:\\n- **os:** provides a way to interact with the operating system.\\n- **sys:** provides a way to interact with the Python interpreter.\\n- **json:** provides a way to work with JSON data.\\n- **re:** provides a way to work with regular expressions.\\n- **math:** provides a way to work with mathematical operations.\\n- **random:** provides a way to work with random numbers.\\n- **datetime:** provides a way to work with dates and times.\\n- **urllib:** provides a way to work with URLs.It is a very useful module to work with APIs. We are going to use it in the next section to call the Google Cloud Vision API. \\n:::\\n\\n### Functions\\n\\nFunctions are a way to encapsulate a block of code that we can reuse in our program. In Python, we can define a function using the `def` keyword followed by the function name and the parameters. Let\'s see an example:\\n\\n```python\\ndef add(a, b):\\n    \\"\\"\\"\\n    This function adds two numbers\\n    :param a: first number\\n    :param b: second number\\n    :return: sum of the two numbers\\n    \\"\\"\\"\\n    return a + b\\n\\nif __name__ == \\"__main__\\":\\n    print(add(1, 2))\\n```\\n\\nThe paremeters are the variables that we need to pass to the function to perform the task. In the example above, we define a function called `add` that takes two parameters `a` and `b`, and the function returns the sum of the values of the two parameters. We can call the function by using the function name followed by the parameters. In the example above, we call the function `add` with the parameters `1` and `2`. The function returns the value `3` and we print it in the console.\\n\\nIn our program, we have two main functions that we need to implement, `read_image` and `call_face_detection_gcp_api.` The first takes the image path as a parameter and returns the image data. The second takes the image data as a parameter, requests the Google Cloud Vision API to detect faces in the image, and returns the face annotations in JSON format. Let\'s see how we can translate the `read_image`  function from Pseudocode into Python:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n```\\n\\nThere is a new syntax here in the `read_image` function to be discussed. \\n\\n- **Function annotations:** Although not mandatory, we can specify the parameters\' type and the functions\' return value in Python. These are called function annotations. Although the Python interpreter does not enforce them, and we still have to check the type of the parameters programmatically,  annotations are extremely useful for other project contributors to navigate through the code and understand how the function must be called. In the example above, we specify that the function takes a string as a parameter and returns a bytes object.\\n\\nAnother good thing about function annotations is that It makes the function more readable and will also helps to avoid errors when function is called in other parts of the program.\\n\\n- **Context Managers:** It can also be noticed that we use the `with` statement to open the file in the `read_image` function. These blocks of code are called context managers in Python, and in this case, it ensures that the file is closed after the block of code is executed. We will discuss context managers later in other articles since this is an advanced topic. For more information about context managers, you can check the [Python documentation](https://docs.python.org/3/reference/compound_stmts.html#with).\\n\\n- **Encoding:** We can also see that we use the `rb` mode to open the file. This mode allows us to read the file as bytes so we can encode it in base64 to send it to the Google Cloud Vision API. That is required because the API only accepts images encoded in this format. For more information about the `rb` mode, you can check the [Python documentation](https://docs.python.org/3/library/functions.html#open), and face detection API documentation [here](https://cloud.google.com/vision/docs/detecting-faces).\\n\\n:::info Encoding data\\nEncodings are a way to represent a sequence of bytes in a different format. The most common encodings are ASCII, UTF-8, and base64. ASCII is a 7-bit encoding that represents the first 128 characters of Unicode. UTF-8 is a variable-length encoding that represents the first 1,112,064 characters of Unicode. Base64 is a way to represent binary data in ASCII characters and it is used to send binary data in text-based protocols such as HTTP. For more information about encodings, you can check the [Python documentation](https://docs.python.org/3/library/codecs.html#standard-encodings).\\n:::\\n\\n- **Error handling:** In order to catch the errors in our python functions we can use the `try` and `except` block. The `try` statement allows us to execute a block of code and catch the errors that can happen in the `except` statement. A `finally` block can also be used to execute a block of code after the `try` and `except` blocks.\\nLet\'s see how to do this:\\n\\n```python\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    try:\\n        # read and load the image into memory\\n        with open(image_path, \\"rb\\") as f:\\n            return f.read() # read the image\'s bytes\\n    except Exception as e:\\n        raise Exception(\\"Error reading the image: \\", e)\\n    finally:\\n        print(\\"finally block\\")\\n```\\n\\n### Modules\\n\\nModules are a way to group a set of functions and classes in our programs. In Python, we can import a module using the `import` keyword followed by the module name.\\n\\n```python\\nimport os # import the os module\\nif __name__ == \\"__main__\\":\\n    print(os.path.exists(\\"image.jpg\\"))\\n```\\n\\nIn the example above, we import the `os` module and use the `os.path.exists()` function to check if the file `image.jpg` exists. We can also import a specific function from a module using the `from` keyword. Let\'s see an example:\\n\\n```python\\nfrom os import path\\nif __name__ == \\"__main__\\":\\n    print(path.exists(\\"image.jpg\\"))\\n```\\n\\nFollowing with our example, to implement the `call_face_detection_gcp_api` function, we need to import the `urllib` module. This module provides a set of function we can use to call the Google Cloud Vision API. Let\'s see how to do this:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string so it can be sent to the API\\n    :param image_bytes:\\n    :return: base64 string\\n    \\"\\"\\"\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call GCP Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: response the face annotations as JSON\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    image_base64 = image_to_base64(image_bytes)\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # Create request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n```\\n\\nFor more information about using the `urlib` module, you can check the [Python documentation](https://docs.python.org/3/library/urllib.request.html).\\n\\n:::tip What is an REST API?\\nREST stands for Representational State Transfer. It is an architectural style for designing networked applications, that allows to expose data and functionality to external clients in public(wan) or private(lan) networks. Clients could be web applications, mobile applications, or even other services. For more information about REST APIs, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Representational_state_transfer).REST APIs are implemented using HTTP methods. The most common methods are `GET`, `POST`, `PUT`, `PATCH`, and `DELETE`, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods). It also provides standard data formats to send and receive data, for instance `JSON` and `XML`. More information [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages).\\n:::\\n\\nThe video below give you a quick overview of how REST APIs work:\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/7YcW25PHnAA\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Loops\\n\\nLoops are a way to execute a block of code multiple times. In Python, we can use the `for` loop to iterate over a list of elements. Let\'s see an example:\\n\\n```python\\nfor i in range(10):\\n    print(i)\\n```\\n\\nThe `range` function returns a list of numbers from 0 to 10. The `for` loop iterates over the list and prints each element. The `range` function can also receive a start and end value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10):\\n    print(i)\\n```\\n\\nThe `range` function can also receive a step value. Let\'s see an example:\\n\\n```python\\nfor i in range(5, 10, 2):\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a list of elements. Let\'s see an example:\\n\\n```python   \\nfor i in [1, 2, 3, 4, 5]:\\n    print(i)\\n```\\n\\nThe `for` loop can also be used to iterate over a dictionary. Let\'s see an example:\\n\\n```python\\nfor key, value in {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3}.items():\\n    print(f\\"key: {key}, value: {value}\\")\\n```\\n\\nThe `for` loop can also be used to iterate over a string. Let\'s see an example:\\n\\n```python\\nfor char in \\"Hello World\\":\\n    print(char)\\n```\\n\\nThe `while` loop is used to execute a block of code while a condition is true. Let\'s see an example:\\n\\n```python\\ni = 0\\nwhile i < 10:\\n    print(i)\\n    i += 1\\n```\\n\\nIn our code in the last function of our script, we need to iterate over the list of faces returned by the API. Let\'s see how we do this in line `18`\\n\\n```python showLineNumbers {18-23}\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return: list of faces found\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n```\\n\\nI skipped the `call_face_detection_gcp_api` function explanation since it was supposed to be an introductory tutorial. However, I have tried my best to comment on the code so you can develop an intuition on what the function does. To get more information about how to call the `GCP face detection API,` you can check the official documentation [here](https://cloud.google.com/vision/docs/detecting-faces). You must create a Google Cloud Platform account to use the API. To see how to create the project and get the API key, you can check the [official documentation](https://cloud.google.com/vision/docs/libraries#client-libraries-install-python).\\n\\nIn the next section, we will see how to do more advance things with Python using third party packages and libraries. For now I will leave you with the full code of the script:\\n\\n```python showLineNumbers\\nimport base64\\nimport urllib.error\\nimport urllib.parse\\nimport urllib.request\\nimport json\\nimport os\\n\\n\\ndef read_image(image_path: str) -> bytes:\\n    \\"\\"\\"\\n    Read image from file and return as bytes\\n    :param image_path: path of the image\\n    :return: image as bytes\\n    \\"\\"\\"\\n    # read and load the image into memory\\n    with open(image_path, \\"rb\\") as f:\\n        return f.read() # read the image\'s bytes\\n\\n\\ndef image_to_base64(image_bytes: bytes) -> str:\\n    \\"\\"\\"\\n    Convert image to base64 string\\n    :param image_bytes:\\n    :return:\\n    \\"\\"\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    return base64.b64encode(image_bytes).decode(\\"utf-8\\")\\n\\n\\ndef call_face_detection_gcp_api(image_bytes: bytes, API_KEY: str = None) -> dict:\\n    \\"\\"\\"\\n    Call Google Cloud Platform Face Detection API\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    api_url = f\\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\\"\\n    # Convert image to base64 string, so it can be sent to the API\\n    image_base64 = image_to_base64(image_bytes)\\n    # Create request body\\n    request_body = {\\n        \\"requests\\": [\\n            {\\n                \\"image\\": {\\n                    \\"content\\": image_base64\\n                },\\n                \\"features\\": [\\n                    {\\n                        \\"type\\": \\"FACE_DETECTION\\",\\n                        \\"maxResults\\": 10\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n    # Convert request body to JSON format\\n    request_body = json.dumps(request_body).encode(\\"utf-8\\")\\n    # make request\\n    request = urllib.request.Request(api_url, data=request_body)\\n    # Set request header\\n    request.add_header(\\"Content-Type\\", \\"application/json\\")\\n    try:\\n        # Send request\\n        response = urllib.request.urlopen(request)\\n        # Read response body as bytes\\n        response_body_bytes = response.read()\\n        # # Convert response body to JSON format\\n        response_body_text = response_body_bytes.decode(\\"utf-8\\")\\n        # Convert response body to JSON format\\n        response_body_json = json.loads(response_body_text)\\n        # Convert response to JSON format\\n        return response_body_json[\\"responses\\"][0][\\"faceAnnotations\\"]\\n\\n    except urllib.error.HTTPError as e:\\n        # Get error message\\n        error_message = json.loads(e.read())[\\"error\\"][\\"message\\"]\\n        error_code = e.code\\n        if e.code == 400:\\n            error_status = \\"Bad Request\\"\\n        elif e.code == 401:\\n            error_status = \\"Unauthorized\\"\\n        elif e.code == 403:\\n            error_status = \\"Forbidden\\"\\n        elif e.code == 404:\\n            error_status = \\"Not Found\\"\\n        elif e.code == 500:\\n            error_status = \\"Internal Server Error\\"\\n        elif e.code == 503:\\n            error_status = \\"Service Unavailable\\"\\n        else:\\n            error_status = \\"Unknown Error\\"\\n        raise Exception(f\\"Error {error_code} calling the GCP Face Detection API: {error_status} - {error_message}\\")\\n\\n\\ndef detect_faces_on_image(image_bytes: bytes, API_KEY: str = None) -> list:\\n    \\"\\"\\"\\n    Detect faces on image\\n    :param API_KEY: API Key for Google Cloud Platform\\n    :param image_bytes: image as bytes\\n    :return:\\n    \\"\\"\\"\\n    # Call Google Cloud Platform Face Detection API\\n    api_response = call_face_detection_gcp_api(image_bytes, API_KEY)\\n\\n    # Check if API response is empty\\n    if not api_response:\\n        print(\\"No faces found\\")\\n        return []  # return empty list\\n\\n    # Create list to store faces\\n    faces = []\\n    for json_entity in api_response:\\n        face = {\\n            \\"bounding_box\\": json_entity[\\"boundingPoly\\"],\\n            \\"is_happy\\": json_entity[\\"joyLikelihood\\"] in [\\"VERY_LIKELY\\", \\"LIKELY\\"],\\n        }\\n        faces.append(face)\\n    return faces\\n\\n\\ndef main():\\n    try:\\n        image_path = input(\\"Please provide the path of the image:\\")\\n        assert image_path != \\"\\" and os.path.exists(image_path), \\"image path could not be empty or the image does not exist\\"\\n        # read and return image as bytes\\n        image = read_image(image_path)\\n        # pass the image to the face detection function to detect faces\\n        faces = detect_faces_on_image(image, API_KEY=\\"<GCP API KEY>\\")\\n        # print the number of faces found\\n        print(\\"number of faces found:\\", len(faces))\\n        # iterate over the faces and do something\\n        for face in faces:\\n            print(face[\\"is_happy\\"])\\n    except Exception as e:\\n        print(f\\"Error running the script: {e}\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\nThis is all for this tutorial. I hope you enjoyed it. If you have any questions, please leave a comment below or contact me on LinkedIn. If you want to see more tutorials like this, please subscribe to my newsletter (See top menu). To access the code for this tutorial, you can check the [GitHub repository](https://github.com/haruiz/blog-code/blob/main/python-for-data-science-exploring-the-syntax/main.py)\\n\\n\\n## Useful Links\\n- [GCP Face Detection API](https://cloud.google.com/vision/docs/detecting-faces)\\n- [GCP Machine Learning APIs](https://cloud.google.com/products/ai)\\n- [Python Cheat Sheet](https://www.pythoncheatsheet.org/)\\n- [Python Documentation](https://docs.python.org/3/)\\n- [Python Tutorial](https://docs.python.org/3/tutorial/index.html)\\n- [Python Standard Library](https://docs.python.org/3/library/index.html)\\n- [Python Package Index](https://pypi.org/)\\n- [Python for Data Science](https://www.python.org/about/apps/)\\n- [What is a REST API?](https://www.youtube.com/watch?v=lsMQRaeKNDk&ab_channel=IBMTechnology)\\n- [What is JSON?](https://www.youtube.com/watch?v=iiADhChRriM&ab_channel=IBMTechnology)"},{"id":"python-for-data-science-part-getting-started","metadata":{"permalink":"/blog/python-for-data-science-part-getting-started","source":"@site/blog/2022-08-27-python-for-data-science-getting-started/index.mdx","title":"Python for Data Science Series - Getting started","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","date":"2022-08-27T00:00:00.000Z","formattedDate":"August 27, 2022","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":6.265,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz"}],"frontMatter":{"title":"Python for Data Science Series - Getting started","slug":"python-for-data-science-part-getting-started","image":"https://haruiz.github.io/img/2022-08-27-python-for-data-science-getting-started-og-image.jpg","description":"Thinking about jumping into a data science role, but you don\'t know why you should learn how to program and which programming language to choose? In this post, I will show you how to use python and discuss why this programming language is considered one of the top used in data science.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Exploring the syntax","permalink":"/blog/python-for-data-science-exploring-the-syntax"},"nextItem":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","permalink":"/blog/python-environments-with-pyenv-and-poetry"}},"content":"\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\nimport VideoPlayer from \\"@site/src/components/VideoPlayer\\";\\n\\n## Introduction\\n\\nProgramming is an essential skill for data scientists. If you are considering starting a data science career, the sooner you learn how to code, the better it will be. Most data sciences jobs rely on programming to automate cleaning and organizing data sets, design databases, fine-tune machine learning algorithms, etc. Therefore, having some experience in programming Languages such as Python, R, and SQL makes your life easier and will allow you to automate your analysis pipelines.\\n\\nIn this week\'s post, we will focus on Python. A general-purpose programming language that allows us to work with data and explore different algorithms and techniques that would be extremely useful to add to our analysis toolbox.\\n\\n### Why should I learn how to program?\\n\\n\\nTo help organizations make better decisions,  a data scientist is a technical expert who uses mathematical and statistical techniques to manipulate, analyze and extract patterns from raw/noisy data to produce information. Those tools include but are not limited to statistical inference, pattern recognition, machine learning, deep learning, etc. \\n\\nData Scientist\'s responsibilities involve:\\n\\n- Work closely with business stakeholders to understand their goals and determine how data can be used to achieve them.  \\n- Fetching information from various sources and analyzing it to get a clear understanding of how an organization performs\\n- Undertaking data collection, preprocessing, and analysis\\n- Building models to address business problems\\n-  Presenting information in a way that your audience can understand using different data visualization techniques\\n\\nAlthough programming is not required to be a data scientist, taking advantage of the power of computers, most of these tasks can be automated. So, programming skills provide data scientists with the superpowers to manipulate, process, and analyze big datasets, automate and develop computational algorithms to produce results (faster and more effectively), and create neat visualizations to present the data more intuitively.\\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/dU1xS07N-FA\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n## Programming languages for data science\\n\\nThere are hundreds of programming languages out there, built for diverse purposes. Some are better suited for web or mobile development, others for data analysis, etc. Choosing the correct language to use will depend on your level of experience, role, and/or project goals. In the last few years, Python has been ranked as one of the top programming languages data scientists use to manipulate, process, and analyze big datasets.\\n\\nBut why is Python so popular? Well, I will list some reasons why data scientists love Python and what makes this language suitable for high productivity and performance in processing large amounts of data.\\n\\n### Why Python?\\n\\n- Python is **open source**, so is freely available to everyone.You can even use it to develop commercial applications.\\n- Python is **Multi-Platform**. It can be run on any platform, including Windows, Mac, Linux, and Raspberry Pi.\\n- Python is a **Multi-paradigm** language, which means it can be used for both object-oriented and functional programming. It comes from you writing code in a way that is easy to read and understand.\\n- Python is **Multi-purpose**, so you can use it to develop almost any kind of application. You can use it to develop web applications, game development, data analysis, machine learning, and much more.\\n- Python syntax is **easy to read** and **easy to write**. So the learning curve is low in comparison to other languages.\\n- Data Science **packages ecosystem**: Python also has [PyPI package index,a python package repository](https://pypi.org/), where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages.Some of the most used libraries for data science and machine learning are:\\n\\n  - [Tensorflow](https://www.tensorflow.org/), a high-performance numerical programming library for deep learning.\\n  - [Pandas](https://pandas.pydata.org/), a Python library for data analysis and manipulation.\\n  - [NumPy](https://www.numpy.org/), a Python library for scientific computing ( that offers an extensive collection of advanced mathematical functions, including linear algebra, Fourier transforms, random number generation, etc.)\\n  - [Matplotlib](https://matplotlib.org/), a Python library for plotting graphs and charts.\\n  - [Scikit-learn](https://scikit-learn.org/stable/index.html), a Python library for machine learning.\\n  - [Seaborn](https://seaborn.pydata.org/), a Python library for statistical data visualization.\\n\\n- **High performance:** Although some people complain about performance in Python (see [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)), mainly caused by some features such as dynamic typing, it is also simple to extend developing modules in other compiled languages like C++ or C which could [speed up your code by 100x.](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)\\n  \\nThe following section will introduce you to the Python programming language, and we will start learning its syntax.\\n\\n## Hands-on Tutorial\\n\\n:::tip\\nTo set up our python environment, we will use `pyenv` and `poetry.` You can learn more about these tools in the previous post.\\n[Python environments with pyenv and poetry](/blog/python-environments-with-pyenv-and-poetry)\\n:::\\n\\nWe will start with a simple program that prints \\"Hello World\\" on the screen, and from there, we will begin navigating into the python syntax, learning some of its keywords and essential building blocks. Start creating a folder called \\"python_demo\\" and a file called \\"hello_world.py.\\" To do so, run the following commands in the terminal:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'cd workspace\' , comment: \\"moving into the workspace directory. It could be any folder in your machine where you want to have your python_demo folder\\"},\\n{ type: \'input\', value: \'mkdir python_demo\' , comment: \\"creating the python_demo folder inside the workspace folder\\" },\\n{ type: \'input\', value: \'cd python_demo\' , comment: \\"going into the python_demo folder\\" },\\n{ type: \'input\', value: \'touch hello_world.py\' , comment: \\"creating the hello_world.py file inside the python_demo folder, in windows use the command type\\" },\\n{ type: \'input\', value: \\"pyenv version\\", comment: \\"checking the python version being used by pyenv to create the Python environment\\"},\\n{ type: \\"output\\", value: \\"python 3.10.0\\"},\\n{ type: \\"input\\", value: \\"poetry init\\", comment: \\"initialize poetry project into the python_demo directory\\"},\\n{ type: \\"input\\", value: \\"poetry install\\", comment: \\"create python environment within the folder\\"}\\n]} />\\n\\nIf all the command runs successfully, you should see the following folder structure:\\n```bash\\n\u251c\u2500\u2500 python_demo\\n\u2502   \u251c\u2500\u2500 hello_world.py\\n\u2502   \u251c\u2500\u2500 poetry.lock\\n\u2502   \u2514\u2500\u2500 pyproject.toml\\n```\\nAnd the `pyproject.toml` file should look like this:\\n```toml\\n[tool.poetry]\\nname = \\"python_demo\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [`Henry Ruiz  <henryruiz22@gmail.com>`]\\n\\n[tool.poetry.dependencies]\\npython = \\"^3.10\\"\\n\\n[tool.poetry.dev-dependencies]\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\nYou can see that the Python version was set to 3.10.0. that will depend on the version of Python you are using with pyenv.\\n\\n:::tip\\nTo check the python version run the command `pyenv version` in the terminal.\\n:::\\n\\nTo open our python_demo folder in pycharm check the animation below.\\n\\n<VideoPlayer videoUrl={require(\\"./open-folder-pycharm.mp4\\").default} />\\n\\nAt this point, you should know how to create and run python files. So, in the coming tutorials, we will be working on the hello_world.py file, exploring the python syntax, and learning cool things about Python and data science.\\n\\nThanks for reading!, and I hope this tutorial helped you to get started with Python.\\n\\n\\n**Some useful resources**\\n\\n- [Python Tutorial](https://docs.python.org/3/tutorial/)\\n- [Python Language Reference](https://docs.python.org/3/reference/)\\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\\n- [Why Coding is important in Data Science](https://www.dqindia.com/coding-important-data-science/)\\n- [Python for Data Science](https://www.geeksforgeeks.org/python-for-data-science/)\\n- [Top programming languages for data scientists in 2022](https://www.datacamp.com/blog/top-programming-languages-for-data-scientists-in-2022)\\n- [Why Python is so slow and how to speed it up](https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e)\\n- [Write Your Own C-extension to Speed Up Python by 100x](https://towardsdatascience.com/write-your-own-c-extension-to-speed-up-python-x100-626bb9d166e7)"},{"id":"python-environments-with-pyenv-and-poetry","metadata":{"permalink":"/blog/python-environments-with-pyenv-and-poetry","source":"@site/blog/2022-08-07-Python-environments-with-pyenv-and-poetry/index.mdx","title":"Python for Data Science Series - Python environments with pyenv and poetry","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","date":"2022-08-07T00:00:00.000Z","formattedDate":"August 7, 2022","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"data-science","permalink":"/blog/tags/data-science"}],"readingTime":18.045,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz"}],"frontMatter":{"title":"Python for Data Science Series - Python environments with pyenv and poetry","slug":"python-environments-with-pyenv-and-poetry","hide_table_of_contents":false,"image":"https://haruiz.github.io/img/2022-08-07-Python-environments-with-pyenv-and-poetry-og_image.png","description":"If you have been using Python for a while or just started, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! So, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.","authors":["haruiz"],"tags":["python","data-science"]},"unlisted":false,"prevItem":{"title":"Python for Data Science Series - Getting started","permalink":"/blog/python-for-data-science-part-getting-started"}},"content":"\x3c!--truncate--\x3e\\n\\nimport TermynalReact from \\"@site/src/components/Termynal\\";\\n\\nimport TOCInline from \'@theme/TOCInline\';\\n\\nimport Image from \'@theme/IdealImage\';\\n\\nimport Tabs from \'@theme/Tabs\';\\n\\nimport TabItem from \'@theme/TabItem\';\\n\\n[//]: # ()\\n[//]: # (:::tip In this post you will learn)\\n\\n[//]: # ()\\n[//]: # (<TOCInline toc={toc} />)\\n\\n[//]: # ()\\n[//]: # (:::)\\n\\n## Introduction\\n\\nPython, a versatile programming language widely embraced in fields such as web development, data science, machine learning, and scientific computing. However, navigating through different Python installations and dependencies can often become overwhelming. On this post we will explore how tools like pyenv and poetry can simplify this process by effectively managing project dependencies. Let\'s embark on this journey of optimizing code environments together!\\n\\n### Why Python?\\n\\nAccording to the [**2022 stack overflow developer survey**](https://survey.stackoverflow.co/2022/#technology-most-loved-dreaded-and-wanted), Python is one of the most widely used programming languages today. Of 71,467 responses, 68% of developers expressed that they love the language and are planning to continue working with Python, and approximately 12.000 of those who haven\'t got the chance to use it have expressed their interest in starting developing with it. Its popularity is mainly due to its simplicity in syntax, expressiveness, and versatility. We can use Python to create any kind of software, from web applications to scientific computing.\\n\\n\\nPython also has [**PyPI package index**](https://pypi.org/),a python package repository, where you can find many useful packages (Tensorflow, pandas, NumPy, etc.), which facilitates and speeds up your project\'s development. In PyPI, you can also publish your packages and share them with the community. The ecosystem keeps growing fast, and big companies like Google, Facebook, and IBM contribute by adding new packages. \\n\\n:::info\\nThe Python Package Index, abbreviated as PyPI (/\u02ccpa\u026api\u02c8a\u026a/) and also known as the Cheese Shop (a reference to the Monty Python\'s Flying Circus sketch \\"Cheese Shop\\"), is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl and to the CRAN repository for R.<a href=\\"#wikipedia:1\\">[1]</a>\\n:::\\n### Python Dependency hell\\n\\n\\nWell, it sounds like Python is amazing! However, if you have been using Python for a while, you may have already noticed that handling different python-installations and dependencies(packages) can be a nightmare! An issue commonly known as dependency hell, which is a term associated with the frustration arising from problems managing our project\'s dependencies. \\n\\nDependency hell in Python often happens because pip does not have a dependency resolver and because all dependencies are shared across projects. So, other projects could be affected when a given dependency may need to be updated or uninstalled. \\n\\nOn top of it, since Python doesn\'t distinguish between different versions of the same library in the `/site-packages` directory, this leads to many conflicts when you have two projects requiring different versions of the same library or the global installation doesn\'t match.\\n\\nThus, having tools that enable us to isolate and manage our project\'s dependencies is highly convenient. In this post, I will show you how to use pyenv and poetry to create your code environments.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./dependency-hell.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\n### Virtual environments to the rescue!\\n\\nA Python virtual environment is a separate folder where only your project\'s dependencies(packages) are located. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and its own independent set of installed Python packages in its site directories. That is a very convenient way to prevent `Dependency Hell.`\\n\\n:::tip\\nPython virtual environment allows multiple versions of Python to coexist in the same machine, so you can test your application using different Python versions. It also keeps your project\'s dependencies isolated, so they don\'t interfere with the dependencies of others projects.\\n:::\\n\\nThere are different tools out there that can be used to create Python virtual environments. In this post, I will show you how to use pyenv and poetry. However, you can also try other tools, such as [virtualenv](https://virtualenv.pypa.io/en/latest/) or anaconda, and based on your experience, you can choose that one you feel most comfortable with.\\nthe video below will provide you with more information about these kinds of tools.\\n\\n<center>\\n    <iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/3J02sec99RM\\" f allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n</center>\\n\\n### Pyenv\\npyenv is a command line tool which allows you to install and run multiple versions of Python in the same machine. For those who come from a javascript background, pyenv is a very similar tool to nvm.\\n\\n**Setup & get started with pyenv**\\n\\nYou can follow the steps below for installing `pyenv` on macOS or check the [documentation](https://github.com/pyenv/pyenv) for alternative installation methods. \\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl https://pyenv.run | bash\', comment: \\"Install pyenv\\"},\\n{type: \'output\', value: \'Installing pyenv...\'},\\n{type: \'output\', value: \'Installation complete!\'}\\n]} />\\n\\n\\nAfter having installed pyenv, you can then install any python version running the command `pyenv install <version>`.\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv install 3.9.0\', comment: \\"Install python 3.9.0 in my machine\\"},\\n{type: \'output\', value: \'Downloading Python-3.9.0.tar.xz...\'},\\n{type: \'output\', value: \'-> https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xz\', delay: 1000},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installed Python-3.9.0 to /Users/haruiz/.pyenv/versions/3.9.0\'}\\n]} />\\n\\n:::tip\\nif you are not sure about which versions are available to be installed in your machine, you can run the command `pyenv install --list`.\\n:::\\n\\nYou can run the command `pyenv versions` to check which Python versions have been installed.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'pyenv versions\' , comment: \\"Check which versions of Python are installed\\"},\\n{type: \'output\', value: \'system\'},\\n{type: \'output\', value: \'* 3.10.0 (set by /Users/haruiz/.pyenv/version)\'},\\n{type: \'output\', value: \'3.9.0\'}\\n]} />\\n\\nTo set the default version of Python to be used, you can run the command `pyenv global <version>`. This version will be used when you run `python` or `python3` in your terminal.\\n\\n<TermynalReact lines ={[\\n{type: \'input\', value: \'pyenv global 3.10.0\', comment: \\"Set python 3.10.0 as the default version\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nAlternatively to the `pyenv global` command, Sometimes you want to set a specific version of Python to be used within a specific folder. You can create a `.python-version` file in the folder and set the version you want to use,  or by running the command `pyenv local <version>`. pyenv will then use this version when you run `python` or `python3` in the folder.\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'mkdir myproject\', comment: \\"Create a folder called myproject\\"},\\n{type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n{type: \'input\', value: \'pwd\',  comment: \\"Check the current directory after the cd command\\"},\\n{type: \'output\', value: \'/Users/haruiz/myproject\'},\\n{type: \'input\', value: \'pyenv local 3.9.0\', comment: \\"Set python 3.9.0 as the default version in myproject\\"},\\n{type: \'input\', value: \'python --version\', comment: \\"Check the version of python after setting it\\"},\\n{type: \'output\', value: \'Python 3.9.0\'},\\n]} />\\n\\nTo make sure what python version is being used by pyenv, you can run the command `pyenv version`.\\n\\n### Poetry\\n\\nPoetry is a tool that allows you to manage your project\'s dependencies and facilitates the process of packaging for distribution. It resolves your project dependencies and makes sure that there are no conflicts between them.\\n\\nPoetry integrates with the [PyPI](https://pypi.org/) package index to find and install your environment dependencies, and pyenv to set your project python runtime.\\n\\nTo install poetry we follow the steps below:\\n\\n<TermynalReact lines ={[\\n{ type: \'input\', value: \'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\', comment: \\"Install poetry\\"},\\n{type: \'progress\'},\\n{type: \'output\', value: \'Installation complete!\'},\\n{type: \\"input\\", value: `export PATH=\\"\\\\$HOME/.poetry/bin:\\\\$PATH\\"`, comment: \\"Add poetry to the PATH\\"},\\n{type: \'input\', value: \'poetry --version\', comment: \\"Check the version of poetry after installing it\\"},\\n{type: \'output\', value: \'Poetry version 1.1.13\'},\\n{type: \'input\', value: \'poetry help completions\', comment: \\"Check the completions of poetry\\"},\\n{type: \'output\', value: \'poetry completions bash\'},\\n{type: \'input\', value: \'poetry config virtualenvs.in-project true\', comment: \\"Configure poetry to create virtual environments inside the project\'s root directory\\"}\\n]} />\\n\\nIf you were able to run the previous commands, we can then move forward with the rest of the tutorial.\\n\\nTo ask poetry to create a new project, we use the command `poetry new <project name>`. \\nThis will create a new folder with the name `<project name>` and a `pyproject.toml` folder inside it.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry new myproject\', comment: \\"Create a new project called myproject\\"},\\n    {type: \'output\', value: \'Created package myproject in myproject\'}\\n]} />\\n\\nIf you already have a project, and you want to use poetry to manage the dependencies, you can use the command `poetry init`. So, poetry will add the `pyproject.toml` file to your project.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd myproject\', comment: \\"Change directory to myproject\\"},\\n    { type: \'input\', value: \'poetry init\', comment: \\"Initialize poetry in myproject\\"},\\n]} />\\n\\n\\nThe main file of your poetry project is the `pyproject.toml` file. This file defines your project\'s dependencies(python packages) and holds the required metadata for packaging. Poetry updates this file every time a new python package is installed. By sharing this file with others, they can recreate your project environment and run your application. To do so, they will need to have poetry installed and run the command `poetry install` within the same folder where the `pyproject.toml` file is located.\\n\\nNow we can start adding dependencies to our project. To do so, we use the command `poetry add <package name>`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry add numpy pandas\', comment: \\"Add numpy and pandas to the project, this command replaces the pip install command\\"},\\n    {type: \'output\', value: \'Installed requests\'}\\n]} />\\n\\nNow our `pyproject.toml` file looks like:\\n\\n```toml\\n    [tool.poetry]\\n    name = \\"myproject\\"\\n    version = \\"0.1.0\\"\\n    description = \\"\\"\\n    authors = [`Henry Ruiz  <henry.ruiz.tamu@gmail.com>`]\\n    \\n    [tool.poetry.dependencies]\\n    python = \\"^3.10\\"\\n    numpy = \\"^1.23.1\\"\\n    pandas = \\"^1.4.3\\"\\n    \\n    [tool.poetry.dev-dependencies]\\n    pytest = \\"^5.2\\"\\n    \\n    [build-system]\\n    requires = [\\"poetry-core>=1.0.0\\"]\\n    build-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nLest review that file sections:\\n\\n- **\\\\[tool.poetry\\\\]:** This section contains informational metadata about our package, such as the package name, description, author details, etc. Most of the config values here are optional unless you\'re planning on publishing this project as an official PyPi package. \\n- **\\\\[tool.poetry.dependencies\\\\]:** This section defines the dependencies of your project. Here is where you define the python packages that your project requires to run. We can update this file manually if it is needed.\\n- **\\\\[tool.poetry.dev-dependencies\\\\]:** This section defines the dev dependencies of your project. These dependencies are not required for your project to run, but they are useful for development.\\n- **\\\\[build-system\\\\]:** This is rarely a section you\'ll need to touch unless you upgrade your version of Poetry.\\n\\nTo see in a nicer format the dependencies of your project, you can use the command `poetry show --tree`. This command draws a graph of all of our dependencies as well as the dependencies of our dependencies.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --tree\', comment: \\"Show the dependencies of our project\\"}]} />\\n\\nIf we are not sure at some point that we have the latest version of a dependency, we can tell poetry to check on our package repository if there is a new version by using \u201c\u2014 latest\u201d option\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry show --latest\', comment: \\"Show the latest version of our dependencies\\"}]} />\\n\\nIf we list our folder content, we will see that not only the `pyproject.toml` file is created, but also some other folders and files. So, let\'s take a look at the contents of the `myproject` folder.\\n\\n```bash\\n\u251c\u2500\u2500 .venv\\n\u2502\xa0\xa0 \u251c\u2500\u2500 .gitignore\\n\u2502\xa0\xa0 \u251c\u2500\u2500 bin\\n\u2502\xa0\xa0 \u251c\u2500\u2500 lib\\n\u2502\xa0\xa0 \u2514\u2500\u2500 pyvenv.cfg\\n\u251c\u2500\u2500 README.rst\\n\u251c\u2500\u2500 myproject\\n\u2502\xa0\xa0 \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 poetry.lock\\n\u251c\u2500\u2500 pyproject.toml\\n\u2514\u2500\u2500 tests\\n    \u251c\u2500\u2500 __init__.py\\n    \u2514\u2500\u2500 test_myproject.py\\n\\n5 directories, 11 files\\n```\\n\\n- **\\\\`.venv\\\\`**: This folder is created by poetry when it creates a virtual environment.It isolates the project from the system environment and provides a clean environment for your project. It contains the Python interpreter and your projects dependencies. \\n- **poetry.lock**: When Poetry finished installing the dependencies, it writes all of the packages and the exact versions of them to the poetry.lock file, locking the project to those specific versions. \\n\\n:::note\\n Notice that this folder structure is created only if the `poetry new myproject` was executed. When poetry is initialized within a folder that already exists ( using the `poetry init` command), only the `pryproject.toml` and the .env folder are created.\\n:::\\n\\n:::tip\\nYou should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies. For more info, check this link : [Poetry basic usage](https://python-poetry.org/docs/basic-usage/)\\n:::\\n\\nBuilding our project and publishing it is just running the ```poetry build``` and ```poetry publish``` commands, so it is pretty intuitive. The publish command will submit our application to pip, so other developers can easily install it.\\n\\n### Hands-on tutorial \\n\\n**Creating a python package using poetry**\\n\\nIn this section, you will learn how to create a simple python package named `style_image` with poetry. This simple python package takes two images, the style image, and the content image, and performs style transfer. \\"Style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together, so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.\\"<a href=\\"#tensorflow-docs:1\\">[2]</a>\\n\\nFor our `style_image` package we will use the `magenta/arbitrary-image-stylization-v1-256` model available in TensorflowHub under-the-hood.\\n\\nSo, let\'s do it!!\\n\\nWe will start by creating a new project called `style_image` using the command `poetry new style_image`.\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'pyenv version\', comment: \\"Check the version of python that is being used by pyenv, it would be the python version that will be used by poetry\\"},\\n    { type: \'input\', value: \'poetry new style_image\', comment: \\"Create a new project called style_image\\"},\\n]} />\\n\\n**Installing package dependencies**\\n\\nNext we are going to install the dependencies of our project, so we run the commands:\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'cd style_image\', comment: \\"Move into the style_image folder where the `pyproject.toml` file is located\\"},\\n    { type: \'input\', value: \'poetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\', comment: \\"Add the dependencies to the project\\", lineDelay: 10.0},\\n    {type: \\"output\\", value: \\"Updating dependencies\\"}, \\n    {type: \\"output\\", value: \\"Resolving dependencies...\\"}, \\n    {type: \\"progress\\", progressPercent: 50},\\n    { type: \'output\', value: `Solver Problem : \\\\n \\nThe current project\\\\\'s Python requirement (>=3.10,<4.0) \\nis not compatible with \\nsome of the required packages Python requirement..... \\\\n\\nFor tensorflow, a possible solution would be to set the \'python\' property to \\">=3.10,<3.11\\"\\n                    `, color: \\"red\\"},\\n]} />\\n\\nWe will see that there is an error trying to install tensorflow:\\n```bash showLineNumbers {14-15}\\nCreating virtualenv style-image in /Users/haruiz/temp/style_image/.venv\\nUsing version ^0.12.0 for tensorflow-hub\\nUsing version ^2.9.1 for tensorflow\\nUsing version ^1.23.1 for numpy\\nUsing version ^9.2.0 for Pillow\\nUsing version ^0.20.0 for validators\\nUsing version ^0.6.1 for typer\\n\\nUpdating dependencies\\nResolving dependencies... (4.2s)\\n\\nSolverProblemError\\n\\nThe current project\'s Python requirement `(>=3.10,<4.0)` is not compatible with some of the required packages Python requirement:\\n- tensorflow-io-gcs-filesystem requires Python `>=3.7, <3.11`, so it will not be satisfied for Python `>=3.11,<4.0`\\n```\\n\\nThe great thing is that poetry generally provides information on how to fix them. For the error above, poetry suggests restricting the python property to `>=3.10,<3.11` in the pyproject.toml file.\\nFor tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to `>=3.10,<3.11`\\n\\n:::tip\\nMake sure you always check the output in the terminal.\\n:::\\n\\nSo the `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\nWe can then try to install the dependencies again:\\n\\n```bash\\npoetry add tensorflow-hub tensorflow numpy pillow validators \\"typer[all]\\"\\n```\\n\\nAfter installing the dependencies, our `pyproject.toml` file is now:\\n\\n```toml showLineNumbers {9-15}\\n[tool.poetry]\\nname = \\"style_image\\"\\nversion = \\"0.1.0\\"\\ndescription = \\"\\"\\nauthors = [\\"Henry Ruiz  <henryruiz22@gmail.com>\\"]\\nreadme = \\"README.md\\"\\n\\n[tool.poetry.dependencies]\\npython = \\">=3.10,<3.11\\"\\ntensorflow-hub = \\"^0.12.0\\"\\nnumpy = \\"^1.23.1\\"\\nPillow = \\"^9.2.0\\"\\ntensorflow = \\"^2.9.1\\"\\nvalidators = \\"^0.20.0\\"\\ntyper = {extras = [\\"all\\"], version = \\"^0.6.1\\"}\\n\\n[tool.poetry.dev-dependencies]\\npytest = \\"^5.2\\"\\nblack = \\"^22.6.0\\"\\n\\n[tool.poetry.scripts]\\nstyle_image = \\"style_image.main:app\\"\\n\\n[build-system]\\nrequires = [\\"poetry-core>=1.0.0\\"]\\nbuild-backend = \\"poetry.core.masonry.api\\"\\n```\\n\\n**Coding our `style_image` package**\\n\\nAt this point, we are ready to start coding, let\'s create the folder structure below and replace the code in each .py file with the code on this repository [https://github.com/haruiz/style_image](https://github.com/haruiz/style_image):\\n```bash\\n    \u251c\u2500\u2500 README.md\\n    \u251c\u2500\u2500 README.rst\\n    \u251c\u2500\u2500 data\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 content_image.jpg\\n    \u251c\u2500\u2500 main.py\\n    \u251c\u2500\u2500 poetry.lock\\n    \u251c\u2500\u2500 pyproject.toml\\n    \u251c\u2500\u2500 style_image\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 core\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 style_image.py\\n    \u2502\xa0\xa0 \u251c\u2500\u2500 main.py\\n    \u2502\xa0\xa0 \u2514\u2500\u2500 util\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __init__.py\\n    \u2502\xa0\xa0     \u251c\u2500\u2500 __pycache__\\n    \u2502\xa0\xa0     \u2514\u2500\u2500 image_utils.py\\n    \u251c\u2500\u2500 stylized_image.png\\n    \u2514\u2500\u2500 tests\\n        \u251c\u2500\u2500 __init__.py\\n        \u2514\u2500\u2500 test_style_image.py\\n```\\nCode :\\n\\n<Tabs>\\n  <TabItem value=\\"main.py\\" label=\\"main.py\\" default>\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\n\\nif __name__ == \\"__main__\\":\\n\\n    content_image_path = \\"data/content_image.jpg\\"\\n    style_image_path = \\"data/style_image.jpg\\"\\n\\n    stylized_image = (\\n        StyleImage(style_image_path)\\n        .transfer(content_image_path, output_image_size=800)\\n        .save(\\"stylized_image.jpg\\")\\n    )\\n```\\n\\n</TabItem>\\n  <TabItem value=\\"core/style_image.py\\" label=\\"core/style_image.py\\">\\n\\n```python showLineNumbers \\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\n\\nfrom style_image.util import ImageUtils\\nfrom PIL import Image as PILImage\\n\\n\\nclass StyleImage:\\n    def __init__(self, style_image_path):\\n        self._style_image_path = style_image_path\\n        hub_handle = (\\n            \\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\\"\\n        )\\n        self._hub_module = hub.load(hub_handle)\\n\\n    def transfer(\\n        self, content_image_path, output_image_size=384, style_img_size=(256, 256)\\n    ):\\n        \\"\\"\\"\\n        transfer the style of the style image to the content image\\n        :param content_image_path: image path of the content image :\\n        :param output_image_size: The content image size can be arbitrary.\\n        :param style_img_size: The style prediction model was trained with image size 256 and it\'s the\\n        recommended image size for the style image (though, other sizes work as\\n        well but will lead to different results).\\n        Recommended to keep it at 256.\\n        :return:\\n        \\"\\"\\"\\n        content_img_size = (output_image_size, output_image_size)\\n        # Load the content and style images.\\n        content_image = ImageUtils.load_image(content_image_path, content_img_size)\\n        style_image = ImageUtils.load_image(self._style_image_path, style_img_size)\\n        # Stylize image.\\n        stylized_image_tensor = self._hub_module(\\n            tf.constant(content_image), tf.constant(style_image)\\n        )[0]\\n        stylized_image_arr = tf.image.convert_image_dtype(\\n            stylized_image_tensor, tf.uint8\\n        ).numpy()\\n        stylized_image_arr = stylized_image_arr[0]  # Remove batch dimension.\\n        stylized_image = PILImage.fromarray(stylized_image_arr)\\n        return stylized_image\\n```\\n\\n  </TabItem>\\n  <TabItem value=\\"util/image_utils.py\\" label=\\"util/image_utils.py\\">\\n\\n```python showLineNumbers \\nimport functools\\nimport tensorflow as tf\\nimport os\\nimport validators\\n\\n\\nclass ImageUtils:\\n    @staticmethod\\n    def crop_center(image):\\n        \\"\\"\\"Returns a cropped square image.\\"\\"\\"\\n        shape = image.shape\\n        new_shape = min(shape[1], shape[2])\\n        offset_y = max(shape[1] - shape[2], 0) // 2\\n        offset_x = max(shape[2] - shape[1], 0) // 2\\n        image = tf.image.crop_to_bounding_box(\\n            image, offset_y, offset_x, new_shape, new_shape\\n        )\\n        return image\\n\\n    @classmethod\\n    @functools.lru_cache(maxsize=None)\\n    def load_image(cls, image_path, image_size=(256, 256), \\n                   preserve_aspect_ratio=True):\\n        \\"\\"\\"Loads and preprocesses images.\\"\\"\\"\\n        # Cache image file locally.\\n        if validators.url(image_path):\\n            image_path = tf.keras.utils.get_file(\\n                os.path.basename(image_path)[-128:], image_path\\n            )\\n        # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\\n        img = tf.io.decode_image(\\n            tf.io.read_file(image_path), channels=3, dtype=tf.float32\\n        )[tf.newaxis, ...]\\n        img = cls.crop_center(img)\\n        img = tf.image.resize(\\n            img, image_size, preserve_aspect_ratio=preserve_aspect_ratio\\n        )\\n        return img\\n\\n``` \\n\\n  </TabItem>\\n<TabItem value=\\"style_image/main.py\\" label=\\"style_image/main.py\\">\\n\\n```python showLineNumbers \\nfrom style_image import StyleImage\\nimport typer\\n\\napp = typer.Typer()\\n\\n\\ndef style_image_callback(value: str):\\n    style_urls = dict(\\n        kanagawa_great_wave=\\"https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\\",\\n        kandinsky_composition_7=\\"https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\\",\\n        hubble_pillars_of_creation=\\"https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\\",\\n        van_gogh_starry_night=\\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\\",\\n        turner_nantes=\\"https://upload.wikimedia.org/wikipedia/commons/b/b7/JMW_Turner_-_Nantes_from_the_Ile_Feydeau.jpg\\",\\n        munch_scream=\\"https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg\\",\\n        picasso_demoiselles_avignon=\\"https://upload.wikimedia.org/wikipedia/en/4/4c/Les_Demoiselles_d%27Avignon.jpg\\",\\n        picasso_violin=\\"https://upload.wikimedia.org/wikipedia/en/3/3c/Pablo_Picasso%2C_1911-12%2C_Violon_%28Violin%29%2C_oil_on_canvas%2C_Kr%C3%B6ller-M%C3%BCller_Museum%2C_Otterlo%2C_Netherlands.jpg\\",\\n        picasso_bottle_of_rum=\\"https://upload.wikimedia.org/wikipedia/en/7/7f/Pablo_Picasso%2C_1911%2C_Still_Life_with_a_Bottle_of_Rum%2C_oil_on_canvas%2C_61.3_x_50.5_cm%2C_Metropolitan_Museum_of_Art%2C_New_York.jpg\\",\\n        fire=\\"https://upload.wikimedia.org/wikipedia/commons/3/36/Large_bonfire.jpg\\",\\n        derkovits_woman_head=\\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Derkovits_Gyula_Woman_head_1922.jpg\\",\\n        amadeo_style_life=\\"https://upload.wikimedia.org/wikipedia/commons/8/8e/Untitled_%28Still_life%29_%281913%29_-_Amadeo_Souza-Cardoso_%281887-1918%29_%2817385824283%29.jpg\\",\\n        derkovtis_talig=\\"https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg\\",\\n        amadeo_cardoso=\\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Amadeo_de_Souza-Cardoso%2C_1915_-_Landscape_with_black_figure.jpg\\",\\n    )\\n    if value in style_urls:\\n        return style_urls[value]\\n    return value\\n\\n\\n@app.command()\\ndef main(\\n    style_image: str = typer.Option(\\n        ..., \\"--style_image\\", \\"-s\\", callback=style_image_callback\\n    ),\\n    content_image: str = typer.Option(..., \\"--content_image\\", \\"-c\\"),\\n    output_image_size: int = typer.Option(384, \\"--output_image_size\\", \\"-sz\\"),\\n    output_image_path: str = typer.Option(\\"stylized_image.png\\", \\"--output_image_path\\", \\"-o\\"),\\n):\\n    style_image = StyleImage(style_image)\\n    stylized_image = style_image.transfer(content_image, output_image_size=output_image_size)\\n    stylized_image.save(output_image_path)\\n\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n:::note running your scripts using the virtual environment\\nNotice that if you want to execute the `main.py` file or any other file/script using the python environment you just created, you need to run the command `poetry run python main.py.` So, poetry knows that you are running the `main.py` file with the python environment created for the `style_image` package.\\nIf you feel more comfortable running `python main.py,` instead of running `poetry run ...` you can permanently activate the environment running the command `poetry shell.`. So it will be activated for all the commands you run.\\n:::\\n\\nPoetry and pyenv are integrated with `visual studio` code and `Pycharm`. In fact, they will automatically recognize the python environment created by poetry.\\n\\n**Publishing our package to PyPi**\\n\\nPublishing our package in Pypi should be straightforward. We just run the `poetry publish` command. Since this is just a demo, we are going to publish our package to the pypi test repository `https://test.pypi.org/.` However, the steps should be the same in production `https://pypi.org/.`\\n\\n<TermynalReact lines ={[\\n    { type: \'input\', value: \'poetry build\', comment: \\"build package\\" },\\n    { type: \'input\', value: \'poetry config repositories.testpypi https://test.pypi.org/legacy/\', comment: \\"add repository\\" },\\n    { type: \'input\', value: \'poetry config repositories\', comment: \\"list repositories\\" },\\n    { type: \'output\', value: `{\'testpypi\': {\'url\': \'https://test.pypi.org/\'}}`, color: \\"gray\\" },\\n    { type: \'input\', value: \'poetry publish -r testpypi\', comment: \\"publish package to testpypi repository\\" },\\n    { type: \'prompt\', value: \'username : haruiz\'},\\n    { type: \'prompt\', value: \'password\'},\\n    { type: \\"output\\", value: \\"Publishing style_image (0.1.0) to testpypi..\\", color:\\"green\\"}\\n  ]} />\\n\\nIf the `publish` command is successful, you will be able to find the package in the testpypi repository.\\n\\n<div style={{textAlign: \\"center\\"}}>\\n    <Image img={require(\\"./pypi-test.png\\")} alt=\\"Dependency hell\\" />\\n</div>\\n\\nThat is all!! We are done!!. You can check the links below for more information about poetry. \\n\\nThanks for your support and don\'t forget to share,\\n\\n\\n**Some useful resources**\\n- [Poetry Documentation](https://python-poetry.org/)\\n- [Pyenv Documentation](https://github.com/pyenv/pyenv)\\n- [Great talk about poetry](https://www.youtube.com/watch?v=QX_Nhu1zhlg&ab_channel=PyGotham2019)\\n- [Package Python Projects the Proper Way with Poetry](https://hackersandslackers.com/python-poetry-package-manager/)\\n- [Poetry: Finally an all-in-one tool to manage Python packages](https://medium.com/analytics-vidhya/poetry-finally-an-all-in-one-tool-to-manage-python-packages-3c4d2538e828)\\n- [Making Python Packages Part 2: How to Publish & Test Your Package on PyPI with Poetry](https://towardsdatascience.com/packages-part-2-how-to-publish-test-your-package-on-pypi-with-poetry-9fc7295df1a5)\\n- [Publishing to a private Python repository with Poetry](https://medium.com/packagr/publishing-to-a-private-python-repository-with-poetry-23b660484471)\\n- [Python Virtual Environments tutorial using Virtualenv and Poetry](https://serpapi.com/blog/python-virtual-environments-using-virtualenv-and-poetry/)\\n- [The Nine Circles of Python Dependency Hell](https://medium.com/knerd/the-nine-circles-of-python-dependency-hell-481d53e3e025)\\n- [Get started with pyenv & poetry. Saviours in the python chaos!](https://blog.jayway.com/2019/12/28/pyenv-poetry-saviours-in-the-python-chaos/)\\n\\n**References**\\n<ul>\\n <li><a id=\\"wikipedia:1\\" href=\\"https://en.wikipedia.org/wiki/Python_Package_Index\\" target=\\"_blank\\">[1] Python Package Index</a></li>\\n <li><a id=\\"tensorflow-docs:1\\" href=\\"https://www.tensorflow.org/tutorials/generative/style_transfer\\" target=\\"_blank\\">[2] Neural style transfer</a></li>\\n</ul>"}]}')}}]);