"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5417],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(96540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}},87065:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var o=t(99869),r=t(74848),i=t(28453);const s={title:"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK",slug:"implementing-anthropic's-agent-design-patterns-with-google-adk",description:"Agentic systems are rapidly becoming a core design pattern for LLM-powered applications, enabling dynamic reasoning, decision-making, and tool use. Inspired by Anthropic\u2019s influential Building Effective Agents article, this post demonstrates the implementation of their proposed agent design patterns\u2014such as prompt chaining, routing, and parallelization\u2014using Google\u2019s open-source Agent Development Kit (ADK). The guide provides practical, hands-on examples that illustrate how these patterns work and where they can be effectively applied.",authors:["haruiz"],tags:["python","agentic-systems","llm-agents","agent-development-kit","prompt-engineering","ai-workflows","autonomous-agents","multi-agent-systems","adk"]},a=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Hands-on Examples",id:"hands-on-examples",level:2},{value:"Workflow: Prompt chaining",id:"workflow-prompt-chaining",level:2},{value:"Workflow: Routing",id:"workflow-routing",level:2},{value:"Workflow: Parallelization",id:"workflow-parallelization",level:2},{value:"Workflow: Orchestrator-workers",id:"workflow-orchestrator-workers",level:2},{value:"Workflow: Evaluator-optimizer",id:"workflow-evaluator-optimizer",level:2},{value:"Agents",id:"agents",level:2},{value:"Key Differences and Summary",id:"key-differences-and-summary",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"References",id:"references",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["Anthropic\u2019s ",(0,r.jsx)("a",{href:"#anthropic:1",children:'"Building Effective Agents"[1]'})," article has quickly become a widely referenced guide for designing agentic systems, particularly because it introduces the conceptual and functional differences between Workflows and Agents."]}),"\n",(0,r.jsx)(n.admonition,{title:"Anthropic",type:"quote",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Workflows"})," are systems where LLMs and tools are orchestrated through predefined code paths.\n",(0,r.jsx)(n.strong,{children:"Agents"}),", on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks."]})}),"\n",(0,r.jsx)(n.p,{children:"Workflows provide structure, determinism, and reproducibility\u2014making them ideal for applications with well-defined logic and fixed execution paths. Agents, in contrast, offer flexibility and autonomy, enabling LLMs to direct their own reasoning, planning, and tool use to make decisions and solve problems. This autonomy makes agent-based systems especially effective in open-ended, interactive, or partially observable environments."}),"\n",(0,r.jsx)(n.p,{children:"This post explores Anthropic\u2019s proposed agent design patterns and shows how to implement them using Google\u2019s open-source Agent Development Kit (ADK). ADK provides a flexible and extensible framework for building, orchestrating, and deploying LLM-powered agents, making it a practical choice for bringing these concepts into real-world applications."}),"\n",(0,r.jsx)(n.admonition,{title:"In This Post You Will Learn",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The core differences between ",(0,r.jsx)(n.strong,{children:"Workflows"})," and ",(0,r.jsx)(n.strong,{children:"Agents"}),", as introduced by Anthropic, and when to use each approach"]}),"\n",(0,r.jsxs)(n.li,{children:["How to implement six of the foundational agent design patterns from Anthropic\u2019s article using Google ADK:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd01 ",(0,r.jsx)(n.strong,{children:"Prompt Chaining"})," for step-by-step task decomposition"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd00 ",(0,r.jsx)(n.strong,{children:"Routing"})," for directing inputs to specialized agents"]}),"\n",(0,r.jsxs)(n.li,{children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Parallelization"})," to run tasks concurrently using sectioning and voting"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\udde0 ",(0,r.jsx)(n.strong,{children:"Orchestrator\u2013Worker"})," pattern for dynamic task delegation and synthesis"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\uddea ",(0,r.jsx)(n.strong,{children:"Evaluator\u2013Optimizer"})," loop for iterative refinement based on structured feedback"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\udd16 ",(0,r.jsx)(n.strong,{children:"Autonomous Agents"})," that reason, plan, and act independently in open-ended environments"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"How to integrate tools, memory, and execution loops using ADK framework"}),"\n"]})}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-examples",children:"Hands-on Examples"}),"\n",(0,r.jsx)(n.p,{children:"The foundation for all the agent patterns explored and implemented in this post begins with a core building block: an augmented LLM equipped with tools, memory, and retrieval capabilities. This setup enables the agent to access external knowledge, retain contextual awareness over time, and interact with its environment through purposeful actions. These foundational capabilities will be integrated into the implementation of each agent pattern described in the following sections. The code snippet below demonstrates how to augment an LLM with a tool that retrieves the current weather and time\u2014effectively transforming it into a functional agent."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import datetime\nfrom zoneinfo import ZoneInfo\n\nfrom google.adk import Runner\nfrom google.adk.agents import Agent\nfrom dotenv import load_dotenv\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\nfrom rich.panel import Panel\nfrom rich import print\n\nload_dotenv(verbose=True)\n\ndef get_weather(city: str) -> dict:\n    """Retrieves the current weather report for a specified city.\n\n    Args:\n        city (str): The name of the city for which to retrieve the weather report.\n\n    Returns:\n        dict: status and result or error msg.\n    """\n    if city.lower() == "new york":\n        return {\n            "status": "success",\n            "report": (\n                "The weather in New York is sunny with a temperature of 25 degrees"\n                " Celsius (41 degrees Fahrenheit)."\n            ),\n        }\n    else:\n        return {\n            "status": "error",\n            "error_message": f"Weather information for \'{city}\' is not available.",\n        }\n\n\ndef get_current_time(city: str) -> dict:\n    """Returns the current time in a specified city.\n\n    Args:\n        city (str): The name of the city for which to retrieve the current time.\n\n    Returns:\n        dict: status and result or error msg.\n    """\n\n    if city.lower() == "new york":\n        tz_identifier = "America/New_York"\n    else:\n        return {\n            "status": "error",\n            "error_message": (\n                f"Sorry, I don\'t have timezone information for {city}."\n            ),\n        }\n\n    tz = ZoneInfo(tz_identifier)\n    now = datetime.datetime.now(tz)\n    report = (\n        f\'The current time in {city} is {now.strftime("%Y-%m-%d %H:%M:%S %Z%z")}\'\n    )\n    return {"status": "success", "report": report}\n\n\nroot_agent = Agent(\n    name="weather_time_agent",\n    model="gemini-2.0-flash",\n    description=(\n        "Agent to answer questions about the time and weather in a city."\n    ),\n    instruction=(\n        "You are a helpful agent who can answer user questions about the time and weather in a city."\n    ),\n    tools=[get_weather, get_current_time],\n)\n\n# Example usage\nif __name__ == \'__main__\':\n\n    # --- Constants ---\n    APP_NAME = "code_refinement_app"\n    USER_ID = "user_123"\n    SESSION_ID = "session_456"\n\n    # --- Services ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n\n    # Create session once\n    session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n\n    # --- Runner Setup ---\n    runner = Runner(\n        agent=root_agent,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service,\n    )\n    prompt = "What is the current weather in New York?"\n    print(Panel.fit(f"[bold white]User Prompt[/bold white]: {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            response = event.content.parts[0].text\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green] {response}", title="\ud83e\udd16"))\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"workflow-prompt-chaining",children:"Workflow: Prompt chaining"}),"\n",(0,r.jsx)(n.p,{children:"The prompt chaining pattern consists of breaking a complex task into a series of smaller, manageable steps, where each step involves an LLM processing the output of the previous one. This sequential structure allows for greater control and reliability, as you can introduce programmatic checks\u2014or \u201cgates\u201d\u2014at any stage to validate intermediate results and keep the process aligned with the intended goal."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use this pattern:"})}),"\n",(0,r.jsx)(n.p,{children:"Prompt chaining is ideal for scenarios where a task can be cleanly divided into discrete, well-ordered subtasks. It intentionally trades off some latency to gain higher accuracy by reducing the cognitive load on each individual model invocation."}),"\n",(0,r.jsx)(n.admonition,{title:"Example applications",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Creating marketing content and then translating it into another language"}),"\n",(0,r.jsx)(n.li,{children:"Drafting an outline, validating it against predefined criteria, and using it to generate a full document"}),"\n"]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In the following example, we implement a simple joke generation workflow that consists of three steps: generating a joke, improving it, and polishing it. Each step is handled by a separate agent, and the output of one agent is passed as input to the next."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from typing import Optional, List, Dict\nfrom dotenv import load_dotenv\nfrom rich import print\nfrom rich.panel import Panel\n\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent, SequentialAgent\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.models import LlmRequest, LlmResponse\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\n\n# --- Load environment variables ---\nload_dotenv()\n\n# --- Constants ---\nBLOCKED_KEYWORDS = ["apple"]  # Extendable\n\n# --- Agent Configs ---\nAGENT_CONFIGS: List[Dict] = [\n    {\n        "name": "joke_generator",\n        "description": "Generate a joke",\n        "instruction": "Generate a joke based on the user prompt",\n        "output_key": "joke",\n        "temperature": 1.0\n    },\n    {\n        "name": "joke_improver",\n        "description": "Improve the joke",\n        "instruction": "Make the joke funnier and more engaging",\n        "output_key": "improved_joke",\n        "temperature": 0.7\n    },\n    {\n        "name": "joke_polisher",\n        "description": "Polish the joke",\n        "instruction": "Polish the joke, add a surprise twist at the end",\n        "output_key": "polished_joke",\n        "temperature": 0.5\n    },\n]\n\n# --- Guardrail Callback ---\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n    """\n    Guardrail function to block inappropriate prompts.\n    """\n    prompt = llm_request.contents[0].parts[0].text.lower()\n    print(Panel.fit(f"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\n[bold cyan]Prompt:[/bold cyan] {prompt}"))\n\n    for word in BLOCKED_KEYWORDS:\n        if word in prompt:\n            raise ValueError(f"\u274c Prompt contains forbidden word: \'{word}\'. Please rephrase.")\n\n    return None\n\n# --- Agent Factory ---\ndef create_llm_agent(config: Dict) -> LlmAgent:\n    return LlmAgent(\n        name=config["name"],\n        description=config["description"],\n        model="gemini-2.0-flash",\n        global_instruction=f"You are a {config[\'description\'].lower()}.",\n        instruction=config["instruction"],\n        output_key=config["output_key"],\n        generate_content_config=types.GenerateContentConfig(temperature=config["temperature"]),\n        before_model_callback=on_before_model_callback\n    )\n\n# --- Create Sequential Workflow ---\njoke_agents = [create_llm_agent(cfg) for cfg in AGENT_CONFIGS]\njoke_workflow = SequentialAgent(\n    name="joke_generator_workflow",\n    description="Generate, improve, and publish a joke",\n    sub_agents=joke_agents\n)\n\n# --- Set root agent for the web user interface ---\nroot_agent = joke_workflow\n\n# --- Execution Handlers ---\ndef call_agent(prompt: str):\n    """\n    Run the agent workflow with a user prompt.\n    """\n    print(Panel.fit(f"[bold white]User Prompt:[/bold white] {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green] {event.content.parts[0].text}", title="\ud83e\udd16"))\n\ndef inspect_state():\n    """\n    Inspect and print the internal session state.\n    """\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\n    print(Panel.fit("[bold yellow]Session State[/bold yellow]"))\n    for key, value in state.items():\n        print(f"[cyan]{key}[/cyan]:\\n{value}\\n")\n\n# --- Main Execution ---\nif __name__ == \'__main__\':\n    APP_NAME = "joke_generator_app"\n    USER_ID = "dev_user_01"\n    SESSION_ID = "dev_user_session_01"\n\n    # --- Session & Runner Setup ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n\n    session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n\n    runner = Runner(\n        agent=joke_workflow,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service\n    )\n\n    try:\n        call_agent("Tell me a robot joke")\n        inspect_state()\n    except Exception as e:\n        print(Panel.fit(f"[bold red]Error:[/bold red] {str(e)}", title="\u274c"))\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"workflow-routing",children:"Workflow: Routing"}),"\n",(0,r.jsx)(n.p,{children:"The routing pattern involves classifying the incoming task/prompt and directing them to the most appropriate follow-up LLM/Agent call. This approach promotes modularity and specialization, enabling more tailored prompts for each category. Without routing, attempts to optimize for one input type often degrade performance for others due to overly generalized prompts."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use this pattern:"})}),"\n",(0,r.jsx)(n.p,{children:"Routing is particularly effective for tasks involving clearly distinguishable input categories that benefit from separate handling. It works best when accurate classification can be achieved\u2014whether through an LLM, rule-based logic, or a traditional machine learning classifier."}),"\n",(0,r.jsx)(n.admonition,{title:"Example applications",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Handling customer service inquiries by directing general questions, refund requests, and technical issues to different prompt flows or tools"}),"\n",(0,r.jsx)(n.li,{children:"Forwarding simple, frequently asked questions to lightweight models like Claude 3.5 Haiku, while sending complex or edge cases to more capable models like Claude 3.5 Sonnet to balance speed and cost"}),"\n"]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In this example, we implement a routing agent that delegates tasks to sub-agents based on the topic of the user prompt. The routing agent uses a simple keyword-based classification to determine which sub-agent to invoke."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nfrom rich import print\nfrom rich.panel import Panel\nfrom typing import Optional, List, Dict\n\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.models import LlmRequest, LlmResponse\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\n\n# --- Load environment ---\nload_dotenv()\n\n# --- Constants ---\nBLOCKED_KEYWORDS = ["apple"]\n\n# --- Router Config: Define Routing Sub-Agents ---\nROUTER_CONFIG: List[Dict] = [\n    {\n        "name": "joke_generator",\n        "description": "Generate a joke",\n        "instruction": "Generate a joke based on the user prompt",\n        "output_key": "joke",\n        "temperature": 1.0\n    },\n    {\n        "name": "song_generator",\n        "description": "Generate a song",\n        "instruction": "Generate a song based on the user prompt",\n        "output_key": "song",\n        "temperature": 1.0\n    },\n    {\n        "name": "poem_generator",\n        "description": "Generate a poem",\n        "instruction": "Generate a poem based on the user prompt",\n        "output_key": "poem",\n        "temperature": 1.0\n    }\n]\n\n# --- Guardrail Callback ---\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n    prompt = llm_request.contents[0].parts[0].text.lower()\n    print(Panel.fit(f"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\n[bold cyan]Prompt:[/bold cyan] {prompt}"))\n\n    for word in BLOCKED_KEYWORDS:\n        if word in prompt:\n            raise ValueError(f"\u274c Prompt contains forbidden word: \'{word}\'. Please rephrase.")\n\n    return None\n\n\n# --- Helper: Agent Factory from Router Config ---\ndef create_llm_agent(config: Dict) -> LlmAgent:\n    return LlmAgent(\n        name=config["name"],\n        description=config["description"],\n        model="gemini-2.0-flash",\n        global_instruction=f"You are a {config[\'description\'].lower()}.",\n        instruction=config["instruction"],\n        output_key=config["output_key"],\n        generate_content_config=types.GenerateContentConfig(temperature=config.get("temperature", 1.0)),\n        before_model_callback=on_before_model_callback\n    )\n\n# Create sub-agents from config\nsub_agents = [create_llm_agent(cfg) for cfg in ROUTER_CONFIG]\n\n# --- Router Agent ---\nrouter_instruction = (\n    "You are a router agent.\\n"\n    "Given the user prompt, decide whether it\'s a request for a joke, song, or poem, and delegate accordingly.\\n"\n    "Use only the appropriate sub-agent based on the topic.\\n"\n)\n\nrouter_agent = LlmAgent(\n    name="root_router",\n    model="gemini-2.0-flash",\n    description="Router agent that delegates to joke, song, or poem generators.",\n    instruction=router_instruction,\n    sub_agents=sub_agents,\n    output_key="final_response",\n    before_model_callback=on_before_model_callback\n)\n\n# --- Set root agent for the web user interface ---\nroot_agent = router_agent\n\n# --- Execution Helpers ---\ndef call_agent(prompt: str):\n    """\n    Call the router agent with a user prompt and print the response.\n    """\n    print(Panel.fit(f"[bold white]User Prompt:[/bold white] {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            response = event.content.parts[0].text\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green] {response}", title="\ud83e\udd16"))\n\n\ndef inspect_state():\n    """\n    Print the internal session state.\n    """\n    state = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\n    print(Panel.fit("[bold yellow]Session State[/bold yellow]"))\n    for key, value in state.items():\n        print(f"[cyan]{key}[/cyan]: {value}")\n\n\n# --- Entry Point ---\nif __name__ == \'__main__\':\n    APP_NAME = "joke_generator_app"\n    USER_ID = "dev_user_01"\n    SESSION_ID = "dev_user_session_01"\n\n    # --- Session & Runner Setup ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n\n    session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n\n    runner = Runner(\n        agent=router_agent,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service\n    )\n\n    try:\n        topic = "robots"\n        call_agent(f"write a poem about {topic}")\n        inspect_state()\n    except Exception as e:\n        print(Panel.fit(f"[bold red]Error:[/bold red] {str(e)}", title="\u274c"))\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"workflow-parallelization",children:"Workflow: Parallelization"}),"\n",(0,r.jsx)(n.p,{children:"The parallelization pattern allows multiple LLMs to work concurrently on a task, with their outputs combined through programmatic aggregation. This approach typically takes two main forms:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sectioning:"})," Dividing a task into distinct, independent components that can be handled in parallel\n",(0,r.jsx)(n.strong,{children:"Voting:"})," Running the same task multiple times to gather diverse perspectives and aggregate results for higher confidence"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use this pattern:"})}),"\n",(0,r.jsx)(n.p,{children:"Parallelization is ideal when tasks can be broken into parts that are either independent or benefit from multiple viewpoints. It\u2019s especially effective for increasing speed by distributing workload or for improving accuracy in complex evaluations by isolating specific dimensions of a problem across separate LLM calls."}),"\n",(0,r.jsxs)(n.admonition,{title:"Example applications",type:"tip",children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sectioning:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Generating a multi-part report where each section (e.g., summary, key findings, recommendations) is written by a different LLM call"}),"\n",(0,r.jsx)(n.li,{children:"Creating localized versions of a product description, with one model instance handling cultural adaptation per target market in parallel"}),"\n",(0,r.jsx)(n.li,{children:"Conducting a multi-faceted sentiment analysis, where separate LLM calls assess tone, emotional intensity, and subjectivity individually"}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Voting:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rewriting a paragraph for clarity and running several prompts with slight variations, then selecting the best version via ranking or consensus"}),"\n",(0,r.jsx)(n.li,{children:"Classifying user feedback as actionable or non-actionable using multiple interpretations, then aggregating to reduce classification errors"}),"\n",(0,r.jsx)(n.li,{children:"Diagnosing possible root causes from system logs by prompting the model with different framing strategies and combining insights"}),"\n"]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In this example, we implement a parallel agent that generates a joke, song, and poem concurrently based on the user prompt. The outputs are then merged into a structured response."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nfrom rich import print\nfrom rich.panel import Panel\nfrom typing import List, Dict, Optional\n\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent, ParallelAgent, SequentialAgent\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.models import LlmRequest, LlmResponse\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\n\n# --- Load Environment ---\nload_dotenv()\n\n# --- Constants ---\nBLOCKED_KEYWORDS = ["bruno"]\n\n# --- Task Definitions ---\nTASK_CONFIGS: List[Dict[str, str]] = [\n    {\n        "name": "joke_generator",\n        "description": "Generate a joke",\n        "instruction": "Generate a joke based on the user prompt",\n        "output_key": "joke"\n    },\n    {\n        "name": "song_generator",\n        "description": "Generate a song",\n        "instruction": "Generate a song based on the user prompt",\n        "output_key": "song"\n    },\n    {\n        "name": "poem_generator",\n        "description": "Generate a poem",\n        "instruction": "Generate a poem based on the user prompt",\n        "output_key": "poem"\n    },\n]\n\n# --- Callback Guardrail ---\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n    """\n    Guardrail to block LLM execution for specific banned phrases.\n    """\n    prompt = llm_request.contents[0].parts[0].text.lower()\n    print(Panel.fit(f"[bold magenta]Agent:[/bold magenta] {callback_context.agent_name}\\n[bold cyan]Prompt:[/bold cyan] {prompt}"))\n\n    for banned in BLOCKED_KEYWORDS:\n        if banned in prompt:\n            return LlmResponse(\n                content=types.Content(\n                    role="model",\n                    parts=[types.Part(text=f"LLM call blocked. We don\'t talk about {banned.capitalize()}!!")]\n                )\n            )\n    return None\n\n\n# --- Helper: Create Agent from Task Config ---\ndef create_task_handler_agent(task: Dict[str, str]) -> LlmAgent:\n    return LlmAgent(\n        name=task["name"],\n        description=task["description"],\n        model="gemini-2.0-flash",\n        global_instruction=f"You are a {task[\'description\'].lower()} generator.",\n        instruction=task["instruction"],\n        output_key=task["output_key"],\n        generate_content_config=types.GenerateContentConfig(temperature=1.0),\n        before_model_callback=on_before_model_callback\n    )\n\n# --- Create Sub-agents ---\nsub_agents = [create_task_handler_agent(task) for task in TASK_CONFIGS]\n\n# --- Aggregator (Parallel Execution) ---\naggregator_agent = ParallelAgent(\n    name="ParallelGenerator",\n    sub_agents=sub_agents,\n    description="Run joke, song, and poem generators in parallel based on the user prompt."\n)\n\n# --- Merger Agent ---\nmerger_agent = LlmAgent(\n    name="merger_agent",\n    description="Merge the outputs of the sub-agents into a structured response.",\n    model="gemini-2.0-flash",\n    global_instruction="You are a merger agent.",\n    instruction=(\n        "Your task is to merge the outputs of multiple sub-agents into a single, coherent, and structured response.\\n\\n"\n        "- Do **not** add any external information, context, or commentary.\\n"\n        "- Use **only** the provided inputs: {joke}, {song}, and {poem}.\\n"\n        "- Maintain the exact order and structure specified below.\\n\\n"\n        "### Joke:\\n{joke}\\n\\n"\n        "### Song:\\n{song}\\n\\n"\n        "### Poem:\\n{poem}\\n\\n"\n        "Instructions:\\n"\n        "- Do **not** include any introductory or concluding phrases.\\n"\n        "- Do **not** modify, interpret, or enhance the content of the inputs.\\n"\n        "- Strictly follow the format above and output only the merged content as shown."\n    ),\n    output_key="merged_response",\n    generate_content_config=types.GenerateContentConfig(temperature=0.5),\n)\n\n# --- Root Agent (Sequential Flow) ---\nroot_agent = SequentialAgent(\n    name="root_agent",\n    sub_agents=[aggregator_agent, merger_agent],\n    description="Coordinates generation and merging of joke, song, and poem."\n)\n\n\n# --- Interaction Functions ---\ndef call_agent(prompt: str):\n    """\n    Send a prompt to the root agent and print structured results.\n    """\n    print(Panel.fit(f"[bold white]User Prompt:[/bold white] {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green]\\n{event.content.parts[0].text}", title="\ud83e\udd16"))\n\ndef inspect_state():\n    """\n    Print the internal state of the session.\n    """\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\n    print(Panel.fit("[bold yellow]Session State[/bold yellow]"))\n    for key, value in state.items():\n        print(f"[cyan]{key}[/cyan]: {value}")\n\n# --- Main ---\nif __name__ == \'__main__\':\n    APP_NAME = "joke_generator_app"\n    USER_ID = "dev_user_01"\n    SESSION_ID = "dev_user_session_01"\n\n    # --- Session & Runner Setup ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n\n    session_service.create_session(\n        app_name=APP_NAME,\n        user_id=USER_ID,\n        session_id=SESSION_ID\n    )\n\n    runner = Runner(\n        agent=root_agent,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service\n    )\n\n    call_agent("Please generate something funny and poetic.")\n    inspect_state()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"workflow-orchestrator-workers",children:"Workflow: Orchestrator-workers"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"orchestrator\u2013worker pattern"})," involves a central LLM Agent (the orchestrator) that interprets the input, dynamically decomposes the task into subtasks, assigns these to specialized worker LLMs, and then integrates their outputs into a final result."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use this pattern:"})}),"\n",(0,r.jsx)(n.p,{children:"This workflow is ideal for complex, open-ended tasks where the structure and number of subtasks can\u2019t be known in advance. Unlike parallelization, where subtasks are predefined and run concurrently, the orchestrator\u2013worker setup offers greater adaptability\u2014the orchestrator decides what needs to be done based on the specific input context."}),"\n",(0,r.jsx)(n.admonition,{title:"Example applications",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Writing a grant proposal where the orchestrator delegates background research, impact justification, and formatting to different LLM calls based on the application requirements"}),"\n",(0,r.jsx)(n.li,{children:"Preparing a competitive market analysis, where the orchestrator identifies key competitors, then assigns each to a worker to extract data, analyze strategies, and compare strengths"}),"\n",(0,r.jsx)(n.li,{children:"Automating legal document review, where the orchestrator dynamically assigns sections (e.g., clauses, terms, jurisdictional elements) to worker agents depending on the document type and structure"}),"\n"]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In this example, we implement an orchestrator agent that coordinates the generation of a joke, song, and poem based on the user prompt/topic. The orchestrator dynamically assigns tasks to worker agents and merges their outputs into a final response."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from typing import AsyncGenerator, List, Dict, Optional\nfrom dotenv import load_dotenv\nfrom rich import print\nfrom rich.panel import Panel\n\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent, LoopAgent, BaseAgent\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.models import LlmRequest, LlmResponse\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.events import Event, EventActions\nfrom google.genai import types\n\n# Load .env file\nload_dotenv()\n\n\n# --- Task Definitions ---\nTASK_CONFIGS: List[Dict[str, str]] = [\n    {\n        "name": "joke_generator",\n        "instruction": "Generate a joke based on the user prompt",\n        "output_key": "joke"\n    },\n    {\n        "name": "song_generator",\n        "instruction": "Generate a song based on the user prompt",\n        "output_key": "song"\n    },\n    {\n        "name": "poem_generator",\n        "instruction": "Generate a poem based on the user prompt",\n        "output_key": "poem"\n    },\n]\n\n# --- Guardrail Callback ---\ndef on_before_model_callback(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n    """\n    Intercepts the prompt before sending it to the model. Useful for filtering or logging.\n    Blocks LLM call if restricted keyword is present.\n    """\n    prompt = llm_request.contents[0].parts[0].text\n    if "bruno" in prompt.lower():\n        return LlmResponse(\n            content=types.Content(\n                role="model",\n                parts=[types.Part(text="LLM call was blocked. We don\'t talk about Bruno!!")],\n            )\n        )\n    return None\n\n# --- Agent Factory ---\ndef create_task_handler_agent(task: Dict[str, str]) -> LlmAgent:\n    """\n    Creates an LLM Agent from a task configuration dictionary.\n    Each task must include: name, instruction, and output_key.\n    """\n    return LlmAgent(\n        name=task["name"],\n        description=f"Generate a {task[\'output_key\']}",\n        model=task.get("model", "gemini-2.0-flash"),\n        global_instruction=task.get("global_instruction", f"You are a {task[\'output_key\']} generator."),\n        instruction=task["instruction"],\n        output_key=task["output_key"],\n        generate_content_config=types.GenerateContentConfig(\n            temperature=task.get("temperature", 1.0)\n        ),\n        before_model_callback=on_before_model_callback,\n    )\n\n# --- Create Agents from Configs ---\ntask_handler_agents = [create_task_handler_agent(task) for task in TASK_CONFIGS]\n\n# --- Generic CheckCondition Agent ---\nclass CheckCondition(BaseAgent):\n    output_keys : List[str] = []\n\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\n        is_done = all(ctx.session.state.get(key) is not None for key in self.output_keys)\n        yield Event(author=self.name, actions=EventActions(escalate=is_done))\n\n# --- Setup Orchestrator Agent ---\noutput_keys = [task["output_key"] for task in TASK_CONFIGS]\n\norchestrator_agent = LoopAgent(\n    name="coordinator_agent",\n    max_iterations=10,\n    sub_agents=task_handler_agents + [CheckCondition(name="Checker", output_keys=output_keys)]\n)\n\n# --- Set root agent for the web user interface ---\nroot_agent = orchestrator_agent\n\n\n# --- Agent Interaction Functions ---\ndef call_agent(prompt: str):\n    """\n    Trigger the orchestrator with a prompt.\n    """\n    print(Panel.fit(f"[bold white]User Prompt:[/bold white] {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            response = event.content.parts[0].text\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green] {response}", title="\ud83e\udd16"))\n\ndef inspect_state():\n    """\n    Print session state from memory.\n    """\n    state = runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\n    print(Panel.fit("[bold yellow]Session State[/bold yellow]"))\n    for key, value in state.items():\n        print(f"[cyan]{key}[/cyan]: {value}")\n\n# --- Main Entry Point ---\nif __name__ == \'__main__\':\n    # --- Constants ---\n    APP_NAME = "joke_generator_app"\n    USER_ID = "dev_user_01"\n    SESSION_ID = "dev_user_session_01"\n\n    # --- Session & Runner Setup ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n    session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n\n    runner = Runner(\n        agent=orchestrator_agent,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service,\n    )\n\n    call_agent("Robots")\n    # inspect_state()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"workflow-evaluator-optimizer",children:"Workflow: Evaluator-optimizer"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"evaluator\u2013optimizer pattern"})," involves two roles: one LLM generates an initial output, while another evaluates and provides structured feedback. This feedback is then used to refine the response in an iterative loop, gradually improving quality with each cycle."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use this pattern:"})}),"\n",(0,r.jsx)(n.p,{children:"This workflow is particularly useful when there are well-defined criteria for what makes a response \u201cgood,\u201d and when multiple iterations are likely to produce significant improvements. It\u2019s especially effective in scenarios where human feedback would clearly help\u2014but instead, the LLM can act as the evaluator itself. The process mirrors how a writer might draft, receive feedback, and revise a document to improve clarity, tone, or content."}),"\n",(0,r.jsx)(n.admonition,{title:"Example applications",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Crafting persuasive essays or position statements, where an evaluator LLM critiques argument strength, coherence, and tone before another round of revision"}),"\n",(0,r.jsx)(n.li,{children:"Designing user-facing chatbot replies for customer support, where the evaluator checks for tone, helpfulness, and policy compliance"}),"\n",(0,r.jsx)(n.li,{children:"Developing instructional content, such as tutorials or lesson plans, where the evaluator verifies clarity, pedagogical soundness, and alignment with learning objectives"}),"\n"]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In this example, we implement a code refinement workflow where one agent generates code based on user requirements, while another evaluates the code against those requirements. The process iterates until the code meets the quality standards."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from typing import AsyncGenerator\nfrom rich import print\nfrom rich.panel import Panel\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent, LoopAgent, BaseAgent\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.events import Event, EventActions\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv()\n\n# --- Agent Definitions ---\ndef create_code_refiner() -> LlmAgent:\n    return LlmAgent(\n        name="CodeRefiner",\n        model="gemini-2.0-flash",\n        instruction=(\n            "Read `state[\'current_code\']` (if present) and `state[\'requirements\']`. "\n            "Generate or refine Python code to meet the requirements, "\n            "then store the result in `state[\'current_code\']`."\n        ),\n        output_key="current_code"\n    )\n\ndef create_quality_checker() -> LlmAgent:\n    return LlmAgent(\n        name="QualityChecker",\n        model="gemini-2.0-flash",\n        instruction=(\n            "Evaluate the code in `state[\'current_code\']` against `state[\'requirements\']`. "\n            "If the code meets the requirements, output \'pass\'; otherwise, output \'fail\'."\n        ),\n        output_key="quality_status"\n    )\n\nclass CheckStatusAndEscalate(BaseAgent):\n    """\n    Terminates the loop when the quality check passes.\n    """\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\n        status = ctx.session.state.get("quality_status", "fail")\n        should_stop = status.strip().lower() == "pass"\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\n\n# --- Loop Agent ---\nrefinement_loop = LoopAgent(\n    name="CodeRefinementLoop",\n    max_iterations=5,\n    sub_agents=[\n        create_code_refiner(),\n        create_quality_checker(),\n        CheckStatusAndEscalate(name="StopChecker")\n    ]\n)\n\n# --- Set root agent for the web user interface ---\nroot_agent = refinement_loop\n\ndef call_agent(prompt: str):\n    """\n    Send user input to the orchestrator agent and stream responses.\n    """\n    print(Panel.fit(f"[bold white]User Prompt[/bold white]: {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        if event.is_final_response() and event.content:\n            response = event.content.parts[0].text\n            print(Panel.fit(f"[bold green]{event.author}:[/bold green] {response}", title="\ud83e\udd16"))\n\ndef inspect_state():\n    """\n    Print the internal session state.\n    """\n    state = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID).state\n    print(Panel.fit("[bold yellow]Session State[/bold yellow]"))\n    for key, value in state.items():\n        print(f"[cyan]{key}[/cyan]:\\n{value}\\n")\n\n# --- Entry Point ---\nif __name__ == \'__main__\':\n    # --- Constants ---\n    APP_NAME = "code_refinement_app"\n    USER_ID = "user_123"\n    SESSION_ID = "session_456"\n\n    # --- Services ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n\n    session = session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n\n    # --- Runner Setup ---\n    runner = Runner(\n        agent=refinement_loop,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service,\n    )\n    call_agent("Write a Python function that calculates the factorial of a number."\n               "Make sure to add type hints to the function parameters and return type.")\n    # inspect_state()\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"agents",children:"Agents"}),"\n",(0,r.jsx)(n.p,{children:"As LLMs gain capabilities in reasoning, planning, tool use, and error recovery, agents are becoming practical for production use. Agents typically start from a user command or conversation, then plan and act autonomously\u2014looping through tool use, feedback, and self-correction. They may pause for human input when encountering uncertainty or reaching checkpoints, and they rely on real-time feedback (e.g., tool outputs) to stay grounded.\nDespite their ability to tackle complex, open-ended problems, agents are often simple in implementation\u2014frequently just an LLM operating within a loop, using tools and adjusting based on observed outcomes. For this reason, careful tool design and clear documentation are critical."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use this pattern"}),":"]}),"\n",(0,r.jsx)(n.p,{children:"Use agents for open-ended tasks with unpredictable steps, where hardcoding logic is impractical. They\u2019re ideal in trusted environments requiring autonomy over multiple iterations."}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsx)(n.p,{children:"The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appr"})}),"\n",(0,r.jsx)(n.admonition,{title:"Examples of Agent Use",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.strong,{children:"research assistant agent"})," that autonomously gathers, filters, and summarizes information from multiple sources"]}),"\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.strong,{children:"data cleaning agent"})," that iteratively inspects a dataset, flags issues, and proposes structured fixes based on schema rules"]}),"\n"]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ADK Implementation:"})}),"\n",(0,r.jsx)(n.p,{children:"In this example, we implement a code execution agent that generates and executes Python code based on user input. The agent uses the built-in code execution tool to run the generated code and return the results."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import io\n\nfrom PIL import Image\nfrom dotenv import load_dotenv\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import built_in_code_execution\nfrom google.genai import types\nfrom rich import print\nfrom rich.panel import Panel\n\n# Load .env file\nload_dotenv()\n\n\nroot_agent = LlmAgent(\n    name="CodeAgent",\n    model="gemini-2.0-flash",\n    tools=[built_in_code_execution],\n    instruction="""You are a calculator agent.\n    When given a mathematical expression, function, or task, write and EXECUTE the Python code to obtain the result.\n    """,\n    description="Executes Python code to perform calculations.",\n)\n\ndef call_agent(prompt: str):\n    """\n    Send user input to the orchestrator agent and stream responses.\n    """\n    # --- Services ---\n    session_service = InMemorySessionService()\n    artifact_service = InMemoryArtifactService()\n    session = session_service.create_session(\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n    )\n    # --- Runner Setup ---\n    runner = Runner(\n        agent=root_agent,\n        app_name=APP_NAME,\n        session_service=session_service,\n        artifact_service=artifact_service,\n    )\n    print(Panel.fit(f"[bold white]User Prompt[/bold white]: {prompt}", title="\ud83d\udc64"))\n    content = types.Content(role="user", parts=[types.Part(text=prompt)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        # --- Check for specific parts FIRST ---\n        has_specific_part = False\n        if event.content and event.content.parts:\n            for part in event.content.parts:  # Iterate through all parts\n                if part.executable_code:\n                    # Access the actual code string via .code\n                    print(f"  Debug: Agent generated code:\\n```python\\n{part.executable_code.code}\\n```")\n                    has_specific_part = True\n                elif part.code_execution_result:\n                    # Access the code execution result for debugging\n                    print(f"  Debug: Code Execution Result: {part.code_execution_result.outcome} - Output:\\n{part.code_execution_result.output}")\n                    has_specific_part = True\n                elif part.inline_data:\n                    # Access the inline data (e.g., image) and display it\n                    if part.inline_data.mime_type == "image/png":\n                        # Access the image data and display it\n                        image_data = part.inline_data.data\n                        image = Image.open(io.BytesIO(image_data))\n                        image.show()\n                # Also print any text parts found in any event for debugging\n                elif part.text and not part.text.isspace():\n                    print(f"  Text: \'{part.text.strip()}\'")\n                    # Do not set has_specific_part=True here, as we want the final response logic below\n\n        # --- Check for final response AFTER specific parts ---\n        # Only consider it final if it doesn\'t have the specific code parts we just handled\n        if not has_specific_part and event.is_final_response():\n            if event.content and event.content.parts and event.content.parts[0].text:\n                final_response_text = event.content.parts[0].text.strip()\n                print(f"==> Final Agent Response: {final_response_text}")\n            else:\n                print("==> Final Agent Response: [No text content in final event]")\n\n\n# --- Entry Point ---\nif __name__ == \'__main__\':\n    # --- Constants ---\n    APP_NAME = "code_refinement_app"\n    USER_ID = "user_123"\n    SESSION_ID = "session_456"\n\n\n    call_agent("Generates an array of 1000 random numbers from a normal distribution with mean 0 and standard deviation 1, "\n               "create a histogram of the data, and "\n               "save the histogram as a PNG file plot.png")\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"key-differences-and-summary",children:"Key Differences and Summary"}),"\n",(0,r.jsx)(n.p,{children:"To help you choose the right pattern for your use case, the table below summarizes the core ideas, ideal use cases, and ADK components for each of Anthropic\u2019s agent design patterns. This comparison highlights the trade-offs between structure, flexibility, and autonomy when building LLM-powered systems."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"Pattern"})}),(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"Core Idea"})}),(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"When to Use"})}),(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"ADK Agent Types"})})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\ud83d\udd01 Prompt Chaining"}),(0,r.jsx)(n.td,{children:"Break a task into sequential steps where each step builds on the last"}),(0,r.jsx)(n.td,{children:"Tasks with clear, fixed subtasks that benefit from intermediate validation"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"SequentialAgent"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\ud83d\udd00 Routing"}),(0,r.jsx)(n.td,{children:"Classify input and delegate to specialized agents"}),(0,r.jsx)(n.td,{children:"Inputs that fall into distinct categories requiring tailored responses"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"LlmAgent"})," with ",(0,r.jsx)(n.code,{children:"sub_agents"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\u26a1 Parallelization"}),(0,r.jsx)(n.td,{children:"Run tasks concurrently and aggregate results via sectioning or voting"}),(0,r.jsx)(n.td,{children:"Tasks that benefit from speed or multiple perspectives"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"ParallelAgent"})," + merger agent"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\ud83e\udde0 Orchestrator\u2013Worker"}),(0,r.jsx)(n.td,{children:"Dynamically break down tasks and assign to sub-agents"}),(0,r.jsx)(n.td,{children:"Tasks where subtasks are not known in advance and depend on the input"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"LoopAgent"})," or custom loop + ",(0,r.jsx)(n.code,{children:"LlmAgent"})," workers"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\ud83e\uddea Evaluator\u2013Optimizer"}),(0,r.jsx)(n.td,{children:"Use one agent to generate, and another to evaluate and refine"}),(0,r.jsx)(n.td,{children:"When iterative improvement is valuable and evaluation criteria are well-defined"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"LoopAgent"})," + ",(0,r.jsx)(n.code,{children:"LlmAgent"})," evaluator"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"\ud83e\udd16 Autonomous Agents"}),(0,r.jsx)(n.td,{children:"LLMs operate independently, using tools and state in a reasoning loop"}),(0,r.jsx)(n.td,{children:"Open-ended tasks with unclear paths or evolving context over multiple turns"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"LoopAgent"})," with memory + tools"]})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"This post explored Anthropic\u2019s agent design patterns and demonstrated how to implement them using Google\u2019s Agent Development Kit (ADK). Each pattern was accompanied by hands-on examples\u2014from prompt chaining to complex orchestrator\u2013worker setups\u2014showcasing how to build LLM agents capable of handling a wide range of tasks.\nWhile Anthropic\u2019s original post emphasizes that many of these patterns can be implemented in just a few lines of code using direct LLM APIs\u2014allowing developers to avoid the overhead of complex frameworks:"}),"\n",(0,r.jsx)(n.admonition,{title:"Anthropic",type:"quote",children:(0,r.jsx)(n.p,{children:"We've worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks....We suggest that developers start by using LLM APIs directly as many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error."})}),"\n",(0,r.jsx)(n.p,{children:"It\u2019s also important to recognize the practical advantages that frameworks can offer\u2014particularly as agentic systems grow in complexity and scale. Tools like Google\u2019s ADK, LangGraph, and others provide essential capabilities out of the box, including state persistence across sessions, streaming support for long-running interactions, built-in observability and debugging tools, along with deployment-ready infrastructure. These features can greatly accelerate development, improve reliability, and simplify the orchestration of complex, multi-agent workflows in real-world applications."}),"\n",(0,r.jsx)(n.p,{children:"In short, while starting simple is often the right approach, thoughtfully leveraging a framework can help you move faster and build more robust, scalable agentic systems."}),"\n",(0,r.jsx)(n.p,{children:"In future posts, we\u2019ll explore more advanced topics in agent design and implementation, including how to build agents that can learn from user interactions, adapt to changing environments, and collaborate with other agents. We\u2019ll also explore more of ADK\u2019s capabilities through real-world use cases and end-to-end implementations."}),"\n",(0,r.jsxs)(n.p,{children:["Thanks for reading! We hope this guide offered useful insights and practical tools to support your journey in building smarter, more capable LLM agents.\nAll code examples are available in the ",(0,r.jsx)(n.a,{href:"https://github.com/haruiz/agents-experiments",children:"Agents Experiments repository."}),".\nFor more on ADK, check out the ",(0,r.jsx)(n.a,{href:"https://google.github.io/adk-docs/",children:"official documentation"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"anthropic:1",href:"https://www.anthropic.com/engineering/building-effective-agents",target:"_blank",children:(0,r.jsx)(n.p,{children:"[1] Anthropic (2024). Building Effective Agents."})})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"adk:1",href:"https://google.github.io/adk-docs/",target:"_blank",children:(0,r.jsx)(n.p,{children:"[2] Agent Development Kit (ADK)"})})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{id:"langgraph:1",href:"https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent",target:"_blank",children:(0,r.jsx)(n.p,{children:"[3] LangGraph Tutorials \u2013 Workflows and Agents"})})})]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},99869:e=>{e.exports=JSON.parse('{"permalink":"/blog/implementing-anthropic\'s-agent-design-patterns-with-google-adk","source":"@site/blog/2025-04-30-implementing-anthropic\'s-agent-design-patterns-with-google-adk/index.md","title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","description":"Agentic systems are rapidly becoming a core design pattern for LLM-powered applications, enabling dynamic reasoning, decision-making, and tool use. Inspired by Anthropic\u2019s influential Building Effective Agents article, this post demonstrates the implementation of their proposed agent design patterns\u2014such as prompt chaining, routing, and parallelization\u2014using Google\u2019s open-source Agent Development Kit (ADK). The guide provides practical, hands-on examples that illustrate how these patterns work and where they can be effectively applied.","date":"2025-04-30T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"agentic-systems","permalink":"/blog/tags/agentic-systems"},{"inline":true,"label":"llm-agents","permalink":"/blog/tags/llm-agents"},{"inline":true,"label":"agent-development-kit","permalink":"/blog/tags/agent-development-kit"},{"inline":true,"label":"prompt-engineering","permalink":"/blog/tags/prompt-engineering"},{"inline":true,"label":"ai-workflows","permalink":"/blog/tags/ai-workflows"},{"inline":true,"label":"autonomous-agents","permalink":"/blog/tags/autonomous-agents"},{"inline":true,"label":"multi-agent-systems","permalink":"/blog/tags/multi-agent-systems"},{"inline":true,"label":"adk","permalink":"/blog/tags/adk"}],"readingTime":26.55,"hasTruncateMarker":true,"authors":[{"name":"Henry Ruiz","title":"Blog Author","url":"https://github.com/haruiz","imageURL":"https://github.com/haruiz.png","key":"haruiz","page":null}],"frontMatter":{"title":"Implementing Anthropic\u2019s Agent Design Patterns with Google ADK","slug":"implementing-anthropic\'s-agent-design-patterns-with-google-adk","description":"Agentic systems are rapidly becoming a core design pattern for LLM-powered applications, enabling dynamic reasoning, decision-making, and tool use. Inspired by Anthropic\u2019s influential Building Effective Agents article, this post demonstrates the implementation of their proposed agent design patterns\u2014such as prompt chaining, routing, and parallelization\u2014using Google\u2019s open-source Agent Development Kit (ADK). The guide provides practical, hands-on examples that illustrate how these patterns work and where they can be effectively applied.","authors":["haruiz"],"tags":["python","agentic-systems","llm-agents","agent-development-kit","prompt-engineering","ai-workflows","autonomous-agents","multi-agent-systems","adk"]},"unlisted":false,"prevItem":{"title":"Accelerating Science with JAX - Simulations, Physics, and Beyond","permalink":"/blog/accelerating-science-with-jax-simulations-physics-and-beyond"},"nextItem":{"title":"Getting Started with Ray on Google Cloud Platform","permalink":"/blog/getting-started-with-ray-on-google-cloud-platform"}}')}}]);